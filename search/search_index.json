{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenido al Curso MLOps","text":"<p>Este repositorio contiene el conjunto de pr\u00e1cticas desarrolladas en el marco del curso de MLOps de la Maestr\u00eda en Ciencia de Datos. El objetivo principal de estas pr\u00e1cticas es que los estudiantes adquieran experiencia en el uso de herramientas esenciales para gestionar todo el ciclo de vida de un proyecto de aprendizaje autom\u00e1tico, desde la recolecci\u00f3n y preparaci\u00f3n de los datos hasta el despliegue y monitoreo del modelo en producci\u00f3n.</p> <p>Dado que el desarrollo, la implementaci\u00f3n y el mantenimiento de modelos de machine learning en entornos reales puede ser complejo, las pr\u00e1cticas se apoyan en los principios de MLOps (Machine Learning Operations), un conjunto de pr\u00e1cticas que busca automatizar y simplificar los flujos de trabajo y despliegues de modelos, facilitando as\u00ed su integraci\u00f3n efectiva en sistemas productivos.</p> <p>Como caso pr\u00e1ctico, se desarrolla un modelo de predicci\u00f3n de precios para tiendas de consignaci\u00f3n, un modelo de negocio en el que los productos son vendidos por una tienda en nombre de terceros, a cambio de una comisi\u00f3n. El objetivo es ayudar a propietarios y vendedores a determinar precios \u00f3ptimos para sus art\u00edculos, considerando factores como la condici\u00f3n del producto, la marca y las tendencias del mercado. Una mejor estrategia de precios puede traducirse en un aumento de las ventas y una mayor rentabilidad para ambas partes.</p> <p>Las pr\u00e1cticas de este curso est\u00e1n organizadas en niveles progresivos de madurez en MLOps, que reflejan el grado de automatizaci\u00f3n y robustez del ciclo de vida de un proyecto de aprendizaje autom\u00e1tico:</p> <ul> <li> <p>MLOps Nivel 1 Automatizaci\u00f3n del pipeline de ML. Se enfoca en establecer un flujo automatizado para el entrenamiento continuo del modelo, integrando validaciones de datos y modelos, as\u00ed como el manejo de metadatos y activadores del pipeline.</p> </li> <li> <p>MLOps Nivel 2 Automatizaci\u00f3n con CI/CD. Permite implementar cambios de forma r\u00e1pida y confiable mediante sistemas CI/CD automatizados, facilitando la experimentaci\u00f3n y el despliegue de mejoras en producci\u00f3n.</p> </li> </ul> <p>Estos niveles sientan las bases para alcanzar el MLOps Nivel 3, que se centra en el despliegue y monitoreo continuo del modelo en producci\u00f3n, asegurando su rendimiento a lo largo del tiempo y permitiendo ajustes din\u00e1micos ante cambios en los datos o en el entorno de operaci\u00f3n.</p>"},{"location":"ambiente/p1/","title":"Linea de Comandos","text":"<p>En esta pr\u00e1ctica inicial comenzamos nuestro recorrido en el mundo de las operaciones de aprendizaje autom\u00e1tico (MLOps). Antes de adentrarnos en los conceptos fundamentales, es crucial asegurarnos de que todos cuenten con una base s\u00f3lida en ciertos temas esenciales que utilizaremos a lo largo del curso.</p> <p>Nos enfocaremos en repasar el uso de la l\u00ednea de comandos, una herramienta clave para la configuraci\u00f3n de un entorno de desarrollo adecuado. Este entorno ser\u00e1 fundamental para acompa\u00f1arlos durante todo el proceso de aprendizaje y aplicaci\u00f3n de MLOps.</p>"},{"location":"ambiente/p1/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Esta pr\u00e1ctica tiene como objetivo familiarizarse con el uso de la l\u00ednea de comandos como herramienta esencial en proyectos de MLOps.</p>"},{"location":"ambiente/p1/#que-es-la-linea-de-comandos","title":"\u00bfQu\u00e9 es la l\u00ednea de comandos?","text":"<p>La l\u00ednea de comandos, o terminal, no es un concepto arcaico, sino una herramienta fundamental que precede a las interfaces gr\u00e1ficas. Es ampliamente utilizada en Linux, aunque en Mac y Windows suele pasarse por alto. En MLOps, muchas herramientas carecen de interfaz gr\u00e1fica, por lo que dominar la terminal mejora el flujo de trabajo y es esencial, especialmente cuando trabajemos en la nube m\u00e1s adelante.</p>"},{"location":"ambiente/p1/#instalacion-de-wsl-subsistema-de-windows-para-linux","title":"Instalaci\u00f3n de WSL (Subsistema de Windows para Linux)","text":"<p>Si no usas Linux, debes instalar WSL para tener este sistema operativo en tu m\u00e1quina Windows. En la secci\u00f3n prerequistos de la gu\u00eda oficial de instalaci\u00f3n de WSL puedes verificar que tu sistema cumple con los requisitos necesarios.  Si ya usas Linux como sistema operativo puedes omitir esta secci\u00f3n.</p> <p>WSL puede instalarse principalmente de dos formas, dependiendo de la versi\u00f3n y n\u00famero de compilaci\u00f3n del sistema operativo Windows que tengas instalado en tu m\u00e1quina.</p> <ul> <li> <p>Compilaciones recientes de Windows.     En la seccion Get Started del documento que gu\u00eda en el proceso de configuraci\u00f3n de un entorno de desarrollo WSL, puedes revisar los requistos para usar la versi\u00f3n simplificada de instalaci\u00f3n usando el comando <code>wsl --install</code> desde la terminal de Windows (PowerShell o CMD).</p> <p>Si cumples los requisitos, te recomendamos sigas la guia de instalaci\u00f3n pero cambies la distribuci\u00f3n por omisi\u00f3n, de Ubuntu a Ubuntu 24.04.1 LTS. En la secci\u00f3n Change the default Linux distribution installed, del mismo documento oficial podras encontrar como hacerlo.</p> </li> <li> <p>Compilaciones antiguas de Windows.     Si tu compilacion de Windows no cumple los requisitos indicados en el punto previo, te recomendamos sigas la guia de instalaci\u00f3n manual descrita en este documento. Al final del proceso, recuerde escoger la distribuci\u00f3n Ubuntu 24.04.1 LTS como opci\u00f3n a instalar.</p> </li> </ul> <p>Una vez que haya instalado la distribuci\u00f3n de Ubuntu seleccionada, tendr\u00e1 que crear una cuenta de usuario y una contrase\u00f1a para acceder a esta distribuci\u00f3n de Linux. Toma en cuenta, que WSL(Linux) y Windows son dos sistemas separados, por tanto, al instalar paquetes (como pip) en Linux no los instala autom\u00e1ticamente en Windows. Debes instalarlos en cada sistema por separado si los necesitas en ambos. Sin embargo, el resto de las practicas usar\u00e1n Linux por defecto por lo que no es necesario instalar paquetes en ambos sistemas operativos.</p>"},{"location":"ambiente/p1/#verificacion-de-la-version-wsl-instalada","title":"Verificaci\u00f3n de la versi\u00f3n WSL instalada","text":"<p>Es importante verificar que este usando la versi\u00f3n WSL 2 pues esta versi\u00f3n ofrece una serie de mejoras clave frente a WSL 1 que lo hacen necesario o altamente recomendable para la mayor\u00eda de los casos de uso actuales, especialmente en entornos de desarrollo, ciencia de datos, y pr\u00e1cticas que requieren compatibilidad real con Linux.</p> <p>\u00bfC\u00f3mo verificar la versi\u00f3n de WSL y la distribuci\u00f3n instalada?</p> <p>Ejecuta: <pre><code>wsl -l -v\n</code></pre></p> <p>Esto mostrar\u00e1 la lista de distribuciones instaladas y la versi\u00f3n de WSL que est\u00e1n utilizando. Por ejemplo: <pre><code>NAME      STATE           VERSION\nUbuntu    Running         1\n</code></pre> Si el resultado es el mismo que en el ejemplo, significa que no se ha utilizado la opci\u00f3n correcta de instalaci\u00f3n, acorde con la compilaci\u00f3n de tu sistema Windows</p>"},{"location":"ambiente/p1/#problemas-en-la-instalacion","title":"Problemas en la instalaci\u00f3n","text":"<p>En algunos casos, el proceso de instalaci\u00f3n puede no completarse correctamente. En estas situaciones, te recomendamos consultar los problemas y soluciones proporcionados por Microsoft.</p>"},{"location":"ambiente/p1/#uso-de-la-terminal","title":"Uso de la terminal","text":"<p>Importante</p> <p>Si no est\u00e1 familiarizado con Linux vale la pena revisar un resumen con los principales comandos que se pueden ejecutar en la terminal.</p> <p>Familiarizarse con los comandos b\u00e1sicos de la terminal <code>which</code>, <code>echo</code>, <code>cat</code>, <code>wget</code>, <code>less</code>, y <code>top</code> y el operador de redirecci\u00f3n de salida <code>&gt;</code>. Responde las preguntas en la plataforma virtual</p> <ul> <li> <p>Abre una terminal (en la distribuci\u00f3n de Linux que estes usando)</p> </li> <li> <p>Actualiza la lista de paquetes y sus versiones en el sistema utilizando los siguientes comandos. Esta acci\u00f3n es especialmente importante si acabas de instalar WSL, pues este sistema intenta ser liviano y r\u00e1pido y no fuerza actualizaciones autom\u00e1ticas.</p> <pre><code>sudo apt update\nsudo apt upgrade \n</code></pre> </li> <li> <p>Instala tambi\u00e9n <code>pip</code> (el gestor de paquetes de Python).     <pre><code>sudo apt install python3-pip.\n</code></pre></p> </li> <li> <p>Verifica la instalaci\u00f3n escribiendo <code>pip3 --version</code>.</p> </li> <li> <p>Crea una carpeta principal denominada <code>Practicas-MLOPS</code> para almacenar todas las pr\u00e1cticas del curso. Dentro de esta carpeta, crea una subcarpeta llamada <code>Practica-1</code> y ub\u00edcate dentro de ella para realizar el trabajo correspondiente.</p> </li> <li> <p>Es importante saber editar archivos desde la terminal. La mayor\u00eda de los sistemas tiene instalado el editor <code>nano</code>; si no, identifica cu\u00e1l est\u00e1 disponible.</p> <ul> <li>Escribe <code>nano</code> en la terminal.</li> <li> <p>Escribe el siguiente texto en el script:</p> <pre><code>if __name__ == \"__main__\":\n    print(\"Hola mundo!\")\n</code></pre> </li> <li> <p>Guarda el script como <code>.py</code>, ejec\u00fatalo con <code>python3 &lt;nombre_archivo&gt;.py</code>.</p> </li> <li>Edita el archivo desde la terminal para cambiar el mensaje.</li> </ul> </li> <li> <p>Todas las terminales vienen con un lenguaje de programaci\u00f3n. El sistema m\u00e1s com\u00fan se llama <code>bash</code>, y saber escribir programas simples en bash puede ser muy \u00fatil. Por ejemplo, para ejecutar varios programas de Python de manera secuencial, puedes hacerlo mediante un script en bash.</p> <ul> <li> <p>Escribe un script en bash (usando <code>nano</code>) y gr\u00e1balo como <code>mi_script.sh</code>:</p> <pre><code>#!/bin/bash\n# Un script de ejemplo en bash\necho \"\u00a1Hola Mundo!\"\n</code></pre> </li> <li> <p>Averigua como ejecutarlo. Responde las preguntas en la plataforma virtual</p> </li> <li>Modifica el script <code>mi_script.sh</code> para llamar al programa Python que escribiste. Responde la pregunta en la plataforma virtual</li> <li>Escribe un nuevo script (usando <code>nano</code>) y gr\u00e1balo como <code>ciclo.sh</code>. El script debe implementar un bucle for que ejecute el script (<code>mi_script.sh</code>) 10 veces seguidas. Responda la pregunta en la plataforma virtual.</li> <li> <p>Ejecuta el nuevo script. La salida debe mostrase as\u00ed:</p> <p></p> </li> </ul> </li> <li> <p>Un truco que se va a necesitar a lo largo de este curso es establecer variables de entorno. Una variable de entorno es simplemente un valor con nombre din\u00e1mico que puede alterar el comportamiento de los procesos en ejecuci\u00f3n en una computadora. La sintaxis para establecer una variable de entorno depende del sistema operativo.</p> <ul> <li> <p>Establecer una variable de entorno e imprimirla.</p> <pre><code>export VARIABLE=hola\necho $VARIABLE\n</code></pre> </li> <li> <p>Para usar una variable de entorno en un programa Python, puedes usar la funci\u00f3n <code>os.environ</code> del m\u00f3dulo <code>os</code>. Escriba un programa Python (<code>programa.py</code>) que imprima la variable de entorno que acaba de establecer. Responda pregunta en la plataforma virtual.</p> </li> <li> <p>Si tiene una colecci\u00f3n de variables de entorno, \u00e9stas pueden almacenarse en un archivo <code>.env</code>. El archivo denominado, por ejemplo, <code>ventorno.env</code> deber\u00eda tener el siguiente formato:</p> <pre><code>VARIABLE_1=Hola\nVARIABLE_2=Mundo\n</code></pre> </li> </ul> </li> <li> <p>Para cargar las variables de entorno desde el archivo, puede utilizar el paquete <code>python-dotenv</code>. Puedes instalarlo usando <code>pip3 install python-dotenv</code> y luego intenta cargar las variables de entorno desde el archivo e imprimirlas. Crea el siguiente archivo en python que lea una de las variables del archivo <code>ventorno.env</code>.</p> <pre><code>import os\nfrom dotenv import load_dotenv\nload_dotenv(\u2018path_to_file_env\u2019)\nprint(os.environ[\"VARIABLE_1\"])\n</code></pre> <p>Responde en la plataforma virtual porqu\u00e9 no fue posible instalar el modulo <code>python-dotenv</code>.</p> </li> </ul> <p>Informaci\u00f3n</p> <p>La explicaci\u00f3n sobre el problema de instalaci\u00f3n del modulo <code>python-dotenv</code> lo abordaremos en el pr\u00f3ximo tema.</p>"},{"location":"ambiente/p2/","title":"Gesti\u00f3n de Paquetes y Entornos Virtuales","text":"<p>En esta pr\u00e1ctica continuamos nuestro recorrido por el mundo de las operaciones de aprendizaje autom\u00e1tico (MLOps). Antes de avanzar hacia procesos m\u00e1s complejos, es esencial contar con las herramientas adecuadas para instalar, actualizar y gestionar las dependencias de nuestros proyectos.</p> <p>Nos enfocaremos en el uso de gestores de paquetes como <code>conda</code> y <code>pip</code>, que nos permitir\u00e1n construir entornos reproducibles, mantener versiones compatibles de librer\u00edas y facilitar la colaboraci\u00f3n entre equipos. Estas habilidades ser\u00e1n clave para desarrollar proyectos robustos y eficientes a lo largo del curso de MLOps.</p>"},{"location":"ambiente/p2/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Esta pr\u00e1ctica tiene como objetivo familiarizarse con el uso de gestores de paquetes para instalar, gestionar y documentar dependencias en proyectos de MLOps, asegurando entornos consistentes y controlados para el desarrollo y despliegue de modelos.</p>"},{"location":"ambiente/p2/#instalacion-de-conda-o-miniconda","title":"Instalaci\u00f3n de Conda o Miniconda","text":"<ul> <li> <p>Descargue e instala <code>conda</code> o <code>miniconda</code>. <code>Conda</code> incluye muchos paquetes, mientras que <code>miniconda</code> es m\u00e1s ligera. Para los fines del curso se puede instalar <code>miniconda</code>.  Las instrucciones para la instalaci\u00f3n de Miniconda se encuentran en: https://www.anaconda.com/docs/getting-started/miniconda/install. </p> <p>Observaci\u00f3n-1</p> <p>Recuerde leer las instrucciones para Linux desde la terminal (Linux terminal installer).</p> <p>Observaci\u00f3n-2</p> <p>Cuando se le pregunte si desea modificar la configuraci\u00f3n del shell para inicializar conda cada vez que abras un nuevo shell y para reconocer los comandos de conda autom\u00e1ticamente, responda YES.</p> </li> <li> <p>Verifica la instalaci\u00f3n. Si no funciona, puede ser necesario configurar la variable de entorno correspondiente. Si se ha instalado con \u00e9xito <code>miniconda</code>, entonces deber\u00edas ser capaz de ejecutar el comando <code>conda --version</code> en la terminal.</p> <p></p> <p>Conda siempre indicara en que entorno est\u00e1s actualmente, indicado por (<code>env_name</code>) en el prompt. Por defecto, siempre se iniciar\u00e1 en el entorno (<code>base</code>).</p> </li> </ul>"},{"location":"ambiente/p2/#creacion-de-entornos-virtuales","title":"Creaci\u00f3n de entornos virtuales","text":"<ul> <li>Dentro del directorio principal denominada <code>Practicas-MLOPS</code> crea un subdirectorio <code>Practica-2</code> para alojar los ejercicios correspondientes a este tema.</li> </ul> <p>Las preguntas de las siguientes tareas responder en la plataforma virtual. Si no esta familiarizado con <code>conda</code> puede revisar informaci\u00f3n de los comandos principales.</p> <ul> <li> <p>Crear un nuevo entorno virtual. Aseg\u00fararse de que se llama <code>mi_entorno</code> y que instala la versi\u00f3n 3.11 de Python. </p> <ul> <li>\u00bfQu\u00e9 comando deber\u00edas ejecutar para hacer esto?</li> <li>\u00bfQu\u00e9 comando de conda da una lista de todos los entornos creados?</li> <li>\u00bfQu\u00e9 comando de conda se puede usar para activar el entorno creado?</li> <li>\u00bfQu\u00e9 comando de conda da una lista de los paquetes instalados en el entorno actual?</li> </ul> </li> <li> <p>Pruebe de instalar el paquete <code>dotenv</code> dentro del entorno. Lo recomendable es instalar el paquete usando <code>conda</code>, sin embargo, es bastante seguro usar <code>pip</code> dentro de conda hoy en d\u00eda. Use <code>pip</code> esta vez, y una vez instale el paquete cree un archivo de Python <code>programa_entorno.py</code> para cargar las variables de entorno desde el archivo <code>ventorno.env</code>. !! Recuerda la \u00faltima tarea de la pr\u00e1ctica previa !!.</p> </li> <li> <p>Verifica que el archivo de Python <code>programa_entorno.py</code> imprime una de las variables del archivo <code>ventorno.env</code>.</p> </li> <li> <p>\u00bfC\u00f3mo exportar f\u00e1cilmente la lista de paquetes instalados en el entorno a un archivo de texto? Aseg\u00farate de exportarlo a un archivo llamado <code>environment.yaml</code>, ya que conda usa otro formato por defecto que pip.</p> </li> <li> <p>Inspeccione el archivo para ver qu\u00e9 contiene. El archivo <code>environment.yaml</code> que has creado es una forma de asegurar la reproducibilidad entre usuarios porque cualquiera deber\u00eda ser capaz de obtener una copia exacta de tu entorno si tiene tu archivo <code>environment.yaml</code>. </p> </li> <li> <p>\u00bfEl paquete <code>dotenv</code> recientemente instalado en el entorno <code>conda</code> forma parte de la lista?. \u00bfPorqu\u00e9 no est\u00e1 en la lista si fuese el caso? Puedes crear un archivo <code>environment1.yaml</code> que contenga tambi\u00e9n el paquete <code>dotenv</code>.</p> </li> <li> <p>Intente crear un nuevo entorno (<code>mi_entorno_copia</code>) directamente desde su archivo <code>environment.yaml</code> y compruebe que los paquetes que se instalan coinciden exactamente con los que ten\u00eda originalmente.</p> </li> <li> <p>\u00bfCu\u00e1l es el comando pip correspondiente que da una lista de todos los paquetes <code>pip</code> instalados? \u00bfY c\u00f3mo exportar esto a un archivo <code>requirements.txt</code>?</p> </li> <li> <p>Si echas un vistazo a los requisitos que tanto <code>pip</code> como <code>conda</code> producen, ver\u00e1s que a menudo est\u00e1n llenos de muchos m\u00e1s paquetes de los que est\u00e1s usando en tu proyecto. Lo que te interesa son los paquetes que importas en tu c\u00f3digo: <code>from package import module</code>. Una forma de evitar esto es usar el paquete <code>pipreqs</code>, que autom\u00e1ticamente escanear\u00e1 tu proyecto y crear\u00e1 un archivo de requisitos espec\u00edfico para \u00e9l.  </p> </li> <li> <p>Luego de instalar <code>pipreqs</code> en el entorno de copia, pruebe este paquete sobre el c\u00f3digo de los archivos de Python de la carpeta en la Practica-1.</p> </li> <li> <p>\u00bfQu\u00e9 aspecto tiene el archivo <code>requirements.txt</code> que produce <code>pipreqs</code> comparado con los archivos producidos por <code>pip</code> o <code>conda</code>?</p> </li> </ul> <p>Importante</p> <p>Si bien los m\u00e9todos mencionados en los ejercicios son excelentes maneras de construir archivos de requisitos autom\u00e1ticamente, a veces es m\u00e1s f\u00e1cil sentarse y crear manualmente los archivos, ya que de esa manera te aseguras de que s\u00f3lo se instalen los requisitos m\u00e1s necesarios al crear un nuevo entorno.</p>"},{"location":"ambiente/p3/","title":"Editor","text":"<p>En esta pr\u00e1ctica se describen algunos ejercicios para que empieces a familiarizarte con el editor que has elegido. Si ya eres un experto en uno de ellos, ser\u00eda ideal que se complete tambi\u00e9n los ejercicios. Al menos deber\u00edas ser capaz de:</p> <ul> <li>Crear un nuevo archivo</li> <li>Ejecutar un script/archivo Python</li> <li>Hacer algun cambio y verificar errores</li> </ul> <p>Aunque las instrucciones est\u00e1n pensadas para <code>Visual Studio Code</code> (recomendaci\u00f3n del curso), se sugiere responder las preguntas incluso usando otro editor. Las tareas planteadas son solo el inicio; hay tutoriales disponibles para continuar aprendiendo.</p>"},{"location":"ambiente/p3/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Esta pr\u00e1ctica tiene como objetivo familiarizarse con el uso de editores de c\u00f3digo como <code>Visual Studio Code</code> (en adelante, <code>VS Code</code>), herramienta fundamental para escribir, ejecutar y depurar c\u00f3digo en proyectos de MLOps.</p>"},{"location":"ambiente/p3/#instalacion-de-vs-code","title":"Instalaci\u00f3n de VS Code","text":"<p>Ahora lo primero es instalar <code>VSCode</code>. </p>"},{"location":"ambiente/p3/#usuarios-de-wsl","title":"Usuarios de WSL","text":"<p>En caso de utilizar WSL, le recomendamos seguir las instrucciones de esta gu\u00eda. Recuerda que si tienes Windows ya instalamos previamente WSL para trabajar en un ambiente de Linux. Ahora tienes que instalar <code>VSCode</code> sobre Windows y luego instalar algunas extensiones para conectarse con WSL. </p> <p>Importante</p> <p>En la gu\u00eda se muestra dos formas para abrir una carpeta o espacio de trabajo remoto. Se sugiere probar que ambas opciones funcionen.</p>"},{"location":"ambiente/p3/#usuarios-de-linux","title":"Usuarios de Linux","text":"<p>Si est\u00e1s trabajando en Linux directamente te recomendamos seguir las instruciones de instalaci\u00f3n, dependiendo de la distribuci\u00f3n utilizada.</p>"},{"location":"ambiente/p3/#uso-de-vs-code","title":"Uso de VS Code","text":""},{"location":"ambiente/p3/#usuarios-de-wsl_1","title":"Usuarios de WSL","text":"<p>En caso de trabajar con WSL, siga el tutorial Working in WSL, que gu\u00eda en la creaci\u00f3n de una aplicaci\u00f3n Hello World en WSL. Todo lo descrito en el tutorial en las secciones <code>Prerequisites</code>, <code>Enable WSL</code> y <code>Install a Linux distro</code> ya fueron abordados en los pasos previos. Por tanto, se puede completar el tutorial desde la secci\u00f3n <code>Python development</code>, teniendo en cuenta que Python ya fue instalado previamente al configurar la distribuci\u00f3n de Linux mediante WSL.  En la gu\u00eda se utiliza la carpeta <code>helloWord</code> como espacio de trabajo, pero en su lugar, crea una subcarpeta llamada <code>Practica-3</code> dentro del directorio <code>Practica-MLOPS</code>.</p> <p>Importante</p> <p>Es altamente recomendable cerrar la conexi\u00f3n cuando se termine de trabajar con VS Code. Puedes finalizar tu sesi\u00f3n en WSL y volver a ejecutar VS Code de forma local seleccionando File &gt; Close Remote Connection.</p>"},{"location":"ambiente/p3/#usuarios-de-linux_1","title":"Usuarios de Linux","text":"<p>En caso de trabajar con Linux, le recomendamos revise este tutorial desde la secci\u00f3n <code>Install a language extension</code>. El espacio de trabajo para crear el archivo <code>hello.py</code> (descrito en la gu\u00eda) debe estar en el directorio <code>Practica-3</code> dentro de la carpeta <code>Practica-MLOPS</code>.</p>"},{"location":"arquitectura/flujo_proyecto/","title":"Arquitectura del Proyecto","text":"<p>La siguiente arquitectura es la base de las pr\u00e1cticas del curso de MLOps de la Maestr\u00eda en Ciencia de Datos, y permite estructurar de manera clara y ordenada cada etapa del ciclo de vida de un proyecto de aprendizaje autom\u00e1tico. Desde la comprensi\u00f3n del problema de negocio y la preparaci\u00f3n de los datos, hasta el entrenamiento, despliegue, monitoreo y mejora continua del modelo, esta arquitectura permite aplicar los principios de MLOps de forma progresiva. Cada componente de este flujo gu\u00eda a los estudiantes en el uso de herramientas espec\u00edficas para automatizar procesos, garantizar la reproducibilidad y facilitar la integraci\u00f3n de modelos en entornos reales. La representaci\u00f3n visual de esta arquitectura sirve como referencia central para entender c\u00f3mo se conectan y orquestan todos los elementos del proyecto.</p> <p></p>"},{"location":"arquitectura/flujo_proyecto/#estructura-practica-del-curso-basada-en-la-arquitectura","title":"Estructura pr\u00e1ctica del curso basada en la arquitectura","text":"<p>A continuaci\u00f3n, se detalla la relaci\u00f3n entre las pr\u00e1cticas propuestas y la arquitectura presentada. </p> <ul> <li> <p>Las tres primeras pr\u00e1cticas (Linea de Comandos,      Gesti\u00f3n de Paquetes y Entornos Virtuales y Editor est\u00e1n orientadas a que los estudiantes comprendan los conceptos fundamentales y se familiaricen con las herramientas necesarias para la creaci\u00f3n de un entorno de desarrollo adecuado, que servir\u00e1 como base para el resto del ciclo de vida del proyecto de aprendizaje autom\u00e1tico. </p> </li> <li> <p>Problema del Negocio. En la pr\u00e1ctica Alineaci\u00f3n Estrat\u00e9gica del Proyecto, se utilizar\u00e1n las herramientas AI Canvas y/o ML Canvas con el objetivo de alinear los objetivos del proyecto con una necesidad concreta del negocio, comprender mejor el contexto del problema y analizar los datos disponibles para su desarrollo.</p> <p>Adem\u00e1s, como parte de la preparaci\u00f3n inicial, se ejecutar\u00e1n tareas clave de configuraci\u00f3n del proyecto, incluyendo:</p> <ul> <li> <p>La creaci\u00f3n del entorno virtual utilizando <code>Conda</code>, lo que permitir\u00e1 una gesti\u00f3n adecuada de las dependencias del proyecto, y</p> </li> <li> <p>La definici\u00f3n de la estructura del proyecto mediante <code>Cookiecutter</code>, siguiendo buenas pr\u00e1cticas de organizaci\u00f3n del c\u00f3digo y recursos en proyectos de machine learning.</p> </li> </ul> </li> <li> <p>Datos. Para este proyecto de predicci\u00f3n, se utiliza el conjunto de datos Supply Chain Shipment Pricing Data, que contiene informaci\u00f3n detallada sobre env\u00edos de mercanc\u00edas. Este dataset es especialmente \u00fatil para tareas de modelado predictivo relacionadas con el c\u00e1lculo o estimaci\u00f3n de costos log\u00edsticos.</p> <p>Algunas de la variables incluidas en este conjunto de datos son:</p> <ul> <li>Tipo de env\u00edo: define la modalidad del env\u00edo (por ejemplo, urgente, regular).</li> <li>Origen y destino: indica las ubicaciones geogr\u00e1ficas involucradas en el transporte.</li> <li>Peso y volumen del env\u00edo: caracter\u00edsticas f\u00edsicas relevantes para el c\u00e1lculo de tarifas y la log\u00edstica.</li> <li>Fecha y hora del env\u00edo: permiten explorar aspectos temporales como estacionalidad o plazos de entrega.</li> <li>M\u00e9todo de transporte: medio utilizado (a\u00e9reo, mar\u00edtimo, terrestre, etc.).</li> <li>Costos asociados: variable objetivo que se desea predecir, relacionada con el precio final del env\u00edo.</li> </ul> <p>Este conjunto de datos es el insumo clave para el desarrollo del modelo que permite automatizar la estimaci\u00f3n de costos y optimizar procesos dentro de la cadena de suministro. En la pr\u00e1ctica Versionado del C\u00f3digo, se implementan los scripts necesarios para cargar y preparar esta fuente de datos durante el proceso de entrenamiento del modelo.</p> </li> <li> <p>Entrenamiento. El proceso de entrenamiento en un flujo MLOps, abarca desde la recopilaci\u00f3n y preparaci\u00f3n de los datos hasta la generaci\u00f3n de un modelo listo para producci\u00f3n. En el contexto del curso de MLOps, este proceso se fortalece mediante la aplicaci\u00f3n de pr\u00e1cticas esenciales que aseguran reproducibilidad, trazabilidad y escalabilidad.</p> <p>Durante la pr\u00e1ctica de Versionado del C\u00f3digo, se establecen mecanismos para registrar cambios en los scripts de entrenamiento, facilitando la colaboraci\u00f3n y el control de modificaciones. Con Empaquetado y gesti\u00f3n de dependencias, se garantiza que los entornos sean reproducibles y portables, lo cual es clave para ejecutar el entrenamiento en distintos entornos sin errores.</p> <p>La pr\u00e1ctica de Buenas Pr\u00e1cticas de Codificaci\u00f3n mejora la legibilidad, mantenibilidad y robustez del c\u00f3digo usado en la ingesta, preprocesamiento y entrenamiento. Por otro lado, Almacenamiento remoto con DVC y Control de versiones de datos permiten manejar datasets grandes de forma eficiente y controlada, asegurando que se pueda reproducir exactamente cualquier experimento.</p> <p>Con Trabajando con Pipelines, se estructuran las etapas del flujo de datos y entrenamiento como tareas modulares, lo que facilita la automatizaci\u00f3n y escalabilidad del proceso. Finalmente, el Control de Versiones del Modelo permite registrar cada versi\u00f3n entrenada con sus m\u00e9tricas y configuraci\u00f3n, lo que es vital para hacer seguimiento y decidir qu\u00e9 modelo es apto para desplegar.</p> <p>Estas pr\u00e1cticas integradas fortalecen cada paso del entrenamiento, desde los datos hasta el modelo final, asegurando un flujo de trabajo robusto y alineado con los principios de MLOps.</p> </li> <li> <p>Despliegue. La fase de Despliegue representa el proceso de poner el modelo entrenado en producci\u00f3n para que pueda ser consumido por aplicaciones o usuarios finales. Esta etapa incluye:</p> <ul> <li> <p>Punto de Acceso al Modelo: el modelo se expone a trav\u00e9s de un servicio, usualmente como una API, facilitando su integraci\u00f3n en sistemas reales.</p> </li> <li> <p>Imagen Docker: el modelo, junto con su entorno y dependencias, se empaqueta en un contenedor, asegurando portabilidad y consistencia en distintos entornos.</p> </li> <li> <p>Configuraci\u00f3n del Despliegue y Proveedor de Servicios en la Nube: se define la infraestructura en la nube donde se alojar\u00e1 el modelo, permitiendo escalar y monitorear el servicio.</p> </li> <li> <p>Servicio (API Endpoint): es el punto final donde se reciben solicitudes y se devuelven predicciones, habilitando el uso pr\u00e1ctico del modelo en tiempo real.</p> </li> </ul> <p>Esta fase es clave para cerrar el ciclo del flujo MLOps y conectar el modelo con el mundo real.</p> </li> <li> <p>Monitoreo. Esta fase se encarga de supervisar el comportamiento del modelo una vez desplegado. Esta incluye:</p> <ul> <li> <p>Monitoreo del Modelo: permite detectar desviaciones en el rendimiento del modelo (por ejemplo, por cambios en los datos de entrada), lo que puede indicar necesidad de reentrenamiento.</p> </li> <li> <p>Seguimiento del Modelo: registra m\u00e9tricas clave, como precisi\u00f3n, latencia o tasa de error, a lo largo del tiempo para asegurar que el modelo sigue cumpliendo con los objetivos del negocio.</p> </li> </ul> <p>Esta etapa es fundamental para mantener la calidad y confiabilidad del sistema de IA/ML en producci\u00f3n.</p> </li> </ul>"},{"location":"entorno/p5/","title":"Creaci\u00f3n del Entorno Virtual","text":"<p>Una tarea basica en el flujo de trabajo MLOps es la creaci\u00f3n de un entorno virtual, incluso antes de establecer la estructura base del proyecto. Este paso es fundamental para asegurar un entorno aislado y reproducible desde el inicio. Este entorno permite aislar las dependencias del proyecto y evitar conflictos con otras configuraciones del sistema. Usaremos Conda, un gestor de entornos ampliamente adoptado en la ciencia de datos, por su capacidad para manejar tanto paquetes de Python como binarios del sistema.</p> <p>Crear un entorno virtual no solo mejora la organizaci\u00f3n y portabilidad del proyecto, sino que tambi\u00e9n garantiza que todos los colaboradores trabajen bajo las mismas condiciones, lo cual es clave para la reproducibilidad, la consistencia y el despliegue confiable de modelos.</p>"},{"location":"entorno/p5/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Configurar un entorno virtual utilizando Conda para gestionar las dependencias del proyecto de forma controlada, asegurando un desarrollo reproducible y colaborativo.</p>"},{"location":"entorno/p5/#crear-y-activar-entorno-del-proyecto","title":"Crear y activar entorno del proyecto","text":"<p>Trabajar dentro de un entorno virtual asegura que el proyecto utilice \u00fanicamente las versiones espec\u00edficas de las librer\u00edas requeridas, evitando conflictos con otras configuraciones del sistema. Esto resulta especialmente crucial en contextos colaborativos, ya que permite que todos los miembros del equipo trabajen bajo las mismas condiciones, lo que facilita tanto la reproducibilidad del proyecto como su mantenimiento.</p> <p>Responde las preguntas a estas taraes en la Plataforma Virtual:</p> <ol> <li>Abrir una terminal (o una terminal de WSL si est\u00e1s trabajando Linux remotamente sobre Windows).</li> <li>Suponga que el proyecto estar\u00e1 alojado dentro de la carpeta <code>Pr\u00e1ctica-5</code>, la cual es parte del directorio principal <code>Practicas-MLOPS</code>. </li> <li> <p>Crear el entorno virtual, usando la siguiente informaci\u00f3n:</p> <pre><code>nombre entorno: mlops_env\nversi\u00f3n Pythton: 3.11\n</code></pre> </li> <li> <p>Activa el entorno creado</p> </li> <li>Verifica que el entorno est\u00e1 activo, observando que su nombre aparece al inicio de la l\u00ednea de comandos.</li> <li>A partir de este momento, todas las librer\u00edas necesarias para el proyecto deber\u00e1n instalarse dentro de este entorno usando <code>conda install</code> o <code>pip install</code>.</li> </ol>"},{"location":"entorno/p5/#tarea","title":"\ud83d\udee0\ufe0f Tarea","text":"<p>Como parte de los requerimientos del proyecto, necesitas crear un archivo de Python llamado <code>estadisticas.py</code>. Este archivo debe utilizar la biblioteca <code>pandas</code> para construir un peque\u00f1o DataFrame con datos de ejemplo y mostrar estad\u00edsticas descriptivas b\u00e1sicas.</p> <p>Para facilitar la implementaci\u00f3n se usar\u00e1 VS Code</p> <ol> <li>Abre VS Code y aseg\u00farate de estar trabajando dentro de la carpeta <code>Practica-5</code>.</li> <li>Abre la terminal integrada en VS Code (Ctrl + o Ver &gt; Terminal), y activa el entorno de conda creado previamente.</li> <li>Instala la biblioteca <code>pandas</code> version=1.5.3 en el entorno virtual.</li> <li>Verifica que el entorno tiene instalada la biblioteca <code>pandas</code></li> <li>Crea el archivo <code>estadisticas.py</code> dentro de la carpeta actual.</li> <li> <p>Escribe el siguiente c\u00f3digo en <code>estadisticas.py</code>:</p> <pre><code>import pandas as pd\n\n# Crear un DataFrame con datos de ejemplo\ndatos = {\n    'Nombre': ['Ana', 'Luis', 'Carlos', 'Marta'],\n    'Edad': [23, 35, 29, 42],\n    'Ingresos': [2500, 4000, 3200, 5000]\n}\n\ndf = pd.DataFrame(datos)\n\n# Mostrar el DataFrame\nprint(\"Datos del DataFrame:\")\nprint(df)\n\n# Mostrar estad\u00edsticas descriptivas\nprint(\"\\nEstad\u00edsticas b\u00e1sicas:\")\nprint(df.describe())\n\n# Edad promedio\nprint(f\"\\nEdad promedio: {df['Edad'].mean():.2f} a\u00f1os\")\n</code></pre> </li> <li> <p>Antes de ejecutar no se olvide de seleccionar el interprete <code>mlops_env</code> que contiene todas las librerias necesarias. En la barra inferior izquierda de VS Code, localiza la secci\u00f3n que indica el int\u00e9rprete actual de Python.</p> <p></p> </li> </ol>"},{"location":"entorno/p5/#como-compartir-el-entorno-con-conda","title":"C\u00f3mo compartir el entorno con Conda","text":"<p>En un proyecto colaborativo, es fundamental que todos los miembros trabajen en un entorno de desarrollo id\u00e9ntico. Para lograrlo, se utiliza un archivo <code>environment.yml</code>, que documenta la configuraci\u00f3n exacta del entorno Conda (incluyendo Python y todas las librer\u00edas instaladas). Este archivo facilita la reproducci\u00f3n del entorno en diferentes equipos y evita errores por incompatibilidades.</p> <p>\ud83d\udcdd Esta explicaci\u00f3n fue introducida en la pr\u00e1ctica Ambiente de Desarrollo / Gestor de Paquetes.</p>"},{"location":"entorno/p5/#tarea_1","title":"\ud83d\udee0\ufe0f Tarea","text":"<p>Como l\u00edder del proyecto, una vez que has finalizado la implementaci\u00f3n y verificado que todo funciona correctamente, es momento de compartir el entorno de Conda con el resto del equipo. Para ello, debes exportar el entorno a un archivo <code>environment.yml</code>. Este archivo permitir\u00e1 que tus compa\u00f1eros puedan reproducir las mismas condiciones de trabajo en sus propias m\u00e1quinas y continuar con el desarrollo sin contratiempos.</p> <ul> <li>\u00bfQu\u00e9 comando usar\u00edas para generar este archivo?</li> <li> <p>Si revisas el archivo <code>environment.yml</code>, ver\u00e1s que contiene muchos paquetes, pero en este ejercicio solo hemos instalado dos de forma expl\u00edcita:</p> <ul> <li>python=3.11 al momento de configurar el entorno.</li> <li>pandas=1.5.3 para implementar las estad\u00edsticas del proyecto.</li> </ul> <p>\u00bfQu\u00e9 comando usar para exportar \u00fanicamente los paquetes que instalaste directamente? (y no todas las dependencias internas que Conda a\u00f1adi\u00f3 autom\u00e1ticamente).</p> </li> </ul> <p>\ud83d\udcdd Una vez que el archivo <code>environment.yml</code> ha sido creado y compartido, cada colaborador debe clonar el repositorio del proyecto (este proceso se detalla en la pr\u00e1ctica: Versionado de C\u00f3digo), ubicarse en la ra\u00edz del proyecto y ejecutar el siguiente comando para recrear el entorno Conda de forma id\u00e9ntica: <pre><code>conda env create -f environment.yml\n</code></pre> Esto crea un nuevo entorno con el mismo nombre y configuraci\u00f3n que el original.</p> <p>\ud83d\udca1 Consejo: Si deseas usar un nombre distinto para el entorno, puedes usar: <pre><code> conda env create -f environment.yml -n &gt;nombre_personalizado\n</code></pre></p> <p>Una vez creado el entorno, act\u00edvalo con: <pre><code>conda activate nombre_entorno\n</code></pre> As\u00ed, todos los integrantes del equipo estar\u00e1n trabajando en condiciones id\u00e9nticas.</p>"},{"location":"entorno/p6/","title":"Estructura del Proyecto","text":"<p>En esta pr\u00e1ctica, estableceremos la base para un proyecto de predicci\u00f3n de precios de consignaci\u00f3n utilizando herramientas modernas de MLOps. Una vez configurado el entorno de trabajo, el siguiente paso es generar la estructura del proyecto. Para ello, utilizaremos Cookiecutter, una herramienta que permite clonar plantillas con buenas pr\u00e1cticas predefinidas, facilitando as\u00ed una organizaci\u00f3n clara y escalable desde el inicio.</p>"},{"location":"entorno/p6/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<ul> <li>Instalar y usar Cookiecutter desde la terminal.</li> <li>Clonar un template base de proyecto MLOps.</li> <li>Explorar e interpretar la estructura generada.</li> </ul>"},{"location":"entorno/p6/#requisitos-previos","title":"Requisitos previos","text":"<ul> <li>Tener instalado y configurado WSL o Linux.</li> <li>Tener instalado Conda (miniconda o anaconda).</li> </ul>"},{"location":"entorno/p6/#estructura-del-proyecto","title":"Estructura del Proyecto","text":"<p>Para comenzar con la configuraci\u00f3n de un proyecto MLOPS, existen varias formas de crear la carpeta principal</p> <ul> <li> <p>Crear desde cero: Opci\u00f3n directa que implica definir manualmente la estructura y archivos del proyecto. No recomendada para proyectos medianos o grandes debido a su dificultad de mantenimiento y escalabilidad.</p> </li> <li> <p>Importar una plantilla existente (recomendada): Permite estructurar el proyecto de forma eficiente, facilita la colaboraci\u00f3n, y mejora la transparencia, reproducibilidad y reutilizaci\u00f3n. Es la opci\u00f3n sugerida para esta pr\u00e1ctica.</p> </li> <li> <p>Clonar o hacer fork de un proyecto existente: Ideal para continuar o adaptar proyectos ya desarrollados. Soporta colaboraci\u00f3n activa y reutilizaci\u00f3n de c\u00f3digo. Ser\u00e1 usada m\u00e1s adelante en el curso.</p> </li> </ul>"},{"location":"entorno/p6/#instalacion-de-cookiecutter","title":"Instalaci\u00f3n de Cookiecutter","text":"<p>En esta pr\u00e1ctica emplearemos una estructura de proyecto basada en una plantilla de MLOps que puede ser generada utilizando la herramienta cookiecutter. Usaremos el repositorio Cookiecutter MLOps, el cual est\u00e1 dise\u00f1ado espec\u00edficamente para proyectos de aprendizaje autom\u00e1tico que requieren una buena organizaci\u00f3n desde el inicio. </p> <ul> <li> <p>Abre una terminal para crear la carpeta <code>Practica-6</code> dentro de <code>Practicas-MLOPS</code>. En esta carpeta se crear\u00e1 la estructura del proyecto.</p> </li> <li> <p>En la misma terminal, instala <code>cookiecutter</code>. Hay dos opciones:</p> <pre><code>pip3 install cookiecutter\nconda install -c conda-forge cookiecutter\n</code></pre> <p>IMPORTANTE</p> <p>Se recomienda instalar herramientas como <code>cookiecutter</code> desde el canal <code>conda-forge</code>, ya que este canal suele ofrecer versiones m\u00e1s actualizadas, bien mantenidas y con mejor compatibilidad entre paquetes en comparaci\u00f3n con <code>pip</code> o con el canal por defecto de Conda (defaults). Esto ayuda a evitar conflictos y facilita la gesti\u00f3n del entorno.</p> </li> <li> <p>Para clonar la plantilla que contiene la estructura del proyecto puede usar el siguiente comando:</p> <pre><code>cookiecutter https://github.com/Chim-SO/cookiecutter-mlops\n</code></pre> </li> <li> <p>Durante la generaci\u00f3n del proyecto con la plantilla de Cookiecutter, se te pedir\u00e1 que proporciones varios par\u00e1metros de configuraci\u00f3n. Responde a las preguntas que aparecen en consola. Si no deseas modificar un par\u00e1metro, simplemente presiona Enter para aceptar su valor predeterminado. La tabla adjunta tiene un ejemplo de c\u00f3mo llenar los valores.</p> Pregunta Ejemplo de respuesta project_name ProyectoMLOPS repo_name [project_name] author_name Mauricio_Espinoza description Proyecto de predicci\u00f3n usando MLOps Select open_source_license:1 - MIT2 - BSD-3-Clause3 - No license fileChoose from 1, 2, 3 [1]: 1 s3_bucket [[OPTIONAL] your-bucket-for-syncing-data (do not include 's3://')]: aws_profile [default]: Select python_interpreter:1 - python32 - pythonChoose from 1, 2 [1]: <p>Nota1:</p> <p>El campo s3_bucket [[OPTIONAL] your-bucket-for-syncing-data (do not include 's3://')]: es opcional y se refiere a un bucket de Amazon S3 que puedes usar para sincronizar datos de tu proyecto, como archivos de entrenamiento, modelos o resultados. Si tu proyecto no utilizar\u00e1 almacenamiento en la nube (o a\u00fan no tienes un bucket configurado), puedes dejar este campo en blanco. Solo presiona Enter para aceptar el valor predeterminado y omitir esta configuraci\u00f3n por ahora.</p> <p>Nota2:</p> <p>El campo aws_profile [default]: se refiere al perfil de configuraci\u00f3n de AWS que se puede usar para interactuar con servicios en la nube, como Amazon S3. Estos perfiles se definen en el archivo ~/.aws/credentials y permiten gestionar distintas cuentas o configuraciones de acceso a AWS desde la misma m\u00e1quina. Si ya tienes configurado un perfil llamado \u201cdefault\u201d (que es el caso m\u00e1s com\u00fan), puedes simplemente presionar Enter para aceptarlo. Si usas otro perfil, escribe su nombre aqu\u00ed. Si a\u00fan no has configurado ning\u00fan perfil de AWS o no planeas usar servicios de AWS en este proyecto, tambi\u00e9n puedes dejarlo en blanco presionando Enter.</p> </li> <li> <p>Se debe haber creado una carpeta con el nombre del proyecto asignado. Ingresa a dicha carpeta y prueba abrirla en VS Code utilizando el comando <code>code .</code> desde la terminal.</p> </li> </ul>"},{"location":"entorno/p6/#tarea","title":"\ud83d\udee0\ufe0f Tarea","text":"<p>Sup\u00f3n que has descubierto recientemente la herramienta Cookiecutter, la cual permite generar estructuras de proyecto preconfiguradas a partir de plantillas que siguen buenas pr\u00e1cticas de desarrollo. Deseas aprovechar esta herramienta para organizar adecuadamente tu proyecto de Machine Learning y facilitar el trabajo colaborativo.</p> <p>Para continuar con la implementaci\u00f3n del proyecto, decides reutilizar el entorno Conda creado previamente en la pr\u00e1ctica Creaci\u00f3n del Entorno Virtual, ya que en \u00e9l se encuentran instaladas las dependencias principales como Python y pandas. A partir de ese entorno, continuar\u00e1s trabajando para estructurar el proyecto correctamente. </p> <p>\u00bfEs necesario instalar Cookiecutter dentro del entorno Conda a pesar que esta herramienta solo se utiliza al inicio del proyecto para generar la estructura y no forma parte del c\u00f3digo fuente que se ejecutar\u00e1? Justifica razonadamente tu respuesta.</p>"},{"location":"entorno/p6/#explorar-la-estructura-del-proyecto","title":"Explorar la estructura del proyecto","text":"<p>Revisa la estructura del proyecto usando VS Code, deber\u00e1s encontrar estas carpetas:</p> <ul> <li> <p><code>configs/</code>: Esta carpeta puede contener archivos de   configuraci\u00f3n, como los hiperpar\u00e1metros del modelo. Aqu\u00ed se definen variables clave para la experimentaci\u00f3n y entrenamiento, y facilita modificar la configuraci\u00f3n sin alterar el c\u00f3digo fuente.</p> </li> <li> <p><code>src/data/</code>: Esta subcarpeta agrupa todos los pasos del procesamiento de datos:</p> <ul> <li><code>ingestion.py</code>: se encarga de reunir los datos. Aqu\u00ed tambi\u00e9n puedes a\u00f1adir funcionalidades como respaldos, anonimizaci\u00f3n de datos sensibles o generaci\u00f3n de metadatos.</li> <li><code>cleaning.py</code>: limpia los datos, reduce el ruido y maneja valores at\u00edpicos o faltantes.</li> <li><code>labeling.py</code>: asigna etiquetas a los datos si es necesario.</li> <li><code>splitting.py</code>: divide los datos en conjuntos de entrenamiento y prueba.</li> <li><code>validation.py</code>: valida que los datos est\u00e9n completos y en formato adecuado para el entrenamiento.</li> <li><code>build_features.py</code>: organiza y transforma los datos en caracter\u00edsticas \u00fatiles para el modelo.</li> </ul> </li> <li> <p><code>src/models/</code>: Contiene los scripts espec\u00edficos del modelo.</p> <ul> <li><code>model.py</code>: define la arquitectura del modelo.</li> <li><code>dataloader.py</code>: prepara los datos para ser usados en el modelo.</li> <li><code>preprocessing.py</code>: incluye funciones de preprocesamiento espec\u00edficas de este modelo</li> <li><code>train.py</code>: entrena el modelo con los datos.</li> <li><code>hyperparameters_tuning.py</code>: ajusta los hiperpar\u00e1metros del modelo.</li> <li><code>predict.py</code>: realiza predicciones sobre nuevas instancias (por ejemplo, im\u00e1genes) externas al conjunto de datos original.</li> </ul> </li> <li> <p><code>src/visualization/</code></p> <ul> <li><code>exploration.py</code>: visualiza los datos para entender su distribuci\u00f3n y caracter\u00edsticas durante la etapa de ingenier\u00eda.</li> <li><code>evaluation.py</code>: visualiza resultados del entrenamiento y ayuda a interpretar el rendimiento del modelo.</li> </ul> </li> </ul> <p>Notas importantes</p> <ul> <li>Esta plantilla es b\u00e1sica y puede adaptarse seg\u00fan las necesidades del proyecto: puedes eliminar o a\u00f1adir carpetas y scripts.</li> <li>Si varias arquitecturas de modelo usan el mismo preprocesamiento, puedes mover el archivo <code>preprocessing.py</code> a la carpeta <code>data</code> para evitar duplicaci\u00f3n, aunque mantenerlo separado puede facilitar la reutilizaci\u00f3n y evitar errores.</li> <li>El script <code>predict.py</code> asume que la predicci\u00f3n se hace sobre datos externos a la base original (como los enviados por una app), por lo que puede requerir pasos adicionales de preprocesamiento.</li> </ul>"},{"location":"entorno/p6/#tarea_1","title":"\ud83d\udee0\ufe0f Tarea","text":"<ul> <li> <p>Responde las preguntas propuestas en la plataforma virtual sobre la estructura de la plantilla bas\u00e1ndote en tu criterio t\u00e9cnico. Puedes apoyarte en la documentaci\u00f3n proporcionada por el autor en el repositorio del proyecto en GitHub para justificar tus respuestas.</p> </li> <li> <p>Copia el archivo <code>estadisticas.py</code> generado en la pr\u00e1ctica Creaci\u00f3n del Entorno Virtual dentro de una de las carpetas del proyecto creado con Cookiecutter. \u00bfEn donde deber\u00eda alojarse este archivo? Justifica tu respuesta.</p> </li> <li> <p>Una vez copiado el archivo, accede al mismo y verifica que se ejecuta correctamente desde la terminal o directamente desde VS Code, asegur\u00e1ndote de que est\u00e9 activado el entorno Conda correspondiente.</p> </li> <li> <p>Copia el archivo <code>environment.yml</code> generado en la pr\u00e1ctica anterior a la ra\u00edz del proyecto. \u00bfQu\u00e9 comando puedes utilizar para actualizar el entorno Conda actual?</p> </li> <li> <p>Verifica que el archivo <code>environment.yml</code> contiene ahora tres librerias.</p> <ul> <li>python=3.11</li> <li>pandas=1.5.3</li> <li>cookiecuttier</li> </ul> </li> </ul>"},{"location":"entorno/p6/#control-de-versiones-y-github","title":"Control de versiones y GitHub","text":"<p>Esta estructura de proyecto que has creado y organizado usando la plantilla Cookiecutter, puede ser f\u00e1cilmente integrado con un sistema de control de versiones como Git, y alojado en una plataforma como GitHub. Esto te permitir\u00e1 continuar con el desarrollo del proyecto, mantener un historial de cambios, y colaborar con otros de manera m\u00e1s efectiva.</p> <p>Nota informativa</p> <p>A continuaci\u00f3n, se describen los pasos y comandos esenciales para subir tu proyecto local a un nuevo repositorio vac\u00edo en GitHub. No los ejecutes a\u00fan, ya que se utilizar\u00e1n m\u00e1s adelante en otra pr\u00e1ctica.</p> <ol> <li>Inicializa un repositorio local (si a\u00fan no lo hiciste): <code>git init</code></li> <li>Agrega los archivos al repositorio: <code>git add .</code></li> <li>Crea un commit inicial: <code>git commit -m \"Primer commit del proyecto de predicci\u00f3n de precios de consignaci\u00f3n\"</code></li> <li>Agrega la URL de tu repositorio remoto (c\u00e1mbiala por la tuya): <code>git remote add origin https://github.com/tu-usuario/tu-repo.git</code></li> <li>Sube tu proyecto: <code>git push -u origin main</code></li> </ol> <p>Si el branch por defecto en tu GitHub es main, aseg\u00farate de estar en ese branch localmente usando <code>git branch -M main.</code></p> <p>En la pr\u00e1ctica Versionado del C\u00f3digo, en lugar de crear un proyecto desde cero, trabajaremos clonando un proyecto existente en GitHub basado en esta misma plantilla. Esto refleja un flujo de trabajo com\u00fan en la industria, donde se parte de un repositorio compartido para contribuir con nuevas funcionalidades, en este caso, enfocadas en implementar el proceso de predicci\u00f3n.</p>"},{"location":"entrenamiento/p7/","title":"Versionado del C\u00f3digo","text":"<p>En esta pr\u00e1ctica trabajaremos con Git y GitHub para simular un entorno colaborativo en el desarrollo de un proyecto de Machine Learning para la Predicci\u00f3n de Precios en Art\u00edculos de Consignaci\u00f3n (ver detalles en AI/ML canvas). Cada estudiante trabajar\u00e1 con su propia copia del repositorio inicial, desarrollar\u00e1 funcionalidades en una rama independiente y colaborar\u00e1 con sus compa\u00f1eros para integrar esos cambios al proyecto principal.</p> <p>Es importante destacar que la colaboraci\u00f3n en proyectos de programaci\u00f3n requiere una forma de trabajo organizada sobre una misma base de c\u00f3digo. Para lograrlo, se utilizan los sistemas de control de versiones, y Git es uno de los m\u00e1s utilizados en la actualidad.</p> <p>Git permite registrar de forma clara:</p> <ul> <li>Qui\u00e9n hizo un cambio en el c\u00f3digo.</li> <li>Cu\u00e1ndo se realiz\u00f3 ese cambio.</li> <li>Qu\u00e9 se modific\u00f3 exactamente.</li> </ul> <p>Estas capacidades facilitan el trabajo en equipo, permiten mantener un historial completo de cambios y hacen posible experimentar sin poner en riesgo la versi\u00f3n principal del proyecto.</p> <p>Para profundizar m\u00e1s en el uso de Git, puedes consultar el libro oficial de Git o explorar tutoriales interactivos disponibles en l\u00ednea.</p>"},{"location":"entrenamiento/p7/#git-vs-github","title":"Git vs GitHub","text":"<p>Es fundamental distinguir entre Git y GitHub:</p> <ul> <li>Git es la herramienta de control de versiones.</li> <li>GitHub es una plataforma web que permite alojar repositorios Git, colaborar con otros desarrolladores y revisar cambios desde una interfaz gr\u00e1fica.</li> </ul> <p>Aunque GitHub es la plataforma m\u00e1s popular, tambi\u00e9n existen otras alternativas como GitLab, Bitbucket o servidores privados.</p>"},{"location":"entrenamiento/p7/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<p>Al finalizar esta pr\u00e1ctica, deber\u00e1s ser capaz de:</p> <ul> <li>Clonar y configurar correctamente un repositorio remoto de GitHub.</li> <li>Crear y trabajar en ramas para desarrollar en paralelo.</li> <li>Subir y sincronizar tus cambios con el repositorio remoto.</li> <li>Resolver conflictos de integraci\u00f3n cuando existan.</li> </ul>"},{"location":"entrenamiento/p7/#limpieza-de-entornos-conda-no-utilizados","title":"\ud83e\uddf9 Limpieza de Entornos Conda No Utilizados","text":"<p>A lo largo de las pr\u00e1cticas anteriores, se han introducido y aplicado los conceptos fundamentales para desarrollar un proyecto de Machine Learning de forma profesional, incluyendo la creaci\u00f3n de entornos virtuales, la gesti\u00f3n de dependencias y la organizaci\u00f3n del c\u00f3digo. Estos entornos han sido clave para asegurar la reproducibilidad y el aislamiento de cada fase del desarrollo.</p> <p>Sin embargo, como resultado del trabajo pr\u00e1ctico, es probable que se hayan generado varios entornos Conda que ya no ser\u00e1n utilizados en las siguientes etapas del proyecto. Estos entornos contin\u00faan ocupando espacio en disco y consumiendo recursos del sistema, aunque no est\u00e9n en uso.</p> <p>Por este motivo, se recomienda eliminar los entornos que ya no se necesitan. Esta es una pr\u00e1ctica saludable para mantener tu sistema limpio, organizado y con espacio disponible, especialmente cuando trabajas con m\u00faltiples proyectos que requieren distintos entornos.</p> <p>Adem\u00e1s, la limpieza regular de entornos ayuda a evitar confusiones y errores derivados del uso accidental de entornos obsoletos.</p>"},{"location":"entrenamiento/p7/#tarea","title":"\ud83d\udee0\ufe0f Tarea","text":"<p>Responde las siguientes preguntas en la plataforma virtual:</p> <ul> <li>\u00bfC\u00f3mo listar los entornos existentes?</li> <li>\u00bfC\u00f3mo conocer todos los entornos creados y su ubicaci\u00f3n?</li> <li>\u00bfC\u00f3mo conocer el entorno activo?</li> <li>\u00bfC\u00f3mo eliminar un entorno que ya no usas?</li> </ul> <p>Usando esta informacion puedes eliminar los entornos:</p> <ul> <li>mi_entorno</li> <li>mi_entorno_copia</li> <li>mlops_env</li> </ul> <p>\ud83d\udd0d Consejo adicional</p> <p>Despu\u00e9s de eliminar entornos, tambi\u00e9n puedes liberar espacio limpiando paquetes en cach\u00e9 con:       <pre><code>conda clean --all\n</code></pre></p> <p>Este comando:</p> <ul> <li>Elimina paquetes descargados que ya no se usan.</li> <li>Borra archivos de instalaci\u00f3n temporales.</li> <li>Libera espacio ocupado por versiones antiguas.</li> </ul> <p>\u26a0\ufe0f Revisa lo que se va a eliminar antes de aceptar: te pedir\u00e1 confirmaci\u00f3n.</p>"},{"location":"entrenamiento/p7/#inicio-del-proyecto","title":"Inicio del Proyecto","text":"<p>Se asume que el responsable del proyecto ya ha iniciado la configuraci\u00f3n inicial, ejecutando tareas previas como la i) definici\u00f3n del entorno de paquetes y librer\u00edas, ii) la estructuraci\u00f3n del c\u00f3digo del proyecto (seg\u00fan lo trabajado en las pr\u00e1cticas Creaci\u00f3n del Entorno Virtual y Estructura del Proyecto) y iii) la implementaci\u00f3n de algunas etapas clave del proceso de ingenier\u00eda de datos, como la ingesta, limpieza y etiquetado de los datos necesarios para entrenar el modelo de predicci\u00f3n. Con esta base establecida, el proyecto ha sido compartido con los colaboradores para continuar con su implementaci\u00f3n de forma colaborativa.</p>"},{"location":"entrenamiento/p7/#escenario","title":"Escenario","text":"<p>En esta pr\u00e1ctica se simula una colaboraci\u00f3n entre dos personas trabajando sobre un mismo proyecto de Machine Learning, utilizando Git y GitHub como herramientas de control de versiones y coordinaci\u00f3n.</p>"},{"location":"entrenamiento/p7/#roles","title":"Roles","text":"<ul> <li> <p>Persona A ser\u00e1 responsable de desarrollar un script que, a partir de un archivo de datos previamente procesado (en el que ya se han eliminado los valores at\u00edpicos, los datos nulos y los faltantes), realice la divisi\u00f3n del conjunto de datos en entrenamiento y prueba.</p> </li> <li> <p>Persona B trabajar\u00e1 en paralelo desarrollando un script independiente que utilice los datos ya divididos para entrenar y validar un modelo de Machine Learning.</p> </li> </ul> <p>Cada persona trabajar\u00e1 en su propia rama de desarrollo, y al finalizar sus tareas, integrar\u00e1n sus aportes al proyecto principal mediante GitHub, siguiendo buenas pr\u00e1cticas de colaboraci\u00f3n y control de versiones.</p>"},{"location":"entrenamiento/p7/#requisitos-previos-para-cada-persona","title":"Requisitos previos: para cada persona","text":"<p>Antes de comenzar la pr\u00e1ctica, aseg\u00farate de cumplir con los siguientes requisitos. Si a\u00fan no los tienes listos, aqu\u00ed te explicamos c\u00f3mo cumplirlos:</p>"},{"location":"entrenamiento/p7/#1-tener-una-cuenta-de-github","title":"1. Tener una cuenta de GitHub","text":"<p>Pasos para crear una cuenta:</p> <ul> <li>Ingresa a https://github.com</li> <li>Haz clic en Sign up (Registrarse).</li> <li>Completa los datos solicitados:<ul> <li>Nombre de usuario</li> <li>Correo electr\u00f3nico</li> <li>Contrase\u00f1a segura</li> </ul> </li> <li>Sigue las instrucciones para verificar tu correo.</li> <li>Puedes elegir el plan gratuito (Free plan) para comenzar.</li> </ul> <p>Una vez creada tu cuenta, podr\u00e1s clonar repositorios, crear ramas, hacer cambios y colaborar con otras personas.</p>"},{"location":"entrenamiento/p7/#2-tener-git-instalado","title":"2. Tener Git instalado","text":"<p>Pasos para instalar Git:</p> <ul> <li> <p>Abre una terminal y ejecuta los siguientes comandos:</p> <pre><code>sudo apt update\nsudo apt install git\n</code></pre> </li> <li> <p>Verifica la instalaci\u00f3n con:</p> <pre><code>git \u2013-version\n</code></pre> </li> </ul>"},{"location":"entrenamiento/p7/#3-configurar-acceso-remoto-a-github-opcional","title":"3. Configurar acceso remoto a GitHub [opcional]","text":"<p>Aunque este requisito es opcional, se recomienda cumplirlo para facilitar el trabajo con Git. Su principal ventaja es que evita tener que ingresar tu usuario y contrase\u00f1a cada vez que realizas un <code>git push</code>. Para lograrlo, puedes configurar una conexi\u00f3n segura con GitHub utilizando una llave SSH.</p> <p>IMPORTANTE</p> <p>Si decides no usar el m\u00e9todo de conexi\u00f3n segura debes configurar Git para usar tu usuario de GitHub. De esta forma <code>git</code> sabr\u00e1 quien es la persona autor de los <code>git commit</code>. El resto de pasos no son necesarios.</p> <pre><code>git config --global user.name \"Tu Nombre Usuario\"\ngit config --global user.email \"tu-email@ejemplo.com\"\n</code></pre> <ul> <li> <p>Verificar si ya tienes una clave SSH. Abre tu terminal y ejecuta:       <pre><code>ls ~/.ssh\n</code></pre>       Si ves archivos como id_rsa y id_rsa.pub o id_ed25519, ya tienes una clave SSH. Si no, crea una.</p> </li> <li> <p>Generar una nueva llave SSH:</p> <p><pre><code>ssh-keygen -t rsa -b 4096 -C \"tucorreo@example.com\"\n</code></pre>   Ingrese el nombre del archivo para grabar la clave o presione ENTER para usar el nombre por omisi\u00f3n. Puede omitir el ingreso de una passphrase (o frase de paso).</p> </li> <li> <p>Mostrar la clave p\u00fablica:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> </li> <li> <p>Copiar la clave y agregarla en GitHub:</p> <ul> <li>Ir a Settings \u2192 SSH and GPG keys \u2192 New SSH Key.</li> <li>Pegar la clave en el campo correspondiente</li> </ul> </li> <li> <p>Configurar Git para usar tu usuario de GitHub</p> <pre><code>git config --global user.name \"Tu Nombre Usuario\"\ngit config --global user.email \"tu-email@ejemplo.com\"\n</code></pre> </li> <li> <p>Probar conexi\u00f3n</p> <p><pre><code>ssh -T git@github.com\n</code></pre> Si todo est\u00e1 bien, ver\u00e1s algo como:</p> <p>The authenticity of host '[ssh.github.com]:443 ([140.82.112.36]:443)' can't be established.   XXXXXXX key fingerprint is SHA256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.</p> <p>This key is not known by any other names.</p> <p>Are you sure you want to continue connecting (yes/no/[fingerprint])? yes</p> <p>Hi tu_usuario! You've successfully authenticated.</p> <p>En caso de no poder conectarse y obtener un mensaje como este :</p> <p>ssh: connect to host github por un error de timed out port 22: Connection timed out. </p> <ul> <li>Puedes configurar Git a usar otro puerto (Ej. 443) para SSH. </li> <li>Edita (o crea) el archivo ~/.ssh/config:</li> </ul> <pre><code>nano ~/.ssh/config\n</code></pre> <ul> <li>Agrega este contenido</li> </ul> <pre><code>Host github.com\nHostname ssh.github.com\nPort 443\nUser git\n</code></pre> <ul> <li>Guarda y prueba otra vez.</li> </ul> <pre><code>ssh -T git@github.com\n</code></pre> </li> </ul> <p>IMPORTANTE</p> <p>Por omisi\u00f3n, el acceso remoto en Git a plataformas como GitHub, GitLab o Bitbucket es mediante HTTPS. Esto ocurre porque:</p> <ul> <li>Es m\u00e1s sencillo de configurar inicialmente (no requiere claves SSH).</li> <li>Cualquier usuario con nombre de usuario y contrase\u00f1a (o token personal) puede clonar un repositorio.</li> </ul> <p>Sin embargo, SSH es la opci\u00f3n recomendada para mayor seguridad y comodidad, especialmente si trabajas frecuentemente con Git, ya que:</p> <ul> <li>Evita tener que introducir el usuario/contrase\u00f1a o token en cada operaci\u00f3n.</li> <li>Utiliza autenticaci\u00f3n basada en claves, que es m\u00e1s segura</li> </ul> <p>Para cambiar el acceso remoto de HTTPS a SSH, debes actualizar la URL del repositorio remoto una sola vez con el siguiente comando:</p> <pre><code>git remote set-url origin git@github.com:TU-USUARIO/Proyecto-MLOPS.git\n</code></pre> <p>Esto configura el repositorio para usar autenticaci\u00f3n SSH en lugar de HTTPS en futuras operaciones como <code>git push</code> o <code>git pull</code>.</p>"},{"location":"entrenamiento/p7/#configuracion-del-repositorio-de-equipo","title":"Configuraci\u00f3n del repositorio de equipo","text":"<p>Uno de los estudiantes (Persona A o Persona B) realizar\u00e1 un fork del repositorio base proporcionado por el docente (Responsable del Proyecto). Un fork en GitHub es una copia de un repositorio que se crea en tu propia cuenta. Te permite experimentar y trabajar en un proyecto sin afectar el original.</p>"},{"location":"entrenamiento/p7/#pasos-para-hacer-el-fork","title":"Pasos para hacer el fork:","text":"<ol> <li> <p>Ir al repositorio original (del docente) en GitHub https://github.com/jmem-ec/Proyecto-MLOPS-Base. </p> </li> <li> <p>Hacer clic en el bot\u00f3n \"Fork\" en la parte superior derecha.</p> </li> <li> <p>GitHub crear\u00e1 una copia del repositorio en la cuenta de la persona, por ejemplo: </p> <p><code>https://github.com/Persona_A/Proyecto-MLOPS</code></p> <p>Se recomienda cambiar el nombre del repositorio de Proyecto-MLOPS-Base a Proyecto-MLOPS, ya que este repositorio servir\u00e1 como base principal para el desarrollo del proyecto de MLOps a lo largo de todo el curso.</p> </li> </ol> <p>Esta persona se convertir\u00e1 en el \"due\u00f1o\" del fork, es decir, el responsable principal de esta copia del proyecto. A partir de aqu\u00ed:</p> <ul> <li> <p>Debe invitar a sus compa\u00f1eros como colaboradores del repositorio:</p> <ul> <li>Ir a Settings &gt; Collaborators</li> <li>Hacer clic en \"Add collaborator\"</li> <li>Ingresar los nombres de usuario de GitHub de los compa\u00f1eros para que puedan clonar, crear ramas, hacer cambios y subirlos</li> </ul> </li> </ul> <p>\ud83d\udcdd Nota explicativa: El objetivo de realizar un fork y no empezar desde cero es aprovechar una base inicial de c\u00f3digo ya estructurada, lo cual es muy com\u00fan en proyectos reales. Por ejemplo, un equipo puede comenzar a desarrollar un sistema de Machine Learning a partir de un repositorio de prueba, una plantilla anterior o incluso un proyecto experimental del responsable t\u00e9cnico (en este caso, el docente). A partir de ese punto de partida, los estudiantes comienzan a construir, adaptar y colaborar en su propia versi\u00f3n del proyecto.</p>"},{"location":"entrenamiento/p7/#clonar-el-repositorio-del-equipo","title":"Clonar el repositorio del equipo","text":"<p>Para comenzar a trabajar, cada estudiante \u2014incluido quien haya realizado el fork\u2014 debe clonar en su equipo local el repositorio que se gener\u00f3 a partir del fork. Este ser\u00e1 su repositorio de trabajo a lo largo del curso.</p> <pre><code>git clone https://github.com/Persona_A/Proyecto-MLOPS\ncd Proyecto-MLOPS\n</code></pre>"},{"location":"entrenamiento/p7/#crear-y-activar-el-entorno-conda","title":"Crear y activar el entorno <code>conda</code>","text":"<p>Cada estudiante debe crear y activar el entorno <code>conda</code> a partir del archivo <code>environment.yml</code></p>"},{"location":"entrenamiento/p7/#tarea_1","title":"\ud83d\udee0\ufe0f Tarea","text":"<ul> <li>\u00bfPuedes identificar los cambios realizados sobre la plantilla base de MLOps (utilizada en la pr\u00e1ctica Estructura del Proyecto) que han sido implementados por el responsable en este nuevo proyecto?</li> <li>Al revisar el c\u00f3digo implementado, \u00bfcu\u00e1l es el nombre del archivo que contiene los datos fuente utilizados para el entrenamiento del modelo?</li> <li>\u00bfEl entorno creado en que ruta est\u00e1 almacenado?</li> </ul>"},{"location":"entrenamiento/p7/#tareas-del-equipo-de-desarrollo","title":"Tareas del equipo de desarrollo","text":"<p>Una vez que cada persona del equipo ha clonado y activado el entorno del proyecto, debe ejecutar las siguientes tareas de implementaci\u00f3n de forma paralela. Cada integrante trabajar\u00e1 en su propia rama para luego integrar los cambios al repositorio principal.</p>"},{"location":"entrenamiento/p7/#persona-a-division-de-datos","title":"Persona A: Divisi\u00f3n de datos","text":"<ol> <li> <p>Crear y cambiar a una nueva rama:</p> <pre><code>git checkout -b division-datos\n</code></pre> </li> <li> <p>Agregar el archivo <code>stage4_splitting.py</code> en la carpeta  <code>src/data_eng</code>.</p> <p>\ud83d\udee0\ufe0f Tarea</p> <p>Responda algunas preguntas en la plataforma virtual.</p> </li> <li> <p>Guardar los cambios y subirlos al repositorio remoto:</p> <pre><code>git add src/data_eng/stage4_splitting.py\ngit commit -m \"Agrega script para dividir datos en train/test\"\ngit push origin division-datos\n</code></pre> </li> </ol>"},{"location":"entrenamiento/p7/#persona-b-entrenamiento-del-modelo","title":"Persona B: Entrenamiento del Modelo","text":"<ol> <li> <p>Crear y cambiar a una nueva rama:</p> <pre><code>git checkout -b entrenamiento-modelo\n</code></pre> </li> <li> <p>Agregar el archivo <code>stage1-2_train-evaluate.py</code> en la carpeta  <code>src/model_eng/model1</code>.</p> <p>\ud83d\udee0\ufe0f Tarea</p> <p>Responda algunas preguntas en la plataforma virtual.</p> </li> <li> <p>Guardar los cambios y subirlos al repositorio remoto:</p> <pre><code>git add src/model_eng/model1/stage1-2_train-evaluate.py\ngit commit -m \"Agrega script de entrenamiento y validaci\u00f3n del modelo\"\ngit push origin entrenamiento-modelo\n</code></pre> </li> </ol>"},{"location":"entrenamiento/p7/#persona-a-o-b-integrar-ramas-en-main","title":"Persona A \u00f3 B: Integrar ramas en <code>main</code>","text":"<p>Una vez que las tareas de cada persona est\u00e1n completas y no contiene errores, se debe integrar el trabajo en la rama principal (<code>main</code>). Esto lo puede hacer una persona responsable del proyecto o el equipo siguiendo estos pasos:</p> <pre><code>git checkout main\ngit pull origin main\ngit merge division-datos\ngit merge entrenamiento-modelo\ngit push origin main\n</code></pre>"},{"location":"entrenamiento/p7/#persona-a-o-b-prediccion-del-modelo","title":"Persona A \u00f3 B: Predicci\u00f3n del Modelo","text":"<ol> <li> <p>Crear y cambiar a una nueva rama:</p> <pre><code>git checkout -b prediccion-modelo\n</code></pre> </li> <li> <p>Agregar el archivo <code>stage3_prediction.py</code> en la carpeta  <code>src/model_eng/model1</code>.</p> <p>\ud83d\udee0\ufe0f Tarea</p> <p>Responda algunas preguntas en la plataforma virtual.</p> </li> <li> <p>Guardar los cambios y subirlos al repositorio remoto:</p> <pre><code>git add src/model_eng/model1/stage3_prediction.py\ngit commit -m \"Agrega script de predicci\u00f3n del modelo\"\ngit push origin prediccion-modelo\n</code></pre> </li> <li> <p>Integrar el trabajo en la rama principal (<code>main</code>).</p> </li> </ol>"},{"location":"entrenamiento/p7/#actualizar-el-codigo-local-de-cada-persona","title":"Actualizar el c\u00f3digo local de cada persona","text":"<p>Una vez que las ramas han sido integradas en la rama principal del repositorio (main), cada integrante del equipo debe actualizar su copia local para trabajar con la versi\u00f3n m\u00e1s reciente del proyecto. A continuaci\u00f3n, se indican los pasos recomendados para hacerlo correctamente.</p> <ol> <li> <p>Cambiar a la rama <code>main</code> (si a\u00fan no est\u00e1n en ella)</p> <pre><code>git checkout main\n</code></pre> </li> <li> <p>Actualizar su rama <code>main</code> desde el repositorio remoto:</p> <pre><code>git pull origin main\n</code></pre> </li> </ol> <p>Esto descargar\u00e1 e integrar\u00e1 todos los cambios m\u00e1s recientes realizados por otros (como las integraciones de ramas que hiciste).</p>"},{"location":"entrenamiento/p8/","title":"Empaquetado y gesti\u00f3n de dependencias","text":"<p>En el desarrollo de proyectos de Machine Learning, no basta con que el c\u00f3digo funcione en el entorno local de un desarrollador. Para que un proyecto sea escalable, mantenible y reproducible \u2014objetivos clave dentro del ciclo de vida de MLOps \u2014 es fundamental contar con una buena estrategia de empaquetado y gesti\u00f3n de dependencias.</p> <p>El empaquetado convierte un proyecto en una unidad instalable, facilitando su distribuci\u00f3n, reutilizaci\u00f3n y ejecuci\u00f3n en distintos entornos (desarrollo, pruebas, producci\u00f3n). Por su parte, la gesti\u00f3n de dependencias garantiza que todas las librer\u00edas y herramientas necesarias est\u00e9n disponibles en versiones compatibles, evitando conflictos y fallos durante la ejecuci\u00f3n del pipeline.</p> <p>En este contexto, herramientas modernas como <code>pyproject.toml</code> o <code>conda</code> cumplen un rol clave al facilitar la instalaci\u00f3n ordenada del proyecto y sus componentes.</p>"},{"location":"entrenamiento/p8/#objetivos-de-la-practica","title":"\ud83c\udfaf Objetivos de la pr\u00e1ctica","text":"<p>Esta pr\u00e1ctica tiene como finalidad que los estudiantes comprendan y apliquen los principios fundamentales del empaquetado de proyectos y la gesti\u00f3n de dependencias, elementos clave para garantizar la reproducibilidad, portabilidad y mantenibilidad de soluciones de Machine Learning en entornos reales.</p> <p>Al finalizar esta pr\u00e1ctica, el estudiante ser\u00e1 capaz de:</p> <ul> <li>Diferenciar el rol del empaquetado frente al entorno de ejecuci\u00f3n (conda/env) en el flujo de trabajo MLOps.</li> <li>Detectar errores comunes como el t\u00edpico ModuleNotFoundError, y entender c\u00f3mo evitarlos mediante un empaquetado adecuado.</li> <li>Crear y configurar el archivo pyproject.toml para definir metadatos del proyecto y declarar sus dependencias.</li> <li>Instalar el proyecto como un paquete local y verificar que los m\u00f3dulos internos puedan ser importados correctamente.</li> </ul>"},{"location":"entrenamiento/p8/#trabajo-previo","title":"Trabajo Previo","text":"<p>Antes de abordar el empaquetado y la gesti\u00f3n de dependencias, se realizaron dos pasos fundamentales que sientan las bases para un flujo de trabajo robusto en MLOps:</p> <ol> <li> <p>Estructura del Proyecto con Cookiecutter. Se utiliz\u00f3 la herramienta cookiecutter para generar una estructura de proyecto estandarizada, que incluye:</p> <ul> <li>Separaci\u00f3n clara entre c\u00f3digo fuente (src/), datos (data/) y modelos (models/) .</li> <li>Archivos auxiliares como .gitignore, README.md, y carpetas para reportes (reports/).</li> </ul> <p>Esta estructura facilita la colaboraci\u00f3n, el versionado y la escalabilidad del proyecto.</p> </li> <li> <p>Creaci\u00f3n del Entorno de Ejecuci\u00f3n. Se defini\u00f3 y cre\u00f3 un entorno virtual con Conda, especificando:</p> <ul> <li>La versi\u00f3n de Python adecuada (por ejemplo, python=3.12).</li> <li>Dependencias base como pandas, scikit-learn, cookiecutter, entre otras.</li> <li>Posible inclusi\u00f3n del entorno en un archivo environment.yml para facilitar su recreaci\u00f3n.</li> </ul> <p>Este entorno garantiza que todos los participantes trabajen bajo las mismas condiciones de ejecuci\u00f3n, evitando errores relacionados con diferencias de versiones o paquetes.</p> </li> </ol>"},{"location":"entrenamiento/p8/#empaquetado-vs-entorno-entendiendo-sus-roles-en-mlops","title":"Empaquetado vs entorno: entendiendo sus roles en MLOps","text":"<p>El empaquetado y la configuraci\u00f3n del entorno son dos componentes fundamentales pero complementarios en un proyecto de Machine Learning dentro del ciclo de vida MLOps:</p> Aspecto Empaquetado (<code>pyproject.toml</code>) Entorno de Ejecuci\u00f3n (<code>conda</code>, <code>env</code>) \u00bfQu\u00e9 es? Descripci\u00f3n del proyecto y su instalaci\u00f3n como paquete de Python Definici\u00f3n de las librer\u00edas, versiones y dependencias para ejecutar el c\u00f3digo \u00bfQu\u00e9 resuelve? Facilita la distribuci\u00f3n e instalaci\u00f3n del c\u00f3digo como un m\u00f3dulo reutilizable. Garantiza reproducibilidad y compatibilidad del entorno. \u00bfQui\u00e9n lo usa? Otros desarrolladores o sistemas que consumen el proyecto. T\u00fa y tu equipo, durante desarrollo, entrenamiento o pruebas. Archivo principal <code>pyproject.toml</code> o antes <code>setup.py</code> + <code>setup.cfg</code> <code>environment.yml</code>, <code>requirements.txt</code>, o comandos <code>conda</code>"},{"location":"entrenamiento/p8/#error-comun-modulenotfound-al-ejecutar-un-script-del-proyecto","title":"\ud83d\udca5 Error com\u00fan: <code>ModuleNotFound</code> al ejecutar un script del proyecto","text":"<p>Para ilustrar un problema com\u00fan en proyectos mal configurados, ejecutaremos el script <code>GetData.py</code>, cuya funci\u00f3n es obtener datos desde una fuente externa.</p> <p>Este script importa otros m\u00f3dulos del mismo proyecto, como por ejemplo:</p> <pre><code>from app_logging import logging\n</code></pre> <p>Sin embargo, si intentamos ejecutarlo directamente con (o desde VS Code):</p> <pre><code>python src/data_eng/GetData.py\n</code></pre> <p>Es muy probable que obtengamos un error como:</p> <pre><code>ModuleNotFoundError: No module named 'app_logging'\n</code></pre> <p>\u2753 \u00bfPor qu\u00e9 ocurre este error?</p> <p>Este error ocurre porque Python no reconoce autom\u00e1ticamente la ra\u00edz del proyecto ni las rutas relativas a otros m\u00f3dulos internos cuando se ejecuta un archivo de forma directa. Si el empaquetado del proyecto no est\u00e1 configurado correctamente (por ejemplo, sin un <code>pyproject.toml</code>, <code>setup.py</code>, o sin a\u00f1adir el proyecto al <code>PYTHONPATH</code>), Python no sabe c\u00f3mo resolver los imports relativos a la estructura del proyecto.</p> <p>Adem\u00e1s, en muchos casos, el desarrollador principal puede haber configurado su entorno de desarrollo (IDE) \u2014como PyCharm o VSCode\u2014 para que funcione solo en su m\u00e1quina, estableciendo manualmente el int\u00e9rprete o la carpeta ra\u00edz. Esto no garantiza que el proyecto funcione correctamente en otros entornos o m\u00e1quinas.</p>"},{"location":"entrenamiento/p8/#empaquetar-el-proyecto-como-un-modulo-python","title":"Empaquetar el proyecto como un m\u00f3dulo Python","text":"<p>Para garantizar que todos los m\u00f3dulos internos del proyecto se puedan importar correctamente \u2014sin depender de configuraciones espec\u00edficas del entorno o del IDE\u2014 es necesario empaquetar el proyecto como un m\u00f3dulo Python instalable. Esto se logra definiendo un archivo <code>pyproject.toml</code> (recomendado por los est\u00e1ndares modernos de Python) o, alternativamente, un <code>setup.py</code>, donde se describe la informaci\u00f3n del proyecto y sus dependencias.</p> <ol> <li> <p>Crear el archivo <code>pyproject.toml</code> en la ra\u00edz del proyecto:</p> <pre><code>[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"MLOPS-Project\"\nversion = \"0.1.0\"\nauthors = [{name = \"Tu Nombre\", email = \"tu@correo.com\"}]\ndescription = \"Proyecto de predicci\u00f3n de costos en cadena de suministro\"\nrequires-python = \"&gt;=3.12\"\ndynamic = [\"dependencies\"]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n[tool.setuptools.dynamic]\ndependencies = {file = [\"requirements.txt\"]}\n</code></pre> <p>Nota</p> <p>Para que <code>setuptools</code> encuentre los paquetes Python dentro de <code>src/</code>, debes indic\u00e1rselo expl\u00edcitamente con esta l\u00ednea en el archivo <code>pyproject.toml</code>.</p> <pre><code>[tool.setuptools.packages.find]\nwhere = [\"src\"]\n</code></pre> </li> <li> <p>Instala tu proyecto en modo editable. Usa el siguiente comando desde la ra\u00edz del proyecto: </p> <pre><code>pip install -e .\n</code></pre> <p>Esto permitir\u00e1 que cualquier script dentro del proyecto pueda importar otros m\u00f3dulos internos sin necesidad de hacks o configuraci\u00f3n especial en el IDE.</p> </li> </ol>"},{"location":"entrenamiento/p9/","title":"Buenas Pr\u00e1cticas de Codificaci\u00f3n","text":"<p>El desarrollo de sistemas de aprendizaje autom\u00e1tico (ML) ha evolucionado desde simples prototipos en notebooks hacia aplicaciones robustas que requieren principios de ingenier\u00eda de software. En este contexto, MLOps surge como un enfoque que combina las mejores pr\u00e1cticas del desarrollo de software y la operaci\u00f3n de modelos de ML, facilitando la reproducibilidad, escalabilidad y mantenibilidad de los sistemas de inteligencia artificial.</p> <p>Una parte fundamental de MLOps es la implementaci\u00f3n de buenas pr\u00e1cticas de codificaci\u00f3n, que no solo permiten mejorar la calidad del c\u00f3digo, sino tambi\u00e9n facilitar el trabajo colaborativo, la automatizaci\u00f3n de flujos de trabajo y la integraci\u00f3n con herramientas de versionado, testing y despliegue continuo. En esta pr\u00e1ctica se explorar\u00e1n tres pilares clave para lograr un desarrollo m\u00e1s profesional y organizado:</p> <ul> <li> <p>Uso de archivos de configuraci\u00f3n (Config Files): Separar la l\u00f3gica del c\u00f3digo de los par\u00e1metros y configuraciones del sistema facilita la experimentaci\u00f3n, la reutilizaci\u00f3n del c\u00f3digo y la trazabilidad de los cambios.</p> </li> <li> <p>Interfaces de l\u00ednea de comandos (Command Line Interfaces, CLI): Incorporar interfaces CLI robustas y flexibles permite ejecutar scripts de forma controlada y program\u00e1tica, facilitando la integraci\u00f3n en pipelines automatizados y la ejecuci\u00f3n reproducible de experimentos.</p> </li> <li> <p>Buenas pr\u00e1cticas de codificaci\u00f3n: Aplicar principios como modularidad, documentaci\u00f3n, tipado, control de errores, pruebas unitarias y seguimiento de estilo (PEP8, linters) permite construir c\u00f3digo m\u00e1s limpio, escalable y confiable, caracter\u00edsticas esenciales para proyectos de ciencia de datos que evolucionan hacia producci\u00f3n.</p> </li> </ul>"},{"location":"entrenamiento/p9/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<ul> <li> <p>Dise\u00f1ar e implementar estructuras de configuraci\u00f3n externas (YAML, JSON, .env) para separar los par\u00e1metros de ejecuci\u00f3n de la l\u00f3gica del c\u00f3digo, siguiendo principios de limpieza y flexibilidad.</p> </li> <li> <p>Explorar distintas formas de construir interfaces de l\u00ednea de comandos (CLI) para aplicaciones de ML:</p> <ul> <li>Uso de librer\u00edas como argparse, click o typer.</li> <li>Construcci\u00f3n de comandos reutilizables y documentados.</li> <li>Manejo de argumentos, subcomandos y ayuda contextual.</li> </ul> </li> <li> <p>Aplicar buenas pr\u00e1cticas de codificaci\u00f3n en el desarrollo de componentes de un proyecto MLOps.</p> </li> </ul>"},{"location":"entrenamiento/p9/#gestion-de-configuraciones-con-hydra","title":"Gesti\u00f3n de configuraciones con Hydra","text":"<p>El c\u00f3digo actual del proyecto utiliza <code>argparse</code> para recibir ciertos par\u00e1metros necesarios para la ejecuci\u00f3n de los scripts; sin embargo, tambi\u00e9n incorpora otros par\u00e1metros directamente codificados en el cuerpo del programa. Por ejemplo, el script <code>stage0_loading.py</code>, utiliza <code>argparse</code> para recibir la URL donde est\u00e1 alojado  los datos externos, pero adem\u00e1s incluye valores codificados directamente en el c\u00f3digo fuente, como la ruta donde se guarda el archivo descargado. Este enfoque limita la flexibilidad y dificulta su uso en distintos entornos o etapas del pipeline.</p> <p>El uso de valores codificados (\"hardcoded\") como:</p> <pre><code>self.external_data = \"data/external\"\n</code></pre> <p>impide modificar rutas de entrada o salida sin alterar directamente el c\u00f3digo, lo cual va en contra de las buenas pr\u00e1cticas en proyectos MLOps.</p> <p>Por ello, se propone migrar a un enfoque basado en archivos de configuraci\u00f3n con Hydra, una herramienta moderna que permite manejar par\u00e1metros de manera flexible y desacoplada del c\u00f3digo, ideal para flujos complejos de entrenamiento, pruebas y despliegue de modelos.</p>"},{"location":"entrenamiento/p9/#propuesta-de-mejora","title":"Propuesta de mejora","text":"<p>Hydra permite manejar configuraciones con archivos <code>.yaml</code>, dejando el c\u00f3digo fuente limpio y desacoplado de decisiones contextuales como rutas de archivos, nombres, etc. A continuaci\u00f3n los pasos para migrar a Hydra:</p> <ol> <li> <p>Instalar Hydra</p> <p>Hasta la fecha actual, Hydra no est\u00e1 disponible directamente como paquete en los canales est\u00e1ndar de Conda (como <code>conda-forge</code> o <code>defaults</code>), por lo que la forma recomendada de instalar <code>hydra-core</code> es usando <code>pip</code>, incluso dentro de un entorno Conda:</p> <pre><code>pip install hydra-core\n</code></pre> </li> <li> <p>Crear un archivo de configuraci\u00f3n YAML</p> <p>Crea el archivo <code>configs/data_eng.yaml</code>, el cual contendr\u00e1 todos los par\u00e1metros necesarios para la fase Ingenier\u00eda de Datos (Data Enginnering) del flujo de trabajo de MLOPS. Por el momento, el archivo tendra el siguiente contenido:</p> <pre><code>data_source:\n    url: \"https://raw.githubusercontent.com/jmem-ec/KRRCourse/ccbd6ccf8389ba0988d53fc9300a64da00e6368b/Consignment_pricing.csv\"\n    external_data_dir: \"data/external\"\n    filename: \"Consignment_pricing.csv\"\n</code></pre> </li> <li> <p>Modificar el script para usar Hydra</p> <p>Las siguientes acciones han sido ejecutadas:</p> <ul> <li> <p>Incorporar en la seccion de importaci\u00f3n      <pre><code>    import hydra\n    from omegaconf import DictConfig\n</code></pre></p> </li> <li> <p>Reemplaza el uso de <code>argparse</code> por hydra, y usa los valores del archivo <code>.yaml</code></p> <p><pre><code>@hydra.main(config_path=\"../../configs\", config_name=\"data_eng\", version_base=None)\ndef main(cfg: DictConfig):\n    logging.basicConfig(level=logging.INFO)\n    data = GetData().get_data(cfg)\n\nif _name_ == \"_main_\":\n    main()\n</code></pre> Toma en cuenta que no puedes llamar directamente a la funci\u00f3n decorada con <code>@hydra.main(...)</code> como <code>main()</code> desde <code>if __name__ == \"__main__\":</code>, porque Hydra necesita controlar el punto de entrada del script para realizar su redirecci\u00f3n de rutas y configuraci\u00f3n del entorno de ejecuci\u00f3n.</p> </li> <li> <p>Modifica la cabecera de la funcion <code>get_data</code> </p> <pre><code>def get_data(self, config)\n</code></pre> </li> <li> <p>Cambia los valores codificados (\"hardcoded\") usando los valores del archivo <code>.yaml</code> <pre><code>...\ngithub_csv_url = config.data_source.url\n\nself.external_data = config.data_source.external_data_dir\n\n...\nlocal_path = os.path.join(self.external_data, config.data_source.filename)\n</code></pre></p> </li> </ul> </li> </ol>"},{"location":"entrenamiento/p9/#tarea","title":"\ud83d\udee0\ufe0f Tarea","text":"<ul> <li> <p>Por defecto, Hydra escribir\u00e1 los resultados en una carpeta de resultados, con una subcarpeta para el d\u00eda en que se ejecut\u00f3 el experimento y, adem\u00e1s, la hora en que se inici\u00f3. Inspeccione su ejecuci\u00f3n revisando cada archivo que Hydra ha generado y compruebe que la informaci\u00f3n ha sido registrada.</p> </li> <li> <p>Hydra tambi\u00e9n permite cambiar y a\u00f1adir par\u00e1metros din\u00e1micamente sobre la marcha desde la l\u00ednea de comandos:</p> <ul> <li> <p>Pruebe a cambiar un par\u00e1metro desde la l\u00ednea de comandos. <pre><code>python src/data_eng/stage0_loading.py data_source.filename=Dataset1.csv\n</code></pre></p> </li> <li> <p>Pruebe a a\u00f1adir un par\u00e1metro desde la l\u00ednea de comandos. <pre><code>python src/data_eng/stage0_loading.py +data_source.additional=50\n</code></pre></p> </li> </ul> </li> <li> <p>Realice un nuevo experimento utilizando un nuevo archivo de configuraci\u00f3n en el que cambie un par\u00e1metro de su elecci\u00f3n. No se le permite cambiar el archivo de configuraci\u00f3n en el script, sino que deber\u00eda poder proporcionarlo como argumento al iniciar el script, por ejemplo, algo como</p> <pre><code>python src/data_eng/stage0_loading.py experiment=exp2\n</code></pre> <p>Le recomendamos que utilice una estructura de archivos como la siguiente:</p> <pre><code>|--confs\n|  |--model1.yaml\n|  |--data_eng.yaml\n|  |--experiments\n|     |--exp1.yaml\n|     |--exp2.yaml\n|--data\n|--...\n</code></pre> <p>Este ser\u00eda el archivo base de configuraci\u00f3n <code>data_eng.yaml</code>, que incluye un grupo <code>experiment</code>:</p> <pre><code>defaults:\n    - experiment: exp1   # valor por defecto\n\ndata_source:\n    url: \"https://raw.githubusercontent.com/jmem-ec/KRRCourse/ccbd6ccf8389ba0988d53fc9300a64da00e6368b/Consignment_pricing.csv\"\n    external_data_dir: \"data/external\"\n    filename: \"Consignment_pricing.csv\"\n</code></pre> <p>Experimento 1: configs/experiments/exp1.yaml</p> <pre><code>data_source:\n    filename: \"Dataset1.csv\"\n</code></pre> <p>Experimento 2: configs/experiments/exp2.yaml</p> <pre><code>data_source:\n    filename: \"Dataset2.csv\"\n</code></pre> <p>Ahora puede probar los experimentos</p> <pre><code># Ejecuta con la configuraci\u00f3n por defecto (exp1)\npython src/data_eng/stage0_loading.py\n\n# Ejecuta con la configuraci\u00f3n de experimento 2\npython src/data_eng/stage0_loading.py experiment=exp2\n</code></pre> </li> <li> <p>Modifique el script <code>stage1_ingestion.py</code> para que obtenga los par\u00e1metros necesarios desde el archivo de configuraci\u00f3n <code>data_eng.yaml</code>, en lugar de tenerlos codificados directamente en el script. En particular, aseg\u00farate de que el archivo <code>data_eng.yaml</code> incluya la siguiente secci\u00f3n:</p> <pre><code>raw_data:\n    raw_data_dir: \"data/raw\"\n    raw_filename: \"Dataset.csv\"\n</code></pre> </li> </ul>"},{"location":"negocio/p4/","title":"Alineaci\u00f3n Estrat\u00e9gica del Proyecto","text":"<p>En el contexto de las operaciones de aprendizaje autom\u00e1tico (MLOps), es fundamental planificar y estructurar los proyectos de forma clara antes de implementar soluciones t\u00e9cnicas. ML Canvas es una herramienta visual que permite modelar, comunicar y alinear los distintos componentes de un proyecto de machine learning, promoviendo una visi\u00f3n compartida entre equipos multidisciplinarios.</p> <p>Esta herramienta facilita la identificaci\u00f3n de objetivos de negocio, fuentes de datos, m\u00e9tricas de \u00e9xito, riesgos y requerimientos t\u00e9cnicos, integrando todos estos elementos dentro del flujo de trabajo de MLOps. Su uso temprano en el desarrollo de un proyecto mejora la comunicaci\u00f3n, reduce ambig\u00fcedades y gu\u00eda la toma de decisiones t\u00e9cnicas con un enfoque sistem\u00e1tico.</p>"},{"location":"negocio/p4/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Esta pr\u00e1ctica tiene como objetivo introducir el uso de ML Canvas como herramienta de planificaci\u00f3n en proyectos de MLOps. Al finalizar, los participantes ser\u00e1n capaces de:</p> <ul> <li> <p>Identificar y documentar los componentes clave de un proyecto de machine learning de forma estructurada.</p> </li> <li> <p>Relacionar los objetivos del negocio con las decisiones t\u00e9cnicas dentro del flujo de trabajo de MLOps.</p> </li> </ul>"},{"location":"negocio/p4/#contexto-del-proyecto","title":"Contexto del Proyecto","text":"<p>El proyecto Predicci\u00f3n de Precios en Art\u00edculos de Consignaci\u00f3n se enfoca en desarrollar un modelo de aprendizaje autom\u00e1tico (ML) que permita predecir con precisi\u00f3n el precio de venta de productos consignados, a partir de diversas caracter\u00edsticas.</p> <p>La consignaci\u00f3n es un modelo de negocio en el cual una tienda vende productos en nombre de terceros, reteniendo una comisi\u00f3n sobre el precio final de venta. Este proyecto busca asistir tanto a los due\u00f1os de tiendas de consignaci\u00f3n como a los vendedores en la determinaci\u00f3n de precios \u00f3ptimos, lo cual puede traducirse en una mayor tasa de ventas y rentabilidad.</p> <p>Para lograr este objetivo, el proyecto se desarrolla dentro de un flujo de trabajo MLOps, lo que implica considerar desde el an\u00e1lisis de datos y el entrenamiento del modelo, hasta su despliegue, monitoreo y mantenimiento continuo. En este contexto, el uso del AI &amp; ML Canvas resulta esencial para planificar y comunicar de forma clara todos los aspectos clave del proyecto.</p>"},{"location":"negocio/p4/#actividad-practica","title":"\ud83e\uddea Actividad Pr\u00e1ctica","text":"<p>Usa esta informaci\u00f3n para completar cada bloque del AI &amp; ML Canvas, asegur\u00e1ndote de:</p> <ul> <li>Redactar cada secci\u00f3n con claridad.</li> <li>Indicar supuestos si falta informaci\u00f3n (se pueden validar despu\u00e9s).</li> <li>Discutir el resultado con tu equipo para afinar los elementos del proyecto.</li> <li>Crea una subcarpeta llamada <code>Practica-4</code> dentro del directorio principal de pr\u00e1cticas y utiliza ese espacio para almacenar las evidencias del trabajo desarrollado en equipo- Suba esta evidencia a la plataforma virtual tambi\u00e9n.</li> </ul>"},{"location":"versionado_datos/p10/","title":"Almacenamiento remoto con DVC","text":"<p>En proyectos de ciencia de datos o machine learning, los conjuntos de datos y modelos pueden ser muy grandes y dif\u00edciles de gestionar dentro de Git. Para resolver esto, DVC (Data Version Control) ofrece una forma eficiente de versionar y compartir archivos grandes sin almacenarlos directamente en el repositorio.</p> <p>DVC permite configurar un almacenamiento remoto (Remote Storage) para sincronizar autom\u00e1ticamente los archivos grandes con servicios como Google Cloud Storage, Google Drive, S3, entre otros. As\u00ed, se facilita la colaboraci\u00f3n en equipo y la reproducci\u00f3n de experimentos.</p> <p>\ud83d\udcddDVC act\u00faa como un complemento de Git: mientras Git versiona el c\u00f3digo, DVC se encarga de los datos y modelos.</p>"},{"location":"versionado_datos/p10/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<p>Aprender a gestionar datos de forma eficiente y colaborativa en proyectos de ciencia de datos mediante la herramienta DVC (Data Version Control). Al finalizar esta pr\u00e1ctica, podr\u00e1s:</p> <ul> <li>Instalar y configurar DVC en tu entorno local.</li> <li>Entender c\u00f3mo DVC complementa a Git para versionar y rastrear archivos grandes.</li> <li>Configurar un almacenamiento remoto para compartir datos con tu equipo.</li> </ul>"},{"location":"versionado_datos/p10/#requisitos-previos","title":"Requisitos Previos","text":"<ul> <li>Tener instalado git.</li> <li>Contar con una cuenta de Google.</li> </ul>"},{"location":"versionado_datos/p10/#instalacion-de-dvc","title":"Instalaci\u00f3n de DVC","text":"<p>\ud83d\udca1 Se recomienda realizar esta instalaci\u00f3n dentro de un entorno Conda previamente creado para el proyecto.</p> <ol> <li>Activar tu entorno Conda     <pre><code>conda activate nombre-del-entorno\n</code></pre></li> <li> <p>Instalar DVC La forma m\u00e1s directa es usando pip, ya que DVC se encuentra activamente mantenido en PyPI:     <pre><code>pip install dvc\n</code></pre> Tambi\u00e9n se recomienda para esta practica instalar DVC con soporte para almacenamiento en la nube. Para usar Google Drive:     <pre><code>pip install 'dvc[gdrive]'\n</code></pre> Y para usar Google Cloud Storage     <pre><code>pip install 'dvc[gcs]'\n</code></pre></p> <p>\ud83d\udca1 Se puede usar el comando <code>pip install 'dvc[all]'</code> para instalar soporte para todos los  principales backends de almacenamiento remoto, incluyendo:</p> <ul> <li>Google Drive (gdrive)</li> <li>Google Cloud Storage (gcs)</li> <li>Amazon S3 (s3)</li> <li>Azure Blob Storage (azure)</li> <li>SSH, HDFS, WebDAV, entre otros.</li> </ul> <p>Porsupuesto, la instalaci\u00f3n con <code>[all]</code> puede tardar m\u00e1s y ocupar m\u00e1s espacio, ya que instala muchas dependencias adicionales.</p> </li> <li> <p>Verificar instalaci\u00f3n     <pre><code>dvc --version\n</code></pre></p> </li> </ol>"},{"location":"versionado_datos/p10/#parte-1-configurar-google-drive-como-almacenamiento-remoto-en-dvc","title":"\ud83d\udd27 Parte 1: Configurar Google Drive como almacenamiento remoto en DVC","text":"<p>Google Drive puede usarse como un almacenamiento remoto para proyectos gestionados con DVC. Esta opci\u00f3n es \u00fatil para proyectos colaborativos que usan almacenamiento en la nube gratuito y accesible. A continuaci\u00f3n se detallan los pasos para conectar tu proyecto a Google Drive.</p>"},{"location":"versionado_datos/p10/#crear-y-conectar-una-carpeta-en-google-drive","title":"\u2601\ufe0f Crear y conectar una carpeta en Google Drive","text":"<ol> <li>Accede a Google Drive.</li> <li>Crea una nueva carpeta y as\u00edgnale un nombre \u00fanico (por ejemplo, dvc-proyecto-datos).</li> <li>Haz clic derecho sobre la carpeta y selecciona \"Compartir\".</li> <li>Configura el acceso como \"Cualquiera con el enlace puede ver\".</li> <li>Copia el identificador de la carpeta desde la URL. Por ejemplo:</li> </ol> <p>https://drive.google.com/drive/folders/1A2B3C4D5E6F7G8H9I</p>"},{"location":"versionado_datos/p10/#parte-2-configurar-google-drive-como-almacenamiento-remoto-en-dvc","title":"\ud83d\udd27 Parte 2: Configurar Google Drive como almacenamiento remoto en DVC","text":"<p>Google Cloud Storage (GCS) puede usarse como un almacenamiento remoto para proyectos gestionados con DVC. Esta opci\u00f3n es ideal para proyectos colaborativos que requieren mayor capacidad, control de acceso y velocidad de transferencia, especialmente en entornos de producci\u00f3n o investigaci\u00f3n. A continuaci\u00f3n se detallan los pasos para conectar tu proyecto a Google Cloud Storage.</p>"},{"location":"versionado_datos/p10/#crear-un-bucket-en-google-cloud-storage","title":"\u2601\ufe0f Crear un Bucket en Google Cloud Storage","text":"<ol> <li>Accede a Google Cloud Console.</li> <li>Aseg\u00farate de haber creado un nuevo proyecto espec\u00edficamente para esta pr\u00e1ctica.</li> <li>En el men\u00fa de navegaci\u00f3n, selecciona \"Cloud Storage\", luego ve a \"Buckets\" y haz clic en \"Create a new bucket\".</li> <li>Asigna un nombre \u00fanico a tu bucket.</li> <li>Selecciona la regi\u00f3n <code>us-east1</code>.</li> <li>Haz clic en \"Continue\" hasta completar la creaci\u00f3n del bucket.</li> </ol>"},{"location":"versionado_datos/p10/#obtener-credenciales-para-conectar-dvc-con-gcp","title":"\ud83d\udd10 Obtener credenciales para conectar DVC con GCP","text":"<p>Una vez creado el bucket, es necesario obtener las credenciales para conectar el almacenamiento remoto de GCP con tu proyecto DVC:</p> <ol> <li>Ve a IAM &amp; Admin y selecciona Service Accounts en la barra lateral izquierda.</li> <li>Haz clic en el bot\u00f3n \"Create Service Account\" para crear una nueva cuenta de servicio que usar\u00e1s para conectar con el proyecto DVC.</li> <li>Asigna un nombre e ID a esta cuenta (por ejemplo, <code>lab2</code>) y deja la configuraci\u00f3n predeterminada.</li> <li>Haz clic en \"Create and Continue\".</li> <li>En la secci\u00f3n de permisos, selecciona Owner en el men\u00fa desplegable y haz clic en \"Continue\".</li> <li>A\u00f1ade tu usuario para que tenga acceso a esta cuenta de servicio y haz clic en \"Done\".</li> </ol>"},{"location":"versionado_datos/p10/#descargar-credenciales","title":"\ud83d\udd11 Descargar credenciales","text":"<ol> <li>Ser\u00e1s redirigido a la p\u00e1gina de Service Accounts.</li> <li>Busca tu cuenta de servicio reci\u00e9n creada, haz clic en \"Actions\" (\u00edcono de tres puntos), y selecciona \"Manage keys\".</li> <li>Haz clic en el bot\u00f3n \"Add Key\".</li> <li>Selecciona la opci\u00f3n de generar una nueva clave en formato JSON.</li> <li>Descarga y almacena de forma segura este archivo <code>.json</code>.</li> </ol> <p>\ud83d\udcdd Este archivo de credenciales ser\u00e1 utilizado por DVC como mecanismo de autenticaci\u00f3n al conectarse con Google Cloud.</p>"},{"location":"versionado_datos/p10/#conectar-dvc-a-google-drive-o-google-cloud-storage","title":"\ud83d\udd17 Conectar DVC a Google Drive o Google Cloud Storage","text":"<p>En esta pr\u00e1ctica hemos dado las instrucciones para conectar DVC a dos almacenamientos remotos. DVC permite definir m\u00faltiples remotos y seleccionar cu\u00e1l usar como predeterminado o indicar uno espec\u00edfico al momento de hacer <code>dvc push</code>, <code>dvc pull</code>, o <code>dvc fetch</code>.</p> <p>Para configurar dos accesos remotos, podemos usar: <pre><code># Agregar Google Drive como remoto\ndvc remote add gdrive-remote gdrive://&lt;ID-de-carpeta&gt;\ndvc remote modify gdrive-remote gdrive_use_service_account true\n\n# Agregar Google Cloud Storage como remoto\ndvc remote add gcs-remote gs://mi-bucket-dvc\ndvc remote modify gcs-remote credentialpath /ruta/a/credenciales.json\n</code></pre></p> <p>Nota: Aqu\u00ed no se establece ninguno como predeterminado a\u00fan.</p>"},{"location":"versionado_datos/p10/#elegir-almacenamiento-remoto-al-hacer-pushpull","title":"Elegir almacenamiento remoto al hacer <code>push/pull</code>","text":"<p>Puedes subir tus datos a un almacenamiento remoto espec\u00edfico sin necesidad de cambiar el predeterminado: <pre><code>dvc push -r gdrive-remote     # Subir a Google Drive\ndvc push -r gcs-remote        # Subir a GCS\n</code></pre></p>"},{"location":"versionado_datos/p10/#o-definir-un-almacenamiento-remoto-por-defecto","title":"\u2b50 O definir un almacenamiento remoto por defecto","text":"<p><pre><code>dvc remote default gcs-remote\n</code></pre> Luego, cada vez que hagas <code>dvc push</code> o <code>dvc pull</code>, se usar\u00e1 ese sin tener que especificarlo.</p> <p>\ud83e\udde0 Recomendaci\u00f3n</p> <p>Si trabajas con equipos o deseas redundancia en almacenamiento, tener m\u00faltiples remotos puede ayudarte a:</p> <ul> <li>Compartir con quienes no tienen acceso a ciertas plataformas.</li> <li>Hacer backups autom\u00e1ticos a diferentes nubes.</li> <li>Evaluar velocidad y costos de acceso.</li> </ul>"},{"location":"versionado_datos/p11/","title":"Control de versiones de datos","text":"<p>En los proyectos de ciencia de datos, los archivos de datos suelen cambiar durante el desarrollo: nuevas versiones, transformaciones, limpieza, etc. Git no es adecuado para archivos grandes, por lo que DVC permite versionar datasets de forma eficiente, integr\u00e1ndose con Git y almacenando los datos reales en un remoto (como Google Drive o Google Cloud Storage). En esta pr\u00e1ctica, aprender\u00e1s a rastrear y recuperar versiones previas de tus datos.</p>"},{"location":"versionado_datos/p11/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a versionar datos en un proyecto de Machine Learning utilizando DVC (Data Version Control). Esta pr\u00e1ctica incluye registrar archivos de datos, hacer seguimiento a los cambios y restaurar versiones anteriores, incluso si los archivos han sido eliminados localmente. </p>"},{"location":"versionado_datos/p11/#supuestos-iniciales","title":"\ud83d\udcc1 Supuestos iniciales","text":"<ul> <li>Tienes configurado un entorno Conda con DVC instalado.</li> <li>El repositorio ya tiene configurado el acceso remoto a Google Drive y/o Google Cloud Storage.</li> <li>El archivo de datos <code>data/dataset.csv</code> ya existe en el repositorio local y ha sido agregado al proyecto.</li> </ul>"},{"location":"versionado_datos/p11/#flujo-de-trabajo-tipico-para-el-versionado-de-datos-con-dvc","title":"Flujo de trabajo t\u00edpico para el versionado de datos con DVC","text":"<p>Una vez configurado el almacenamiento remoto (Google Drive, por ejemplo), puedes comenzar a versionar sus datos de manera sencilla y eficiente.</p>"},{"location":"versionado_datos/p11/#1-inicializar-dvc-en-el-proyecto-si-aun-no-esta-hecho","title":"1. Inicializar DVC en el proyecto (si a\u00fan no est\u00e1 hecho)","text":"<pre><code>dvc init\ngit add .dvc .gitignore\ngit commit -m \"Inicializa DVC en el proyecto\"\n</code></pre> <ul> <li>dvc init: Crea la carpeta .dvc/ y archivos de configuraci\u00f3n necesarios para que DVC funcione. Es similar a git init, pero para el control de versiones de datos.</li> <li>git add .dvc .gitignore: Agrega a Git los archivos generados por DVC. .dvc/ contiene la configuraci\u00f3n de DVC y .gitignore se actualiza para evitar que Git rastree archivos de datos grandes (que ahora ser\u00e1n manejados por DVC).</li> <li>git commit -m \"Inicializa DVC en el proyecto\": Guarda estos cambios en el historial de Git.</li> </ul>"},{"location":"versionado_datos/p11/#2-agregar-archivo-de-datos-para-seguimiento","title":"2. Agregar archivo de datos para seguimiento","text":"<p>Supongamos que has descargado un archivo, por ejemplo <code>dataset.csv</code>, dentro de tu carpeta data/ y quieres poner este archivo bajo el control de versiones de datos de tu proyecto. El primer paso es agregarlo al control local de DVC y a su cach\u00e9 con el comando:</p> <pre><code>dvc add data/dataset.csv\ngit add data/dataset.csv.dvc .gitignore\ngit commit -m \"Agrega archivo dataset.csv al seguimiento de DVC\"\n</code></pre> <ul> <li> <p>dvc add data/dataset.csv: Este comando funciona de manera similar a <code>git add</code>. Tu conjunto de datos ahora est\u00e1 bajo control local de DVC y almacenado en su cach\u00e9 (por defecto local, aunque se puede configurar para ser compartido).</p> <p>Al ejecutar este comando, DVC genera dos archivos en la carpeta donde est\u00e1 tu dato:</p> <ul> <li>dataset.csv.dvc: Este archivo contiene la referencia a la ubicaci\u00f3n real de tu archivo y cambia cada vez que el archivo de datos se modifica.</li> <li>.gitignore: Este archivo evita que Git suba el archivo de datos real al repositorio, evitando que el repositorio se llene de archivos pesados. DVC lo crea autom\u00e1ticamente.</li> </ul> <p>Estos archivos .dvc son archivos YAML que guardan informaci\u00f3n clave sobre tu archivo de datos, incluyendo un hash MD5 que se recalcula cada vez que el archivo cambia. Esto permite a tu equipo seguir los cambios en los datos de forma segura y confiable.</p> </li> </ul>"},{"location":"versionado_datos/p11/#3-sincronizar-datos-con-el-almacenamiento-remoto","title":"3. Sincronizar datos con el almacenamiento remoto","text":"<p>Para subir los datos versionados a tu almacenamiento remoto (Google Drive), usa:</p> <pre><code>dvc push data/dataset.csv\ngit commit -m \"data push\"\n</code></pre> <p>Esto enviar\u00e1 el archivo al almacenamiento remoto configurado. Luego, realiza un commit en Git para registrar este cambio:</p> <p>\u26a0\ufe0f Nota: Las siguientes opciones se presentan \u00fanicamente con fines informativos. No las ejecutes directamente en tu proyecto, pues no tienen relevancia en este momento. Una referencia completa de los comandos DVC puede ser encontrado aqu\u00ed.</p>"},{"location":"versionado_datos/p11/#recuperar-datos","title":"Recuperar datos","text":"<p>Si en alg\u00fan momento eliminas el archivo original de datos, puedes recuperarlo f\u00e1cilmente ejecutando:</p> <pre><code>dvc checkout data/dataset.csv.dvc\n</code></pre> <p>Esto restaurar\u00e1 el archivo de datos tal como estaba bajo la versi\u00f3n controlada.</p>"},{"location":"versionado_datos/p11/#descargar-datos","title":"Descargar datos","text":"<p>Cuando cambies a otra rama o quieras obtener los datos relacionados con otro experimento, simplemente usa:</p> <pre><code>git checkout nombre_de_rama\ndvc pull\n</code></pre> <p>Con esto, descargar\u00e1s desde el almacenamiento remoto la versi\u00f3n correcta de los datos para la rama en la que est\u00e9s trabajando.</p>"},{"location":"versionado_datos/p11/#escenario-eliminar-accidentalmente-el-archivo-local","title":"\ud83d\uddd1\ufe0f Escenario: Eliminar accidentalmente el archivo local","text":"<ul> <li> <p>Simula la eliminaci\u00f3n del archivo de datos local:</p> <pre><code>rm data/dataset.csv\n</code></pre> </li> <li> <p>Verifica que el archivo ya no est\u00e1:</p> <pre><code>ls data/\n</code></pre> </li> <li> <p>Recuperar los datos desde el almacenamiento remoto</p> <pre><code>dvc pull\n</code></pre> </li> </ul> <p>\u2705 Esto restaura <code>dataset.csv</code> desde el remoto. Ideal para trabajar en diferentes m\u00e1quinas o recuperar archivos perdidos.</p>"},{"location":"versionado_datos/p11/#extra-ver-historial-de-versiones","title":"\ud83d\udccc Extra: Ver historial de versiones","text":"<p>Puedes volver a una versi\u00f3n anterior del archivo de datos siguiendo estos pasos:</p> <p><pre><code>git checkout &lt;hash_version_anterior&gt; data/dataset.csv.dvc\ndvc checkout\n</code></pre> Esto restaurar\u00e1 el estado del archivo a la versi\u00f3n vinculada con ese commit espec\u00edfico.</p> <p>Importante</p> <p>Esta metodolog\u00eda asegura que el proyecto completo, incluyendo los datos, se mantenga sincronizado entre todos los colaboradores sin necesidad de subir archivos grandes directamente a GitHub, evitando problemas de espacio y rendimiento.</p>"},{"location":"versionado_datos/p12/","title":"Trabajando con Pipelines","text":"<p>Despu\u00e9s de aprender a versionar datos y modelos con DVC, el siguiente paso es automatizar y estructurar nuestros flujos de trabajo mediante la construcci\u00f3n de un pipeline de experimentos. Un pipeline permite encadenar etapas del proceso (por ejemplo, preprocesamiento, entrenamiento y evaluaci\u00f3n) de forma reproducible y trazable, facilitando el trabajo colaborativo y la experimentaci\u00f3n sistem\u00e1tica.</p> <p>Esta pr\u00e1ctica est\u00e1 basada en el dataset previamente versionado con DVC. El dataset de la secci\u00f3n anterior es la fuente de datos para entrenar un modelo de predicci\u00f3n. Consideremos que nuestro experimento tiene tres etapas principales:</p> <ul> <li>Preprocesamiento de datos (extracci\u00f3n de caracter\u00edsticas, limpieza, etc.)</li> <li>Entrenamiento del modelo</li> <li>Evaluaci\u00f3n del modelo</li> </ul>"},{"location":"versionado_datos/p12/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<ul> <li>Construir un pipeline de ML utilizando DVC.</li> <li>Declarar las dependencias y salidas de cada etapa del flujo de trabajo.</li> <li>Ejecutar y rastrear autom\u00e1ticamente los cambios en los datos o en el c\u00f3digo que afecten el resultado del experimento.</li> <li>Comprender c\u00f3mo DVC administra los archivos intermedios y el modelo final en cada etapa.</li> </ul>"},{"location":"versionado_datos/p12/#requisitos","title":"\ud83d\udee0\ufe0f Requisitos","text":"<p>Debes contar con los siguientes scripts organizados en tu proyecto:</p> <ul> <li>preprocess.py: realiza limpieza y extracci\u00f3n de caracter\u00edsticas.</li> <li>train.py: entrena un modelo de clasificaci\u00f3n.</li> <li>evaluate.py: eval\u00faa el desempe\u00f1o del modelo entrenado.</li> </ul> <p>Advertencia:</p> <p>Los pipelines propuestos a continuaci\u00f3n est\u00e1n basados en la estrucutura del repositorio (creado en la pr\u00e1ctica Estructura del Proyecto. Sin embargo, es posible crear otros scripts y crear pipelines espec\u00edficos para las necesidades de su proyecto.</p>"},{"location":"versionado_datos/p12/#creacion-de-pipelines","title":"Creaci\u00f3n de pipelines","text":"<p>DVC construye un pipeline basado en tres componentes: Entradas (Inputs), Salidas (Outputs) y Comando (Command). Por ejemplo, para la etapa de preprocesamiento, estos ser\u00edan los componentes:</p> <ul> <li>Entradas: archivo <code>dataset.csv</code> y el script <code>preprocess.py</code></li> <li>Salidas: archivo <code>dataset_processed.csv</code></li> <li>Comando: <code>python preprocess.py dataset.csv</code></li> </ul> <p>Para crear esta etapa de preprocesamiento, utilizamos el comando <code>dvc run</code> de la siguiente forma:</p> <pre><code>dvc run -n preprocess \\\n  -d ./src/preprocess_data.py -d data/dataset.csv \\\n  -o ./data/dataset_processed.csv \\\n  python3 ./src/preprocess_data.py ./data/dataset.csv\n</code></pre> <p>Aqu\u00ed nombramos esta etapa como \"preprocess\" usando la opci\u00f3n <code>-n</code>. Tambi\u00e9n definimos las entradas con la opci\u00f3n <code>-d</code> y las salidas con la opci\u00f3n <code>-o</code>. El comando que se ejecuta siempre va al final del comando <code>dvc run</code>, sin ninguna opci\u00f3n.</p> <p>Consejo</p> <p>Los archivos de salida se agregan al control de DVC cuando reproduces una etapa de DVC. Cuando finalices tu experimento, recuerda usar dvc push para versionar no solo los datos usados sino tambi\u00e9n los resultados generados durante el experimento.</p> <p>La etapa de entrenamiento se crea de manera similar con:</p> <pre><code>dvc run -n train \\\n  -d ./src/train.py -d ./data/dataset_processed.csv -d ./src/model.py \\\n  -o ./models/model.joblib \\\n  python3 ./src/train.py ./data/dataset_processed.csv ./src/model.py\n</code></pre> <p>En este punto, es posible que hayas notado que se crearon dos archivos nuevos: <code>dvc.yaml</code> y <code>dvc.lock</code>. El primero es responsable de guardar lo que se describi\u00f3 en cada comando <code>dvc run</code>. Por lo tanto, si deseas crear o modificar una etapa espec\u00edfica, es posible editar directamente el archivo <code>dvc.yaml</code>. El archivo actual se ver\u00eda as\u00ed:</p> <pre><code>stages:\n  preprocess:\n    cmd: python3 ./src/preprocess_data.py ./data/dataset.csv\n    deps:\n    - ./src/preprocess_data.py\n    - data/dataset.csv\n    outs:\n    - ./data/dataset_processed.csv\n    - ./data/features.csv\n  train:\n    cmd: python3 ./src/train.py ./data/dataset_processed.csv ./src/model.py 200\n    deps:\n    - ./data/dataset_processed.csv\n    - ./src/model.py\n    - ./src/train.py\n    outs:\n    - ./models/model.joblib\n</code></pre> <p>El segundo archivo creado es <code>dvc.lock</code>. Este tambi\u00e9n es un archivo YAML y su funci\u00f3n es similar a los archivos <code>.dvc</code>. En su interior, podemos encontrar la ruta y un c\u00f3digo hash para cada archivo de cada etapa, lo que permite a DVC hacer seguimiento de los cambios. Este seguimiento es importante porque ahora DVC puede saber cu\u00e1ndo una etapa debe ejecutarse de nuevo o no, bas\u00e1ndose en si sus dependencias cambiaron. El pipeline actual se ve as\u00ed:</p> <p></p>"},{"location":"versionado_datos/p12/#generar-metricas","title":"Generar m\u00e9tricas","text":"<p>Finalmente, vamos a crear nuestra \u00faltima etapa para poder evaluar nuestro modelo:</p> <pre><code>dvc run -n evaluate -d ./src/evaluate.py -d ./data/weatherAUS_processed.csv \\\n  -d ./src/model.py -d ./models/model.joblib \\\n  -M ./results/metrics.json \\\n  -o ./results/precision_recall_curve.png -o ./results/roc_curve.png \\\n  python3 ./src/evaluate.py ./data/weatherAUS_processed.csv ./src/model.py ./models/model.joblib\n</code></pre> <p>Es importante notar que en esta etapa usamos la opci\u00f3n <code>-M</code> en lugar de la opci\u00f3n <code>-o</code>. Esto es relevante porque ahora podemos almacenar las m\u00e9tricas generadas por cada experimento. Si ejecutamos el comando:</p> <pre><code>dvc metrics show\n</code></pre> <p>podremos ver qu\u00e9 tan bueno fue el experimento, ya que este comando muestra las m\u00e9tricas guardadas en el archivo <code>metrics.json</code>.</p> <pre><code>$ dvc metrics show\nPath                  accuracy    f1       precision    recall        \nresults/metrics.json  0.84973     0.90747  0.8719       0.94607\n</code></pre>"}]}