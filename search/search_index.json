{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenido al Curso MLOps","text":"<p>Este repositorio contiene el conjunto de pr\u00e1cticas desarrolladas en el marco del curso de MLOps de la Maestr\u00eda en Ciencia de Datos. El objetivo principal de estas pr\u00e1cticas es que los estudiantes adquieran experiencia en el uso de herramientas esenciales para gestionar todo el ciclo de vida de un proyecto de aprendizaje autom\u00e1tico, desde la recolecci\u00f3n y preparaci\u00f3n de los datos hasta el despliegue y monitoreo del modelo en producci\u00f3n.</p> <p>Dado que el desarrollo, la implementaci\u00f3n y el mantenimiento de modelos de machine learning en entornos reales puede ser complejo, las pr\u00e1cticas se apoyan en los principios de MLOps (Machine Learning Operations), un conjunto de pr\u00e1cticas que busca automatizar y simplificar los flujos de trabajo y despliegues de modelos, facilitando as\u00ed su integraci\u00f3n efectiva en sistemas productivos.</p> <p>Como caso pr\u00e1ctico, se desarrolla un modelo de predicci\u00f3n de precios para tiendas de consignaci\u00f3n, un modelo de negocio en el que los productos son vendidos por una tienda en nombre de terceros, a cambio de una comisi\u00f3n. El objetivo es ayudar a propietarios y vendedores a determinar precios \u00f3ptimos para sus art\u00edculos, considerando factores como la condici\u00f3n del producto, la marca y las tendencias del mercado. Una mejor estrategia de precios puede traducirse en un aumento de las ventas y una mayor rentabilidad para ambas partes.</p> <p>Las pr\u00e1cticas de este curso est\u00e1n organizadas en niveles progresivos de madurez en MLOps, que reflejan el grado de automatizaci\u00f3n y robustez del ciclo de vida de un proyecto de aprendizaje autom\u00e1tico:</p> <ul> <li> <p>MLOps Nivel 1 Automatizaci\u00f3n del pipeline de ML. Se enfoca en establecer un flujo automatizado para el entrenamiento continuo del modelo, integrando validaciones de datos y modelos, as\u00ed como el manejo de metadatos y activadores del pipeline.</p> </li> <li> <p>MLOps Nivel 2 Automatizaci\u00f3n con CI/CD. Permite implementar cambios de forma r\u00e1pida y confiable mediante sistemas CI/CD automatizados, facilitando la experimentaci\u00f3n y el despliegue de mejoras en producci\u00f3n.</p> </li> </ul> <p>Estos niveles sientan las bases para alcanzar el MLOps Nivel 3, que se centra en el despliegue y monitoreo continuo del modelo en producci\u00f3n, asegurando su rendimiento a lo largo del tiempo y permitiendo ajustes din\u00e1micos ante cambios en los datos o en el entorno de operaci\u00f3n.</p>"},{"location":"ambiente/p1/","title":"Linea de Comandos","text":"<p>En esta pr\u00e1ctica inicial comenzamos nuestro recorrido en el mundo de las operaciones de aprendizaje autom\u00e1tico (MLOps). Antes de adentrarnos en los conceptos fundamentales, es crucial asegurarnos de que todos cuenten con una base s\u00f3lida en ciertos temas esenciales que utilizaremos a lo largo del curso.</p> <p>Nos enfocaremos en repasar el uso de la l\u00ednea de comandos, una herramienta clave para la configuraci\u00f3n de un entorno de desarrollo adecuado. Este entorno ser\u00e1 fundamental para acompa\u00f1arlos durante todo el proceso de aprendizaje y aplicaci\u00f3n de MLOps.</p>"},{"location":"ambiente/p1/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Esta pr\u00e1ctica tiene como objetivo familiarizarse con el uso de la l\u00ednea de comandos como herramienta esencial en proyectos de MLOps.</p>"},{"location":"ambiente/p1/#que-es-la-linea-de-comandos","title":"\u00bfQu\u00e9 es la l\u00ednea de comandos?","text":"<p>La l\u00ednea de comandos, o terminal, no es un concepto arcaico, sino una herramienta fundamental que precede a las interfaces gr\u00e1ficas. Es ampliamente utilizada en Linux, aunque en Mac y Windows suele pasarse por alto. En MLOps, muchas herramientas carecen de interfaz gr\u00e1fica, por lo que dominar la terminal mejora el flujo de trabajo y es esencial, especialmente cuando trabajemos en la nube m\u00e1s adelante.</p>"},{"location":"ambiente/p1/#instalacion-de-wsl-subsistema-de-windows-para-linux","title":"Instalaci\u00f3n de WSL (Subsistema de Windows para Linux)","text":"<p>Si no usas Linux, debes instalar WSL para tener este sistema operativo en tu m\u00e1quina Windows. En la secci\u00f3n prerequistos de la gu\u00eda oficial de instalaci\u00f3n de WSL puedes verificar que tu sistema cumple con los requisitos necesarios.  Si ya usas Linux como sistema operativo puedes omitir esta secci\u00f3n.</p> <p>WSL puede instalarse principalmente de dos formas, dependiendo de la versi\u00f3n y n\u00famero de compilaci\u00f3n del sistema operativo Windows que tengas instalado en tu m\u00e1quina.</p> <ul> <li> <p>Compilaciones recientes de Windows.     En la seccion Get Started del documento que gu\u00eda en el proceso de configuraci\u00f3n de un entorno de desarrollo WSL, puedes revisar los requistos para usar la versi\u00f3n simplificada de instalaci\u00f3n usando el comando <code>wsl --install</code> desde la terminal de Windows (PowerShell o CMD).</p> <p>Si cumples los requisitos, te recomendamos sigas la guia de instalaci\u00f3n pero cambies la distribuci\u00f3n por omisi\u00f3n, de Ubuntu a Ubuntu 24.04.1 LTS. En la secci\u00f3n Change the default Linux distribution installed, del mismo documento oficial podras encontrar como hacerlo.</p> </li> <li> <p>Compilaciones antiguas de Windows.     Si tu compilacion de Windows no cumple los requisitos indicados en el punto previo, te recomendamos sigas la guia de instalaci\u00f3n manual descrita en este documento. Al final del proceso, recuerde escoger la distribuci\u00f3n Ubuntu 24.04.1 LTS como opci\u00f3n a instalar.</p> </li> </ul> <p>Una vez que haya instalado la distribuci\u00f3n de Ubuntu seleccionada, tendr\u00e1 que crear una cuenta de usuario y una contrase\u00f1a para acceder a esta distribuci\u00f3n de Linux. Toma en cuenta, que WSL(Linux) y Windows son dos sistemas separados, por tanto, al instalar paquetes (como pip) en Linux no los instala autom\u00e1ticamente en Windows. Debes instalarlos en cada sistema por separado si los necesitas en ambos. Sin embargo, el resto de las practicas usar\u00e1n Linux por defecto por lo que no es necesario instalar paquetes en ambos sistemas operativos.</p>"},{"location":"ambiente/p1/#verificacion-de-la-version-wsl-instalada","title":"Verificaci\u00f3n de la versi\u00f3n WSL instalada","text":"<p>Es importante verificar que este usando la versi\u00f3n WSL 2 pues esta versi\u00f3n ofrece una serie de mejoras clave frente a WSL 1 que lo hacen necesario o altamente recomendable para la mayor\u00eda de los casos de uso actuales, especialmente en entornos de desarrollo, ciencia de datos, y pr\u00e1cticas que requieren compatibilidad real con Linux.</p> <p>\u00bfC\u00f3mo verificar la versi\u00f3n de WSL y la distribuci\u00f3n instalada?</p> <p>Ejecuta: <pre><code>wsl -l -v\n</code></pre></p> <p>Esto mostrar\u00e1 la lista de distribuciones instaladas y la versi\u00f3n de WSL que est\u00e1n utilizando. Por ejemplo: <pre><code>NAME      STATE           VERSION\nUbuntu    Running         1\n</code></pre> Si el resultado es el mismo que en el ejemplo, significa que no se ha utilizado la opci\u00f3n correcta de instalaci\u00f3n, acorde con la compilaci\u00f3n de tu sistema Windows</p>"},{"location":"ambiente/p1/#problemas-en-la-instalacion","title":"Problemas en la instalaci\u00f3n","text":"<p>En algunos casos, el proceso de instalaci\u00f3n puede no completarse correctamente. En estas situaciones, te recomendamos consultar los problemas y soluciones proporcionados por Microsoft.</p>"},{"location":"ambiente/p1/#uso-de-la-terminal","title":"Uso de la terminal","text":"<p>Importante</p> <p>Si no est\u00e1 familiarizado con Linux vale la pena revisar un resumen con los principales comandos que se pueden ejecutar en la terminal.</p> <p>Familiarizarse con los comandos b\u00e1sicos de la terminal <code>which</code>, <code>echo</code>, <code>cat</code>, <code>wget</code>, <code>less</code>, y <code>top</code> y el operador de redirecci\u00f3n de salida <code>&gt;</code>. Responde las preguntas en la plataforma virtual</p> <ul> <li> <p>Abre una terminal (en la distribuci\u00f3n de Linux que estes usando)</p> </li> <li> <p>Actualiza la lista de paquetes y sus versiones en el sistema utilizando los siguientes comandos. Esta acci\u00f3n es especialmente importante si acabas de instalar WSL, pues este sistema intenta ser liviano y r\u00e1pido y no fuerza actualizaciones autom\u00e1ticas.</p> <pre><code>sudo apt update\nsudo apt upgrade \n</code></pre> </li> <li> <p>Instala tambi\u00e9n <code>pip</code> (el gestor de paquetes de Python).     <pre><code>sudo apt install python3-pip.\n</code></pre></p> </li> <li> <p>Verifica la instalaci\u00f3n escribiendo <code>pip3 --version</code>.</p> </li> <li> <p>Crea una carpeta principal denominada <code>Practicas-MLOPS</code> para almacenar todas las pr\u00e1cticas del curso. Dentro de esta carpeta, crea una subcarpeta llamada <code>Practica-1</code> y ub\u00edcate dentro de ella para realizar el trabajo correspondiente.</p> </li> <li> <p>Es importante saber editar archivos desde la terminal. La mayor\u00eda de los sistemas tiene instalado el editor <code>nano</code>; si no, identifica cu\u00e1l est\u00e1 disponible.</p> <ul> <li>Escribe <code>nano</code> en la terminal.</li> <li> <p>Escribe el siguiente texto en el script:</p> <pre><code>if __name__ == \"__main__\":\n    print(\"Hola mundo!\")\n</code></pre> </li> <li> <p>Guarda el script como <code>.py</code>, ejec\u00fatalo con <code>python3 &lt;nombre_archivo&gt;.py</code>.</p> </li> <li>Edita el archivo desde la terminal para cambiar el mensaje.</li> </ul> </li> <li> <p>Todas las terminales vienen con un lenguaje de programaci\u00f3n. El sistema m\u00e1s com\u00fan se llama <code>bash</code>, y saber escribir programas simples en bash puede ser muy \u00fatil. Por ejemplo, para ejecutar varios programas de Python de manera secuencial, puedes hacerlo mediante un script en bash.</p> <ul> <li> <p>Escribe un script en bash (usando <code>nano</code>) y gr\u00e1balo como <code>mi_script.sh</code>:</p> <pre><code>#!/bin/bash\n# Un script de ejemplo en bash\necho \"\u00a1Hola Mundo!\"\n</code></pre> </li> <li> <p>Averigua como ejecutarlo. Responde las preguntas en la plataforma virtual</p> </li> <li>Modifica el script <code>mi_script.sh</code> para llamar al programa Python que escribiste. Responde la pregunta en la plataforma virtual</li> <li>Escribe un nuevo script (usando <code>nano</code>) y gr\u00e1balo como <code>ciclo.sh</code>. El script debe implementar un bucle for que ejecute el script (<code>mi_script.sh</code>) 10 veces seguidas. Responda la pregunta en la plataforma virtual.</li> <li> <p>Ejecuta el nuevo script. La salida debe mostrase as\u00ed:</p> <p></p> </li> </ul> </li> <li> <p>Un truco que se va a necesitar a lo largo de este curso es establecer variables de entorno. Una variable de entorno es simplemente un valor con nombre din\u00e1mico que puede alterar el comportamiento de los procesos en ejecuci\u00f3n en una computadora. La sintaxis para establecer una variable de entorno depende del sistema operativo.</p> <ul> <li> <p>Establecer una variable de entorno e imprimirla.</p> <pre><code>export VARIABLE=hola\necho $VARIABLE\n</code></pre> </li> <li> <p>Para usar una variable de entorno en un programa Python, puedes usar la funci\u00f3n <code>os.environ</code> del m\u00f3dulo <code>os</code>. Escriba un programa Python (<code>programa.py</code>) que imprima la variable de entorno que acaba de establecer. Responda pregunta en la plataforma virtual.</p> </li> <li> <p>Si tiene una colecci\u00f3n de variables de entorno, \u00e9stas pueden almacenarse en un archivo <code>.env</code>. El archivo denominado, por ejemplo, <code>ventorno.env</code> deber\u00eda tener el siguiente formato:</p> <pre><code>VARIABLE_1=Hola\nVARIABLE_2=Mundo\n</code></pre> </li> </ul> </li> <li> <p>Para cargar las variables de entorno desde el archivo, puede utilizar el paquete <code>python-dotenv</code>. Puedes instalarlo usando <code>pip3 install python-dotenv</code> y luego intenta cargar las variables de entorno desde el archivo e imprimirlas. Crea el siguiente archivo en python que lea una de las variables del archivo <code>ventorno.env</code>.</p> <pre><code>import os\nfrom dotenv import load_dotenv\nload_dotenv(\u2018path_to_file_env\u2019)\nprint(os.environ[\"VARIABLE_1\"])\n</code></pre> <p>Responde en la plataforma virtual porqu\u00e9 no fue posible instalar el modulo <code>python-dotenv</code>.</p> </li> </ul> <p>Informaci\u00f3n</p> <p>La explicaci\u00f3n sobre el problema de instalaci\u00f3n del modulo <code>python-dotenv</code> lo abordaremos en el pr\u00f3ximo tema.</p>"},{"location":"ambiente/p2/","title":"Gesti\u00f3n de Paquetes y Entornos Virtuales","text":"<p>En esta pr\u00e1ctica continuamos nuestro recorrido por el mundo de las operaciones de aprendizaje autom\u00e1tico (MLOps). Antes de avanzar hacia procesos m\u00e1s complejos, es esencial contar con las herramientas adecuadas para instalar, actualizar y gestionar las dependencias de nuestros proyectos.</p> <p>Nos enfocaremos en el uso de gestores de paquetes como <code>conda</code> y <code>pip</code>, que nos permitir\u00e1n construir entornos reproducibles, mantener versiones compatibles de librer\u00edas y facilitar la colaboraci\u00f3n entre equipos. Estas habilidades ser\u00e1n clave para desarrollar proyectos robustos y eficientes a lo largo del curso de MLOps.</p>"},{"location":"ambiente/p2/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Esta pr\u00e1ctica tiene como objetivo familiarizarse con el uso de gestores de paquetes para instalar, gestionar y documentar dependencias en proyectos de MLOps, asegurando entornos consistentes y controlados para el desarrollo y despliegue de modelos.</p>"},{"location":"ambiente/p2/#instalacion-de-conda-o-miniconda","title":"Instalaci\u00f3n de Conda o Miniconda","text":"<ul> <li> <p>Descargue e instala <code>conda</code> o <code>miniconda</code>. <code>Conda</code> incluye muchos paquetes, mientras que <code>miniconda</code> es m\u00e1s ligera. Para los fines del curso se puede instalar <code>miniconda</code>.  Las instrucciones para la instalaci\u00f3n de Miniconda se encuentran en: https://www.anaconda.com/docs/getting-started/miniconda/install. </p> <p>Observaci\u00f3n-1</p> <p>Recuerde leer las instrucciones para Linux desde la terminal (Linux terminal installer).</p> <p>Observaci\u00f3n-2</p> <p>Cuando se le pregunte si desea modificar la configuraci\u00f3n del shell para inicializar conda cada vez que abras un nuevo shell y para reconocer los comandos de conda autom\u00e1ticamente, responda YES.</p> </li> <li> <p>Verifica la instalaci\u00f3n. Si no funciona, puede ser necesario configurar la variable de entorno correspondiente. Si se ha instalado con \u00e9xito <code>miniconda</code>, entonces deber\u00edas ser capaz de ejecutar el comando <code>conda --version</code> en la terminal.</p> <p></p> <p>Conda siempre indicara en que entorno est\u00e1s actualmente, indicado por (<code>env_name</code>) en el prompt. Por defecto, siempre se iniciar\u00e1 en el entorno (<code>base</code>).</p> </li> </ul>"},{"location":"ambiente/p2/#creacion-de-entornos-virtuales","title":"Creaci\u00f3n de entornos virtuales","text":"<ul> <li>Dentro del directorio principal denominada <code>Practicas-MLOPS</code> crea un subdirectorio <code>Practica-2</code> para alojar los ejercicios correspondientes a este tema.</li> </ul> <p>Las preguntas de las siguientes tareas responder en la plataforma virtual. Si no esta familiarizado con <code>conda</code> puede revisar informaci\u00f3n de los comandos principales.</p> <ul> <li> <p>Crear un nuevo entorno virtual. Aseg\u00fararse de que se llama <code>mi_entorno</code> y que instala la versi\u00f3n 3.11 de Python. </p> <ul> <li>\u00bfQu\u00e9 comando deber\u00edas ejecutar para hacer esto?</li> <li>\u00bfQu\u00e9 comando de conda da una lista de todos los entornos creados?</li> <li>\u00bfQu\u00e9 comando de conda se puede usar para activar el entorno creado?</li> <li>\u00bfQu\u00e9 comando de conda da una lista de los paquetes instalados en el entorno actual?</li> </ul> </li> <li> <p>Pruebe de instalar el paquete <code>dotenv</code> dentro del entorno. Lo recomendable es instalar el paquete usando <code>conda</code>, sin embargo, es bastante seguro usar <code>pip</code> dentro de conda hoy en d\u00eda. Use <code>pip</code> esta vez, y una vez instale el paquete cree un archivo de Python <code>programa_entorno.py</code> para cargar las variables de entorno desde el archivo <code>ventorno.env</code>. !! Recuerda la \u00faltima tarea de la pr\u00e1ctica previa !!.</p> </li> <li> <p>Verifica que el archivo de Python <code>programa_entorno.py</code> imprime una de las variables del archivo <code>ventorno.env</code>.</p> </li> <li> <p>\u00bfC\u00f3mo exportar f\u00e1cilmente la lista de paquetes instalados en el entorno a un archivo de texto? Aseg\u00farate de exportarlo a un archivo llamado <code>environment.yaml</code>, ya que conda usa otro formato por defecto que pip.</p> </li> <li> <p>Inspeccione el archivo para ver qu\u00e9 contiene. El archivo <code>environment.yaml</code> que has creado es una forma de asegurar la reproducibilidad entre usuarios porque cualquiera deber\u00eda ser capaz de obtener una copia exacta de tu entorno si tiene tu archivo <code>environment.yaml</code>. </p> </li> <li> <p>\u00bfEl paquete <code>dotenv</code> recientemente instalado en el entorno <code>conda</code> forma parte de la lista?. \u00bfPorqu\u00e9 no est\u00e1 en la lista si fuese el caso? Puedes crear un archivo <code>environment1.yaml</code> que contenga tambi\u00e9n el paquete <code>dotenv</code>.</p> </li> <li> <p>Intente crear un nuevo entorno (<code>mi_entorno_copia</code>) directamente desde su archivo <code>environment.yaml</code> y compruebe que los paquetes que se instalan coinciden exactamente con los que ten\u00eda originalmente.</p> </li> <li> <p>\u00bfCu\u00e1l es el comando pip correspondiente que da una lista de todos los paquetes <code>pip</code> instalados? \u00bfY c\u00f3mo exportar esto a un archivo <code>requirements.txt</code>?</p> </li> <li> <p>Si echas un vistazo a los requisitos que tanto <code>pip</code> como <code>conda</code> producen, ver\u00e1s que a menudo est\u00e1n llenos de muchos m\u00e1s paquetes de los que est\u00e1s usando en tu proyecto. Lo que te interesa son los paquetes que importas en tu c\u00f3digo: <code>from package import module</code>. Una forma de evitar esto es usar el paquete <code>pipreqs</code>, que autom\u00e1ticamente escanear\u00e1 tu proyecto y crear\u00e1 un archivo de requisitos espec\u00edfico para \u00e9l.  </p> </li> <li> <p>Luego de instalar <code>pipreqs</code> en el entorno de copia, pruebe este paquete sobre el c\u00f3digo de los archivos de Python de la carpeta en la Practica-1.</p> </li> <li> <p>\u00bfQu\u00e9 aspecto tiene el archivo <code>requirements.txt</code> que produce <code>pipreqs</code> comparado con los archivos producidos por <code>pip</code> o <code>conda</code>?</p> </li> </ul> <p>Importante</p> <p>Si bien los m\u00e9todos mencionados en los ejercicios son excelentes maneras de construir archivos de requisitos autom\u00e1ticamente, a veces es m\u00e1s f\u00e1cil sentarse y crear manualmente los archivos, ya que de esa manera te aseguras de que s\u00f3lo se instalen los requisitos m\u00e1s necesarios al crear un nuevo entorno.</p>"},{"location":"ambiente/p3/","title":"Editor","text":"<p>En esta pr\u00e1ctica se describen algunos ejercicios para que empieces a familiarizarte con el editor que has elegido. Si ya eres un experto en uno de ellos, ser\u00eda ideal que se complete tambi\u00e9n los ejercicios. Al menos deber\u00edas ser capaz de:</p> <ul> <li>Crear un nuevo archivo</li> <li>Ejecutar un script/archivo Python</li> <li>Hacer algun cambio y verificar errores</li> </ul> <p>Aunque las instrucciones est\u00e1n pensadas para <code>Visual Studio Code</code> (recomendaci\u00f3n del curso), se sugiere responder las preguntas incluso usando otro editor. Las tareas planteadas son solo el inicio; hay tutoriales disponibles para continuar aprendiendo.</p>"},{"location":"ambiente/p3/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Esta pr\u00e1ctica tiene como objetivo familiarizarse con el uso de editores de c\u00f3digo como <code>Visual Studio Code</code> (en adelante, <code>VS Code</code>), herramienta fundamental para escribir, ejecutar y depurar c\u00f3digo en proyectos de MLOps.</p>"},{"location":"ambiente/p3/#instalacion-de-vs-code","title":"Instalaci\u00f3n de VS Code","text":"<p>Ahora lo primero es instalar <code>VSCode</code>. </p>"},{"location":"ambiente/p3/#usuarios-de-wsl","title":"Usuarios de WSL","text":"<p>En caso de utilizar WSL, le recomendamos seguir las instrucciones de esta gu\u00eda. Recuerda que si tienes Windows ya instalamos previamente WSL para trabajar en un ambiente de Linux. Ahora tienes que instalar <code>VSCode</code> sobre Windows y luego instalar algunas extensiones para conectarse con WSL. </p> <p>Importante</p> <p>En la gu\u00eda se muestra dos formas para abrir una carpeta o espacio de trabajo remoto. Se sugiere probar que ambas opciones funcionen.</p>"},{"location":"ambiente/p3/#usuarios-de-linux","title":"Usuarios de Linux","text":"<p>Si est\u00e1s trabajando en Linux directamente te recomendamos seguir las instruciones de instalaci\u00f3n, dependiendo de la distribuci\u00f3n utilizada.</p>"},{"location":"ambiente/p3/#uso-de-vs-code","title":"Uso de VS Code","text":""},{"location":"ambiente/p3/#usuarios-de-wsl_1","title":"Usuarios de WSL","text":"<p>En caso de trabajar con WSL, siga el tutorial Working in WSL, que gu\u00eda en la creaci\u00f3n de una aplicaci\u00f3n Hello World en WSL. Todo lo descrito en el tutorial en las secciones <code>Prerequisites</code>, <code>Enable WSL</code> y <code>Install a Linux distro</code> ya fueron abordados en los pasos previos. Por tanto, se puede completar el tutorial desde la secci\u00f3n <code>Python development</code>, teniendo en cuenta que Python ya fue instalado previamente al configurar la distribuci\u00f3n de Linux mediante WSL.  En la gu\u00eda se utiliza la carpeta <code>helloWord</code> como espacio de trabajo, pero en su lugar, crea una subcarpeta llamada <code>Practica-3</code> dentro del directorio <code>Practica-MLOPS</code>.</p> <p>Importante</p> <p>Es altamente recomendable cerrar la conexi\u00f3n cuando se termine de trabajar con VS Code. Puedes finalizar tu sesi\u00f3n en WSL y volver a ejecutar VS Code de forma local seleccionando File &gt; Close Remote Connection.</p>"},{"location":"ambiente/p3/#usuarios-de-linux_1","title":"Usuarios de Linux","text":"<p>En caso de trabajar con Linux, le recomendamos revise este tutorial desde la secci\u00f3n <code>Install a language extension</code>. El espacio de trabajo para crear el archivo <code>hello.py</code> (descrito en la gu\u00eda) debe estar en el directorio <code>Practica-3</code> dentro de la carpeta <code>Practica-MLOPS</code>.</p>"},{"location":"arquitectura/flujo_proyecto/","title":"Arquitectura del Proyecto","text":"<p>La siguiente arquitectura es la base de las pr\u00e1cticas del curso de MLOps de la Maestr\u00eda en Ciencia de Datos, y permite estructurar de manera clara y ordenada cada etapa del ciclo de vida de un proyecto de aprendizaje autom\u00e1tico. Desde la comprensi\u00f3n del problema de negocio y la preparaci\u00f3n de los datos, hasta el entrenamiento, despliegue, monitoreo y mejora continua del modelo, esta arquitectura permite aplicar los principios de MLOps de forma progresiva. Cada componente de este flujo gu\u00eda a los estudiantes en el uso de herramientas espec\u00edficas para automatizar procesos, garantizar la reproducibilidad y facilitar la integraci\u00f3n de modelos en entornos reales. La representaci\u00f3n visual de esta arquitectura sirve como referencia central para entender c\u00f3mo se conectan y orquestan todos los elementos del proyecto.</p> <p></p>"},{"location":"arquitectura/flujo_proyecto/#estructura-practica-del-curso-basada-en-la-arquitectura","title":"Estructura pr\u00e1ctica del curso basada en la arquitectura","text":"<p>A continuaci\u00f3n, se detalla la relaci\u00f3n entre las pr\u00e1cticas propuestas y la arquitectura presentada. </p> <ul> <li> <p>Las tres primeras pr\u00e1cticas (Linea de Comandos,      Gesti\u00f3n de Paquetes y Entornos Virtuales y Editor est\u00e1n orientadas a que los estudiantes comprendan los conceptos fundamentales y se familiaricen con las herramientas necesarias para la creaci\u00f3n de un entorno de desarrollo adecuado, que servir\u00e1 como base para el resto del ciclo de vida del proyecto de aprendizaje autom\u00e1tico. </p> </li> <li> <p>Problema del Negocio. En la pr\u00e1ctica Alineaci\u00f3n Estrat\u00e9gica del Proyecto, se utilizar\u00e1n las herramientas AI Canvas y/o ML Canvas con el objetivo de alinear los objetivos del proyecto con una necesidad concreta del negocio, comprender mejor el contexto del problema y analizar los datos disponibles para su desarrollo.</p> <p>Adem\u00e1s, como parte de la preparaci\u00f3n inicial, se ejecutar\u00e1n tareas clave de configuraci\u00f3n del proyecto, incluyendo:</p> <ul> <li> <p>La creaci\u00f3n del entorno virtual utilizando <code>Conda</code>, lo que permitir\u00e1 una gesti\u00f3n adecuada de las dependencias del proyecto, y</p> </li> <li> <p>La definici\u00f3n de la estructura del proyecto mediante <code>Cookiecutter</code>, siguiendo buenas pr\u00e1cticas de organizaci\u00f3n del c\u00f3digo y recursos en proyectos de machine learning.</p> </li> </ul> </li> <li> <p>Datos. Para este proyecto de predicci\u00f3n, se utiliza el conjunto de datos Supply Chain Shipment Pricing Data, que contiene informaci\u00f3n detallada sobre env\u00edos de mercanc\u00edas. Este dataset es especialmente \u00fatil para tareas de modelado predictivo relacionadas con el c\u00e1lculo o estimaci\u00f3n de costos log\u00edsticos.</p> <p>Algunas de la variables incluidas en este conjunto de datos son:</p> <ul> <li>Tipo de env\u00edo: define la modalidad del env\u00edo (por ejemplo, urgente, regular).</li> <li>Origen y destino: indica las ubicaciones geogr\u00e1ficas involucradas en el transporte.</li> <li>Peso y volumen del env\u00edo: caracter\u00edsticas f\u00edsicas relevantes para el c\u00e1lculo de tarifas y la log\u00edstica.</li> <li>Fecha y hora del env\u00edo: permiten explorar aspectos temporales como estacionalidad o plazos de entrega.</li> <li>M\u00e9todo de transporte: medio utilizado (a\u00e9reo, mar\u00edtimo, terrestre, etc.).</li> <li>Costos asociados: variable objetivo que se desea predecir, relacionada con el precio final del env\u00edo.</li> </ul> <p>Este conjunto de datos es el insumo clave para el desarrollo del modelo que permite automatizar la estimaci\u00f3n de costos y optimizar procesos dentro de la cadena de suministro. En la pr\u00e1ctica Versionado del C\u00f3digo, se implementan los scripts necesarios para cargar y preparar esta fuente de datos durante el proceso de entrenamiento del modelo.</p> </li> <li> <p>Entrenamiento. El proceso de entrenamiento en un flujo MLOps, abarca desde la recopilaci\u00f3n y preparaci\u00f3n de los datos hasta la generaci\u00f3n de un modelo listo para producci\u00f3n. En el contexto del curso de MLOps, este proceso se fortalece mediante la aplicaci\u00f3n de pr\u00e1cticas esenciales que aseguran reproducibilidad, trazabilidad y escalabilidad.</p> <p>Durante la pr\u00e1ctica de Versionado del C\u00f3digo, se establecen mecanismos para registrar cambios en los scripts de entrenamiento, facilitando la colaboraci\u00f3n y el control de modificaciones. Con Empaquetado y gesti\u00f3n de dependencias, se garantiza que los entornos sean reproducibles y portables, lo cual es clave para ejecutar el entrenamiento en distintos entornos sin errores.</p> <p>La pr\u00e1ctica de Buenas Pr\u00e1cticas de Codificaci\u00f3n mejora la legibilidad, mantenibilidad y robustez del c\u00f3digo usado en la ingesta, preprocesamiento y entrenamiento. Por otro lado, Almacenamiento remoto con DVC y Control de versiones de datos permiten manejar datasets grandes de forma eficiente y controlada, asegurando que se pueda reproducir exactamente cualquier experimento.</p> <p>Con Trabajando con Pipelines, se estructuran las etapas del flujo de datos y entrenamiento como tareas modulares, lo que facilita la automatizaci\u00f3n y escalabilidad del proceso. Finalmente, el Control de Versiones del Modelo permite registrar cada versi\u00f3n entrenada con sus m\u00e9tricas y configuraci\u00f3n, lo que es vital para hacer seguimiento y decidir qu\u00e9 modelo es apto para desplegar.</p> <p>Estas pr\u00e1cticas integradas fortalecen cada paso del entrenamiento, desde los datos hasta el modelo final, asegurando un flujo de trabajo robusto y alineado con los principios de MLOps.</p> </li> <li> <p>Despliegue. La fase de Despliegue representa el proceso de poner el modelo entrenado en producci\u00f3n para que pueda ser consumido por aplicaciones o usuarios finales. Esta etapa incluye:</p> <ul> <li> <p>Punto de Acceso al Modelo: el modelo se expone a trav\u00e9s de un servicio, usualmente como una API, facilitando su integraci\u00f3n en sistemas reales.</p> </li> <li> <p>Imagen Docker: el modelo, junto con su entorno y dependencias, se empaqueta en un contenedor, asegurando portabilidad y consistencia en distintos entornos.</p> </li> <li> <p>Configuraci\u00f3n del Despliegue y Proveedor de Servicios en la Nube: se define la infraestructura en la nube donde se alojar\u00e1 el modelo, permitiendo escalar y monitorear el servicio.</p> </li> <li> <p>Servicio (API Endpoint): es el punto final donde se reciben solicitudes y se devuelven predicciones, habilitando el uso pr\u00e1ctico del modelo en tiempo real.</p> </li> </ul> <p>Esta fase es clave para cerrar el ciclo del flujo MLOps y conectar el modelo con el mundo real.</p> </li> <li> <p>Monitoreo. Esta fase se encarga de supervisar el comportamiento del modelo una vez desplegado. Esta incluye:</p> <ul> <li> <p>Monitoreo del Modelo: permite detectar desviaciones en el rendimiento del modelo (por ejemplo, por cambios en los datos de entrada), lo que puede indicar necesidad de reentrenamiento.</p> </li> <li> <p>Seguimiento del Modelo: registra m\u00e9tricas clave, como precisi\u00f3n, latencia o tasa de error, a lo largo del tiempo para asegurar que el modelo sigue cumpliendo con los objetivos del negocio.</p> </li> </ul> <p>Esta etapa es fundamental para mantener la calidad y confiabilidad del sistema de IA/ML en producci\u00f3n.</p> </li> </ul>"},{"location":"entorno/p5/","title":"Creaci\u00f3n del Entorno Virtual","text":"<p>Una tarea basica en el flujo de trabajo MLOps es la creaci\u00f3n de un entorno virtual, incluso antes de establecer la estructura base del proyecto. Este paso es fundamental para asegurar un entorno aislado y reproducible desde el inicio. Este entorno permite aislar las dependencias del proyecto y evitar conflictos con otras configuraciones del sistema. Usaremos Conda, un gestor de entornos ampliamente adoptado en la ciencia de datos, por su capacidad para manejar tanto paquetes de Python como binarios del sistema.</p> <p>Crear un entorno virtual no solo mejora la organizaci\u00f3n y portabilidad del proyecto, sino que tambi\u00e9n garantiza que todos los colaboradores trabajen bajo las mismas condiciones, lo cual es clave para la reproducibilidad, la consistencia y el despliegue confiable de modelos.</p>"},{"location":"entorno/p5/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Configurar un entorno virtual utilizando Conda para gestionar las dependencias del proyecto de forma controlada, asegurando un desarrollo reproducible y colaborativo.</p>"},{"location":"entorno/p5/#crear-y-activar-entorno-del-proyecto","title":"Crear y activar entorno del proyecto","text":"<p>Trabajar dentro de un entorno virtual asegura que el proyecto utilice \u00fanicamente las versiones espec\u00edficas de las librer\u00edas requeridas, evitando conflictos con otras configuraciones del sistema. Esto resulta especialmente crucial en contextos colaborativos, ya que permite que todos los miembros del equipo trabajen bajo las mismas condiciones, lo que facilita tanto la reproducibilidad del proyecto como su mantenimiento.</p> <p>Responde las preguntas a estas taraes en la Plataforma Virtual:</p> <ol> <li>Abrir una terminal (o una terminal de WSL si est\u00e1s trabajando Linux remotamente sobre Windows).</li> <li>Suponga que el proyecto estar\u00e1 alojado dentro de la carpeta <code>Pr\u00e1ctica-5</code>, la cual es parte del directorio principal <code>Practicas-MLOPS</code>. </li> <li> <p>Crear el entorno virtual, usando la siguiente informaci\u00f3n:</p> <pre><code>nombre entorno: mlops_env\nversi\u00f3n Pythton: 3.11\n</code></pre> </li> <li> <p>Activa el entorno creado</p> </li> <li>Verifica que el entorno est\u00e1 activo, observando que su nombre aparece al inicio de la l\u00ednea de comandos.</li> <li>A partir de este momento, todas las librer\u00edas necesarias para el proyecto deber\u00e1n instalarse dentro de este entorno usando <code>conda install</code> o <code>pip install</code>.</li> </ol>"},{"location":"entorno/p5/#tarea","title":"\ud83d\udee0\ufe0f Tarea","text":"<p>Como parte de los requerimientos del proyecto, necesitas crear un archivo de Python llamado <code>estadisticas.py</code>. Este archivo debe utilizar la biblioteca <code>pandas</code> para construir un peque\u00f1o DataFrame con datos de ejemplo y mostrar estad\u00edsticas descriptivas b\u00e1sicas.</p> <p>Para facilitar la implementaci\u00f3n se usar\u00e1 VS Code</p> <ol> <li>Abre VS Code y aseg\u00farate de estar trabajando dentro de la carpeta <code>Practica-5</code>.</li> <li>Abre la terminal integrada en VS Code (Ctrl + o Ver &gt; Terminal), y activa el entorno de conda creado previamente.</li> <li>Instala la biblioteca <code>pandas</code> version=1.5.3 en el entorno virtual.</li> <li>Verifica que el entorno tiene instalada la biblioteca <code>pandas</code></li> <li>Crea el archivo <code>estadisticas.py</code> dentro de la carpeta actual.</li> <li> <p>Escribe el siguiente c\u00f3digo en <code>estadisticas.py</code>:</p> <pre><code>import pandas as pd\n\n# Crear un DataFrame con datos de ejemplo\ndatos = {\n    'Nombre': ['Ana', 'Luis', 'Carlos', 'Marta'],\n    'Edad': [23, 35, 29, 42],\n    'Ingresos': [2500, 4000, 3200, 5000]\n}\n\ndf = pd.DataFrame(datos)\n\n# Mostrar el DataFrame\nprint(\"Datos del DataFrame:\")\nprint(df)\n\n# Mostrar estad\u00edsticas descriptivas\nprint(\"\\nEstad\u00edsticas b\u00e1sicas:\")\nprint(df.describe())\n\n# Edad promedio\nprint(f\"\\nEdad promedio: {df['Edad'].mean():.2f} a\u00f1os\")\n</code></pre> </li> <li> <p>Antes de ejecutar no se olvide de seleccionar el interprete <code>mlops_env</code> que contiene todas las librerias necesarias. En la barra inferior izquierda de VS Code, localiza la secci\u00f3n que indica el int\u00e9rprete actual de Python.</p> <p></p> </li> </ol>"},{"location":"entorno/p5/#como-compartir-el-entorno-con-conda","title":"C\u00f3mo compartir el entorno con Conda","text":"<p>En un proyecto colaborativo, es fundamental que todos los miembros trabajen en un entorno de desarrollo id\u00e9ntico. Para lograrlo, se utiliza un archivo <code>environment.yml</code>, que documenta la configuraci\u00f3n exacta del entorno Conda (incluyendo Python y todas las librer\u00edas instaladas). Este archivo facilita la reproducci\u00f3n del entorno en diferentes equipos y evita errores por incompatibilidades.</p> <p>\ud83d\udcdd Esta explicaci\u00f3n fue introducida en la pr\u00e1ctica Ambiente de Desarrollo / Gestor de Paquetes.</p>"},{"location":"entorno/p5/#tarea_1","title":"\ud83d\udee0\ufe0f Tarea","text":"<p>Como l\u00edder del proyecto, una vez que has finalizado la implementaci\u00f3n y verificado que todo funciona correctamente, es momento de compartir el entorno de Conda con el resto del equipo. Para ello, debes exportar el entorno a un archivo <code>environment.yml</code>. Este archivo permitir\u00e1 que tus compa\u00f1eros puedan reproducir las mismas condiciones de trabajo en sus propias m\u00e1quinas y continuar con el desarrollo sin contratiempos.</p> <ul> <li>\u00bfQu\u00e9 comando usar\u00edas para generar este archivo?</li> <li> <p>Si revisas el archivo <code>environment.yml</code>, ver\u00e1s que contiene muchos paquetes, pero en este ejercicio solo hemos instalado dos de forma expl\u00edcita:</p> <ul> <li>python=3.11 al momento de configurar el entorno.</li> <li>pandas=1.5.3 para implementar las estad\u00edsticas del proyecto.</li> </ul> <p>\u00bfQu\u00e9 comando usar para exportar \u00fanicamente los paquetes que instalaste directamente? (y no todas las dependencias internas que Conda a\u00f1adi\u00f3 autom\u00e1ticamente).</p> </li> </ul> <p>\ud83d\udcdd Una vez que el archivo <code>environment.yml</code> ha sido creado y compartido, cada colaborador debe clonar el repositorio del proyecto (este proceso se detalla en la pr\u00e1ctica: Versionado de C\u00f3digo), ubicarse en la ra\u00edz del proyecto y ejecutar el siguiente comando para recrear el entorno Conda de forma id\u00e9ntica: <pre><code>conda env create -f environment.yml\n</code></pre> Esto crea un nuevo entorno con el mismo nombre y configuraci\u00f3n que el original.</p> <p>\ud83d\udca1 Consejo: Si deseas usar un nombre distinto para el entorno, puedes usar: <pre><code> conda env create -f environment.yml -n &gt;nombre_personalizado\n</code></pre></p> <p>Una vez creado el entorno, act\u00edvalo con: <pre><code>conda activate nombre_entorno\n</code></pre> As\u00ed, todos los integrantes del equipo estar\u00e1n trabajando en condiciones id\u00e9nticas.</p>"},{"location":"entorno/p6/","title":"Estructura del Proyecto","text":"<p>En esta pr\u00e1ctica, estableceremos la base para un proyecto de predicci\u00f3n de precios de consignaci\u00f3n utilizando herramientas modernas de MLOps. Una vez configurado el entorno de trabajo, el siguiente paso es generar la estructura del proyecto. Para ello, utilizaremos Cookiecutter, una herramienta que permite clonar plantillas con buenas pr\u00e1cticas predefinidas, facilitando as\u00ed una organizaci\u00f3n clara y escalable desde el inicio.</p>"},{"location":"entorno/p6/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<ul> <li>Instalar y usar Cookiecutter desde la terminal.</li> <li>Clonar un template base de proyecto MLOps.</li> <li>Explorar e interpretar la estructura generada.</li> </ul>"},{"location":"entorno/p6/#requisitos-previos","title":"Requisitos previos","text":"<ul> <li>Tener instalado y configurado WSL o Linux.</li> <li>Tener instalado Conda (miniconda o anaconda).</li> </ul>"},{"location":"entorno/p6/#estructura-del-proyecto","title":"Estructura del Proyecto","text":"<p>Para comenzar con la configuraci\u00f3n de un proyecto MLOPS, existen varias formas de crear la carpeta principal</p> <ul> <li> <p>Crear desde cero: Opci\u00f3n directa que implica definir manualmente la estructura y archivos del proyecto. No recomendada para proyectos medianos o grandes debido a su dificultad de mantenimiento y escalabilidad.</p> </li> <li> <p>Importar una plantilla existente (recomendada): Permite estructurar el proyecto de forma eficiente, facilita la colaboraci\u00f3n, y mejora la transparencia, reproducibilidad y reutilizaci\u00f3n. Es la opci\u00f3n sugerida para esta pr\u00e1ctica.</p> </li> <li> <p>Clonar o hacer fork de un proyecto existente: Ideal para continuar o adaptar proyectos ya desarrollados. Soporta colaboraci\u00f3n activa y reutilizaci\u00f3n de c\u00f3digo. Ser\u00e1 usada m\u00e1s adelante en el curso.</p> </li> </ul>"},{"location":"entorno/p6/#instalacion-de-cookiecutter","title":"Instalaci\u00f3n de Cookiecutter","text":"<p>En esta pr\u00e1ctica emplearemos una estructura de proyecto basada en una plantilla de MLOps que puede ser generada utilizando la herramienta cookiecutter. Usaremos el repositorio Cookiecutter MLOps, el cual est\u00e1 dise\u00f1ado espec\u00edficamente para proyectos de aprendizaje autom\u00e1tico que requieren una buena organizaci\u00f3n desde el inicio. </p> <ul> <li> <p>Abre una terminal para crear la carpeta <code>Practica-6</code> dentro de <code>Practicas-MLOPS</code>. En esta carpeta se crear\u00e1 la estructura del proyecto.</p> </li> <li> <p>En la misma terminal, instala <code>cookiecutter</code>. Hay dos opciones:</p> <pre><code>pip3 install cookiecutter\nconda install -c conda-forge cookiecutter\n</code></pre> <p>IMPORTANTE</p> <p>Se recomienda instalar herramientas como <code>cookiecutter</code> desde el canal <code>conda-forge</code>, ya que este canal suele ofrecer versiones m\u00e1s actualizadas, bien mantenidas y con mejor compatibilidad entre paquetes en comparaci\u00f3n con <code>pip</code> o con el canal por defecto de Conda (defaults). Esto ayuda a evitar conflictos y facilita la gesti\u00f3n del entorno.</p> </li> <li> <p>Para clonar la plantilla que contiene la estructura del proyecto puede usar el siguiente comando:</p> <pre><code>cookiecutter https://github.com/Chim-SO/cookiecutter-mlops\n</code></pre> </li> <li> <p>Durante la generaci\u00f3n del proyecto con la plantilla de Cookiecutter, se te pedir\u00e1 que proporciones varios par\u00e1metros de configuraci\u00f3n. Responde a las preguntas que aparecen en consola. Si no deseas modificar un par\u00e1metro, simplemente presiona Enter para aceptar su valor predeterminado. La tabla adjunta tiene un ejemplo de c\u00f3mo llenar los valores.</p> Pregunta Ejemplo de respuesta project_name ProyectoMLOPS repo_name [project_name] author_name Mauricio_Espinoza description Proyecto de predicci\u00f3n usando MLOps Select open_source_license:1 - MIT2 - BSD-3-Clause3 - No license fileChoose from 1, 2, 3 [1]: 1 s3_bucket [[OPTIONAL] your-bucket-for-syncing-data (do not include 's3://')]: aws_profile [default]: Select python_interpreter:1 - python32 - pythonChoose from 1, 2 [1]: <p>Nota1:</p> <p>El campo s3_bucket [[OPTIONAL] your-bucket-for-syncing-data (do not include 's3://')]: es opcional y se refiere a un bucket de Amazon S3 que puedes usar para sincronizar datos de tu proyecto, como archivos de entrenamiento, modelos o resultados. Si tu proyecto no utilizar\u00e1 almacenamiento en la nube (o a\u00fan no tienes un bucket configurado), puedes dejar este campo en blanco. Solo presiona Enter para aceptar el valor predeterminado y omitir esta configuraci\u00f3n por ahora.</p> <p>Nota2:</p> <p>El campo aws_profile [default]: se refiere al perfil de configuraci\u00f3n de AWS que se puede usar para interactuar con servicios en la nube, como Amazon S3. Estos perfiles se definen en el archivo ~/.aws/credentials y permiten gestionar distintas cuentas o configuraciones de acceso a AWS desde la misma m\u00e1quina. Si ya tienes configurado un perfil llamado \u201cdefault\u201d (que es el caso m\u00e1s com\u00fan), puedes simplemente presionar Enter para aceptarlo. Si usas otro perfil, escribe su nombre aqu\u00ed. Si a\u00fan no has configurado ning\u00fan perfil de AWS o no planeas usar servicios de AWS en este proyecto, tambi\u00e9n puedes dejarlo en blanco presionando Enter.</p> </li> <li> <p>Se debe haber creado una carpeta con el nombre del proyecto asignado. Ingresa a dicha carpeta y prueba abrirla en VS Code utilizando el comando <code>code .</code> desde la terminal.</p> </li> </ul>"},{"location":"entorno/p6/#tarea","title":"\ud83d\udee0\ufe0f Tarea","text":"<p>Sup\u00f3n que has descubierto recientemente la herramienta Cookiecutter, la cual permite generar estructuras de proyecto preconfiguradas a partir de plantillas que siguen buenas pr\u00e1cticas de desarrollo. Deseas aprovechar esta herramienta para organizar adecuadamente tu proyecto de Machine Learning y facilitar el trabajo colaborativo.</p> <p>Para continuar con la implementaci\u00f3n del proyecto, decides reutilizar el entorno Conda creado previamente en la pr\u00e1ctica Creaci\u00f3n del Entorno Virtual, ya que en \u00e9l se encuentran instaladas las dependencias principales como Python y pandas. A partir de ese entorno, continuar\u00e1s trabajando para estructurar el proyecto correctamente. </p> <p>\u00bfEs necesario instalar Cookiecutter dentro del entorno Conda a pesar que esta herramienta solo se utiliza al inicio del proyecto para generar la estructura y no forma parte del c\u00f3digo fuente que se ejecutar\u00e1? Justifica razonadamente tu respuesta.</p>"},{"location":"entorno/p6/#explorar-la-estructura-del-proyecto","title":"Explorar la estructura del proyecto","text":"<p>Revisa la estructura del proyecto usando VS Code, deber\u00e1s encontrar estas carpetas:</p> <ul> <li> <p><code>configs/</code>: Esta carpeta puede contener archivos de   configuraci\u00f3n, como los hiperpar\u00e1metros del modelo. Aqu\u00ed se definen variables clave para la experimentaci\u00f3n y entrenamiento, y facilita modificar la configuraci\u00f3n sin alterar el c\u00f3digo fuente.</p> </li> <li> <p><code>src/data/</code>: Esta subcarpeta agrupa todos los pasos del procesamiento de datos:</p> <ul> <li><code>ingestion.py</code>: se encarga de reunir los datos. Aqu\u00ed tambi\u00e9n puedes a\u00f1adir funcionalidades como respaldos, anonimizaci\u00f3n de datos sensibles o generaci\u00f3n de metadatos.</li> <li><code>cleaning.py</code>: limpia los datos, reduce el ruido y maneja valores at\u00edpicos o faltantes.</li> <li><code>labeling.py</code>: asigna etiquetas a los datos si es necesario.</li> <li><code>splitting.py</code>: divide los datos en conjuntos de entrenamiento y prueba.</li> <li><code>validation.py</code>: valida que los datos est\u00e9n completos y en formato adecuado para el entrenamiento.</li> <li><code>build_features.py</code>: organiza y transforma los datos en caracter\u00edsticas \u00fatiles para el modelo.</li> </ul> </li> <li> <p><code>src/models/</code>: Contiene los scripts espec\u00edficos del modelo.</p> <ul> <li><code>model.py</code>: define la arquitectura del modelo.</li> <li><code>dataloader.py</code>: prepara los datos para ser usados en el modelo.</li> <li><code>preprocessing.py</code>: incluye funciones de preprocesamiento espec\u00edficas de este modelo</li> <li><code>train.py</code>: entrena el modelo con los datos.</li> <li><code>hyperparameters_tuning.py</code>: ajusta los hiperpar\u00e1metros del modelo.</li> <li><code>predict.py</code>: realiza predicciones sobre nuevas instancias (por ejemplo, im\u00e1genes) externas al conjunto de datos original.</li> </ul> </li> <li> <p><code>src/visualization/</code></p> <ul> <li><code>exploration.py</code>: visualiza los datos para entender su distribuci\u00f3n y caracter\u00edsticas durante la etapa de ingenier\u00eda.</li> <li><code>evaluation.py</code>: visualiza resultados del entrenamiento y ayuda a interpretar el rendimiento del modelo.</li> </ul> </li> </ul> <p>Notas importantes</p> <ul> <li>Esta plantilla es b\u00e1sica y puede adaptarse seg\u00fan las necesidades del proyecto: puedes eliminar o a\u00f1adir carpetas y scripts.</li> <li>Si varias arquitecturas de modelo usan el mismo preprocesamiento, puedes mover el archivo <code>preprocessing.py</code> a la carpeta <code>data</code> para evitar duplicaci\u00f3n, aunque mantenerlo separado puede facilitar la reutilizaci\u00f3n y evitar errores.</li> <li>El script <code>predict.py</code> asume que la predicci\u00f3n se hace sobre datos externos a la base original (como los enviados por una app), por lo que puede requerir pasos adicionales de preprocesamiento.</li> </ul>"},{"location":"entorno/p6/#tarea_1","title":"\ud83d\udee0\ufe0f Tarea","text":"<ul> <li> <p>Responde las preguntas propuestas en la plataforma virtual sobre la estructura de la plantilla bas\u00e1ndote en tu criterio t\u00e9cnico. Puedes apoyarte en la documentaci\u00f3n proporcionada por el autor en el repositorio del proyecto en GitHub para justificar tus respuestas.</p> </li> <li> <p>Copia el archivo <code>estadisticas.py</code> generado en la pr\u00e1ctica Creaci\u00f3n del Entorno Virtual dentro de una de las carpetas del proyecto creado con Cookiecutter. \u00bfEn donde deber\u00eda alojarse este archivo? Justifica tu respuesta.</p> </li> <li> <p>Una vez copiado el archivo, accede al mismo y verifica que se ejecuta correctamente desde la terminal o directamente desde VS Code, asegur\u00e1ndote de que est\u00e9 activado el entorno Conda correspondiente.</p> </li> <li> <p>Copia el archivo <code>environment.yml</code> generado en la pr\u00e1ctica anterior a la ra\u00edz del proyecto. \u00bfQu\u00e9 comando puedes utilizar para actualizar el entorno Conda actual?</p> </li> <li> <p>Verifica que el archivo <code>environment.yml</code> contiene ahora tres librerias.</p> <ul> <li>python=3.11</li> <li>pandas=1.5.3</li> <li>cookiecuttier</li> </ul> </li> </ul>"},{"location":"entorno/p6/#control-de-versiones-y-github","title":"Control de versiones y GitHub","text":"<p>Esta estructura de proyecto que has creado y organizado usando la plantilla Cookiecutter, puede ser f\u00e1cilmente integrado con un sistema de control de versiones como Git, y alojado en una plataforma como GitHub. Esto te permitir\u00e1 continuar con el desarrollo del proyecto, mantener un historial de cambios, y colaborar con otros de manera m\u00e1s efectiva.</p> <p>Nota informativa</p> <p>A continuaci\u00f3n, se describen los pasos y comandos esenciales para subir tu proyecto local a un nuevo repositorio vac\u00edo en GitHub. No los ejecutes a\u00fan, ya que se utilizar\u00e1n m\u00e1s adelante en otra pr\u00e1ctica.</p> <ol> <li>Inicializa un repositorio local (si a\u00fan no lo hiciste): <code>git init</code></li> <li>Agrega los archivos al repositorio: <code>git add .</code></li> <li>Crea un commit inicial: <code>git commit -m \"Primer commit del proyecto de predicci\u00f3n de precios de consignaci\u00f3n\"</code></li> <li>Agrega la URL de tu repositorio remoto (c\u00e1mbiala por la tuya): <code>git remote add origin https://github.com/tu-usuario/tu-repo.git</code></li> <li>Sube tu proyecto: <code>git push -u origin main</code></li> </ol> <p>Si el branch por defecto en tu GitHub es main, aseg\u00farate de estar en ese branch localmente usando <code>git branch -M main.</code></p> <p>En la pr\u00e1ctica Versionado del C\u00f3digo, en lugar de crear un proyecto desde cero, trabajaremos clonando un proyecto existente en GitHub basado en esta misma plantilla. Esto refleja un flujo de trabajo com\u00fan en la industria, donde se parte de un repositorio compartido para contribuir con nuevas funcionalidades, en este caso, enfocadas en implementar el proceso de predicci\u00f3n.</p>"},{"location":"entrenamiento/p7/","title":"Versionado del C\u00f3digo","text":"<p>En esta pr\u00e1ctica trabajaremos con Git y GitHub para simular un entorno colaborativo en el desarrollo de un proyecto de Machine Learning para la Predicci\u00f3n de Precios en Art\u00edculos de Consignaci\u00f3n (ver detalles en AI/ML canvas). Cada estudiante trabajar\u00e1 con su propia copia del repositorio inicial, desarrollar\u00e1 funcionalidades en una rama independiente y colaborar\u00e1 con sus compa\u00f1eros para integrar esos cambios al proyecto principal.</p> <p>Es importante destacar que la colaboraci\u00f3n en proyectos de programaci\u00f3n requiere una forma de trabajo organizada sobre una misma base de c\u00f3digo. Para lograrlo, se utilizan los sistemas de control de versiones, y Git es uno de los m\u00e1s utilizados en la actualidad.</p> <p>Git permite registrar de forma clara:</p> <ul> <li>Qui\u00e9n hizo un cambio en el c\u00f3digo.</li> <li>Cu\u00e1ndo se realiz\u00f3 ese cambio.</li> <li>Qu\u00e9 se modific\u00f3 exactamente.</li> </ul> <p>Estas capacidades facilitan el trabajo en equipo, permiten mantener un historial completo de cambios y hacen posible experimentar sin poner en riesgo la versi\u00f3n principal del proyecto.</p> <p>Para profundizar m\u00e1s en el uso de Git, puedes consultar el libro oficial de Git o explorar tutoriales interactivos disponibles en l\u00ednea.</p>"},{"location":"entrenamiento/p7/#git-vs-github","title":"Git vs GitHub","text":"<p>Es fundamental distinguir entre Git y GitHub:</p> <ul> <li>Git es la herramienta de control de versiones.</li> <li>GitHub es una plataforma web que permite alojar repositorios Git, colaborar con otros desarrolladores y revisar cambios desde una interfaz gr\u00e1fica.</li> </ul> <p>Aunque GitHub es la plataforma m\u00e1s popular, tambi\u00e9n existen otras alternativas como GitLab, Bitbucket o servidores privados.</p>"},{"location":"entrenamiento/p7/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<p>Al finalizar esta pr\u00e1ctica, deber\u00e1s ser capaz de:</p> <ul> <li>Clonar y configurar correctamente un repositorio remoto de GitHub.</li> <li>Crear y trabajar en ramas para desarrollar en paralelo.</li> <li>Subir y sincronizar tus cambios con el repositorio remoto.</li> <li>Resolver conflictos de integraci\u00f3n cuando existan.</li> </ul>"},{"location":"entrenamiento/p7/#limpieza-de-entornos-conda-no-utilizados","title":"\ud83e\uddf9 Limpieza de Entornos Conda No Utilizados","text":"<p>A lo largo de las pr\u00e1cticas anteriores, se han introducido y aplicado los conceptos fundamentales para desarrollar un proyecto de Machine Learning de forma profesional, incluyendo la creaci\u00f3n de entornos virtuales, la gesti\u00f3n de dependencias y la organizaci\u00f3n del c\u00f3digo. Estos entornos han sido clave para asegurar la reproducibilidad y el aislamiento de cada fase del desarrollo.</p> <p>Sin embargo, como resultado del trabajo pr\u00e1ctico, es probable que se hayan generado varios entornos Conda que ya no ser\u00e1n utilizados en las siguientes etapas del proyecto. Estos entornos contin\u00faan ocupando espacio en disco y consumiendo recursos del sistema, aunque no est\u00e9n en uso.</p> <p>Por este motivo, se recomienda eliminar los entornos que ya no se necesitan. Esta es una pr\u00e1ctica saludable para mantener tu sistema limpio, organizado y con espacio disponible, especialmente cuando trabajas con m\u00faltiples proyectos que requieren distintos entornos.</p> <p>Adem\u00e1s, la limpieza regular de entornos ayuda a evitar confusiones y errores derivados del uso accidental de entornos obsoletos.</p>"},{"location":"entrenamiento/p7/#tarea","title":"\ud83d\udee0\ufe0f Tarea","text":"<p>Responde las siguientes preguntas en la plataforma virtual:</p> <ul> <li>\u00bfC\u00f3mo listar los entornos existentes?</li> <li>\u00bfC\u00f3mo conocer todos los entornos creados y su ubicaci\u00f3n?</li> <li>\u00bfC\u00f3mo conocer el entorno activo?</li> <li>\u00bfC\u00f3mo eliminar un entorno que ya no usas?</li> </ul> <p>Usando esta informacion puedes eliminar los entornos:</p> <ul> <li>mi_entorno</li> <li>mi_entorno_copia</li> <li>mlops_env</li> </ul> <p>\ud83d\udd0d Consejo adicional</p> <p>Despu\u00e9s de eliminar entornos, tambi\u00e9n puedes liberar espacio limpiando paquetes en cach\u00e9 con:       <pre><code>conda clean --all\n</code></pre></p> <p>Este comando:</p> <ul> <li>Elimina paquetes descargados que ya no se usan.</li> <li>Borra archivos de instalaci\u00f3n temporales.</li> <li>Libera espacio ocupado por versiones antiguas.</li> </ul> <p>\u26a0\ufe0f Revisa lo que se va a eliminar antes de aceptar: te pedir\u00e1 confirmaci\u00f3n.</p>"},{"location":"entrenamiento/p7/#inicio-del-proyecto","title":"Inicio del Proyecto","text":"<p>Se asume que el responsable del proyecto ya ha iniciado la configuraci\u00f3n inicial, ejecutando tareas previas como la i) definici\u00f3n del entorno de paquetes y librer\u00edas, ii) la estructuraci\u00f3n del c\u00f3digo del proyecto (seg\u00fan lo trabajado en las pr\u00e1cticas Creaci\u00f3n del Entorno Virtual y Estructura del Proyecto) y iii) la implementaci\u00f3n de algunas etapas clave del proceso de ingenier\u00eda de datos, como la ingesta, limpieza y etiquetado de los datos necesarios para entrenar el modelo de predicci\u00f3n. Con esta base establecida, el proyecto ha sido compartido con los colaboradores para continuar con su implementaci\u00f3n de forma colaborativa.</p>"},{"location":"entrenamiento/p7/#escenario","title":"Escenario","text":"<p>En esta pr\u00e1ctica se simula una colaboraci\u00f3n entre dos personas trabajando sobre un mismo proyecto de Machine Learning, utilizando Git y GitHub como herramientas de control de versiones y coordinaci\u00f3n.</p>"},{"location":"entrenamiento/p7/#roles","title":"Roles","text":"<ul> <li> <p>Persona A ser\u00e1 responsable de desarrollar un script que, a partir de un archivo de datos previamente procesado (en el que ya se han eliminado los valores at\u00edpicos, los datos nulos y los faltantes), realice la divisi\u00f3n del conjunto de datos en entrenamiento y prueba.</p> </li> <li> <p>Persona B trabajar\u00e1 en paralelo desarrollando un script independiente que utilice los datos ya divididos para entrenar y validar un modelo de Machine Learning.</p> </li> </ul> <p>Cada persona trabajar\u00e1 en su propia rama de desarrollo, y al finalizar sus tareas, integrar\u00e1n sus aportes al proyecto principal mediante GitHub, siguiendo buenas pr\u00e1cticas de colaboraci\u00f3n y control de versiones.</p>"},{"location":"entrenamiento/p7/#requisitos-previos-para-cada-persona","title":"Requisitos previos: para cada persona","text":"<p>Antes de comenzar la pr\u00e1ctica, aseg\u00farate de cumplir con los siguientes requisitos. Si a\u00fan no los tienes listos, aqu\u00ed te explicamos c\u00f3mo cumplirlos:</p>"},{"location":"entrenamiento/p7/#1-tener-una-cuenta-de-github","title":"1. Tener una cuenta de GitHub","text":"<p>Pasos para crear una cuenta:</p> <ul> <li>Ingresa a https://github.com</li> <li>Haz clic en Sign up (Registrarse).</li> <li>Completa los datos solicitados:<ul> <li>Nombre de usuario</li> <li>Correo electr\u00f3nico</li> <li>Contrase\u00f1a segura</li> </ul> </li> <li>Sigue las instrucciones para verificar tu correo.</li> <li>Puedes elegir el plan gratuito (Free plan) para comenzar.</li> </ul> <p>Una vez creada tu cuenta, podr\u00e1s clonar repositorios, crear ramas, hacer cambios y colaborar con otras personas.</p>"},{"location":"entrenamiento/p7/#2-tener-git-instalado","title":"2. Tener Git instalado","text":"<p>Pasos para instalar Git:</p> <ul> <li> <p>Abre una terminal y ejecuta los siguientes comandos:</p> <pre><code>sudo apt update\nsudo apt install git\n</code></pre> </li> <li> <p>Verifica la instalaci\u00f3n con:</p> <pre><code>git \u2013-version\n</code></pre> </li> </ul>"},{"location":"entrenamiento/p7/#3-configurar-acceso-remoto-a-github-opcional","title":"3. Configurar acceso remoto a GitHub [opcional]","text":"<p>Aunque este requisito es opcional, se recomienda cumplirlo para facilitar el trabajo con Git. Su principal ventaja es que evita tener que ingresar tu usuario y contrase\u00f1a cada vez que realizas un <code>git push</code>. Para lograrlo, puedes configurar una conexi\u00f3n segura con GitHub utilizando una llave SSH.</p> <p>IMPORTANTE</p> <p>Si decides no usar el m\u00e9todo de conexi\u00f3n segura debes configurar Git para usar tu usuario de GitHub. De esta forma <code>git</code> sabr\u00e1 quien es la persona autor de los <code>git commit</code>. El resto de pasos no son necesarios.</p> <pre><code>git config --global user.name \"Tu Nombre Usuario\"\ngit config --global user.email \"tu-email@ejemplo.com\"\n</code></pre> <ul> <li> <p>Verificar si ya tienes una clave SSH. Abre tu terminal y ejecuta:       <pre><code>ls ~/.ssh\n</code></pre>       Si ves archivos como id_rsa y id_rsa.pub o id_ed25519, ya tienes una clave SSH. Si no, crea una.</p> </li> <li> <p>Generar una nueva llave SSH:</p> <p><pre><code>ssh-keygen -t rsa -b 4096 -C \"tucorreo@example.com\"\n</code></pre>   Ingrese el nombre del archivo para grabar la clave o presione ENTER para usar el nombre por omisi\u00f3n. Puede omitir el ingreso de una passphrase (o frase de paso).</p> </li> <li> <p>Mostrar la clave p\u00fablica:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> </li> <li> <p>Copiar la clave y agregarla en GitHub:</p> <ul> <li>Ir a Settings \u2192 SSH and GPG keys \u2192 New SSH Key.</li> <li>Pegar la clave en el campo correspondiente</li> </ul> </li> <li> <p>Configurar Git para usar tu usuario de GitHub</p> <pre><code>git config --global user.name \"Tu Nombre Usuario\"\ngit config --global user.email \"tu-email@ejemplo.com\"\n</code></pre> </li> <li> <p>Probar conexi\u00f3n</p> <p><pre><code>ssh -T git@github.com\n</code></pre> Si todo est\u00e1 bien, ver\u00e1s algo como:</p> <p>The authenticity of host '[ssh.github.com]:443 ([140.82.112.36]:443)' can't be established.   XXXXXXX key fingerprint is SHA256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.</p> <p>This key is not known by any other names.</p> <p>Are you sure you want to continue connecting (yes/no/[fingerprint])? yes</p> <p>Hi tu_usuario! You've successfully authenticated.</p> <p>En caso de no poder conectarse y obtener un mensaje como este :</p> <p>ssh: connect to host github por un error de timed out port 22: Connection timed out. </p> <ul> <li>Puedes configurar Git a usar otro puerto (Ej. 443) para SSH. </li> <li>Edita (o crea) el archivo ~/.ssh/config:</li> </ul> <pre><code>nano ~/.ssh/config\n</code></pre> <ul> <li>Agrega este contenido</li> </ul> <pre><code>Host github.com\nHostname ssh.github.com\nPort 443\nUser git\n</code></pre> <ul> <li>Guarda y prueba otra vez.</li> </ul> <pre><code>ssh -T git@github.com\n</code></pre> </li> </ul> <p>IMPORTANTE</p> <p>Por omisi\u00f3n, el acceso remoto en Git a plataformas como GitHub, GitLab o Bitbucket es mediante HTTPS. Esto ocurre porque:</p> <ul> <li>Es m\u00e1s sencillo de configurar inicialmente (no requiere claves SSH).</li> <li>Cualquier usuario con nombre de usuario y contrase\u00f1a (o token personal) puede clonar un repositorio.</li> </ul> <p>Sin embargo, SSH es la opci\u00f3n recomendada para mayor seguridad y comodidad, especialmente si trabajas frecuentemente con Git, ya que:</p> <ul> <li>Evita tener que introducir el usuario/contrase\u00f1a o token en cada operaci\u00f3n.</li> <li>Utiliza autenticaci\u00f3n basada en claves, que es m\u00e1s segura</li> </ul> <p>Para cambiar el acceso remoto de HTTPS a SSH, debes actualizar la URL del repositorio remoto una sola vez con el siguiente comando:</p> <pre><code>git remote set-url origin git@github.com:TU-USUARIO/Proyecto-MLOPS.git\n</code></pre> <p>Esto configura el repositorio para usar autenticaci\u00f3n SSH en lugar de HTTPS en futuras operaciones como <code>git push</code> o <code>git pull</code>.</p>"},{"location":"entrenamiento/p7/#configuracion-del-repositorio-de-equipo","title":"Configuraci\u00f3n del repositorio de equipo","text":"<p>Uno de los estudiantes (Persona A o Persona B) realizar\u00e1 un fork del repositorio base proporcionado por el docente (Responsable del Proyecto). Un fork en GitHub es una copia de un repositorio que se crea en tu propia cuenta. Te permite experimentar y trabajar en un proyecto sin afectar el original.</p>"},{"location":"entrenamiento/p7/#pasos-para-hacer-el-fork","title":"Pasos para hacer el fork:","text":"<ol> <li> <p>Ir al repositorio original (del docente) en GitHub https://github.com/jmem-ec/Proyecto-MLOPS-Base. </p> </li> <li> <p>Hacer clic en el bot\u00f3n \"Fork\" en la parte superior derecha.</p> </li> <li> <p>GitHub crear\u00e1 una copia del repositorio en la cuenta de la persona, por ejemplo: </p> <p><code>https://github.com/Persona_A/Proyecto-MLOPS</code></p> <p>Se recomienda cambiar el nombre del repositorio de Proyecto-MLOPS-Base a Proyecto-MLOPS, ya que este repositorio servir\u00e1 como base principal para el desarrollo del proyecto de MLOps a lo largo de todo el curso.</p> </li> </ol> <p>Esta persona se convertir\u00e1 en el \"due\u00f1o\" del fork, es decir, el responsable principal de esta copia del proyecto. A partir de aqu\u00ed:</p> <ul> <li> <p>Debe invitar a sus compa\u00f1eros como colaboradores del repositorio:</p> <ul> <li>Ir a Settings &gt; Collaborators</li> <li>Hacer clic en \"Add collaborator\"</li> <li>Ingresar los nombres de usuario de GitHub de los compa\u00f1eros para que puedan clonar, crear ramas, hacer cambios y subirlos</li> </ul> </li> </ul> <p>\ud83d\udcdd Nota explicativa: El objetivo de realizar un fork y no empezar desde cero es aprovechar una base inicial de c\u00f3digo ya estructurada, lo cual es muy com\u00fan en proyectos reales. Por ejemplo, un equipo puede comenzar a desarrollar un sistema de Machine Learning a partir de un repositorio de prueba, una plantilla anterior o incluso un proyecto experimental del responsable t\u00e9cnico (en este caso, el docente). A partir de ese punto de partida, los estudiantes comienzan a construir, adaptar y colaborar en su propia versi\u00f3n del proyecto.</p>"},{"location":"entrenamiento/p7/#clonar-el-repositorio-del-equipo","title":"Clonar el repositorio del equipo","text":"<p>Para comenzar a trabajar, cada estudiante \u2014incluido quien haya realizado el fork\u2014 debe clonar en su equipo local el repositorio que se gener\u00f3 a partir del fork. Este ser\u00e1 su repositorio de trabajo a lo largo del curso.</p> <pre><code>git clone https://github.com/Persona_A/Proyecto-MLOPS\ncd Proyecto-MLOPS\n</code></pre>"},{"location":"entrenamiento/p7/#crear-y-activar-el-entorno-conda","title":"Crear y activar el entorno <code>conda</code>","text":"<p>Cada estudiante debe crear y activar el entorno <code>conda</code> a partir del archivo <code>environment.yml</code></p>"},{"location":"entrenamiento/p7/#tarea_1","title":"\ud83d\udee0\ufe0f Tarea","text":"<ul> <li>\u00bfPuedes identificar los cambios realizados sobre la plantilla base de MLOps (utilizada en la pr\u00e1ctica Estructura del Proyecto) que han sido implementados por el responsable en este nuevo proyecto?</li> <li>Al revisar el c\u00f3digo implementado, \u00bfcu\u00e1l es el nombre del archivo que contiene los datos fuente utilizados para el entrenamiento del modelo?</li> <li>\u00bfEl entorno creado en que ruta est\u00e1 almacenado?</li> </ul>"},{"location":"entrenamiento/p7/#tareas-del-equipo-de-desarrollo","title":"Tareas del equipo de desarrollo","text":"<p>Una vez que cada persona del equipo ha clonado y activado el entorno del proyecto, debe ejecutar las siguientes tareas de implementaci\u00f3n de forma paralela. Cada integrante trabajar\u00e1 en su propia rama para luego integrar los cambios al repositorio principal.</p>"},{"location":"entrenamiento/p7/#persona-a-division-de-datos","title":"Persona A: Divisi\u00f3n de datos","text":"<ol> <li> <p>Crear y cambiar a una nueva rama:</p> <pre><code>git checkout -b division-datos\n</code></pre> </li> <li> <p>Agregar el archivo <code>stage4_splitting.py</code> en la carpeta  <code>src/data_eng/</code>.</p> <p>\ud83d\udee0\ufe0f Tarea</p> <p>Responda algunas preguntas en la plataforma virtual.</p> </li> <li> <p>Guardar los cambios y subirlos al repositorio remoto:</p> <pre><code>git add src/data_eng/stage4_splitting.py\ngit commit -m \"Agrega script para dividir datos en train/test\"\ngit push origin division-datos\n</code></pre> </li> </ol>"},{"location":"entrenamiento/p7/#persona-b-entrenamiento-del-modelo","title":"Persona B: Entrenamiento del Modelo","text":"<ol> <li> <p>Crear y cambiar a una nueva rama:</p> <pre><code>git checkout -b entrenamiento-modelo\n</code></pre> </li> <li> <p>Agregar el archivo <code>stage1_2_train_evaluate.py</code> en la carpeta  <code>src/model_eng/</code>.</p> <p>\ud83d\udee0\ufe0f Tarea</p> <p>Responda algunas preguntas en la plataforma virtual.</p> </li> <li> <p>Guardar los cambios y subirlos al repositorio remoto:</p> <pre><code>git add src/model_eng/stage1_2_train_evaluate.py\ngit commit -m \"Agrega script de entrenamiento y validaci\u00f3n del modelo\"\ngit push origin entrenamiento-modelo\n</code></pre> </li> </ol>"},{"location":"entrenamiento/p7/#persona-a-o-b-integrar-ramas-en-main","title":"Persona A \u00f3 B: Integrar ramas en <code>main</code>","text":"<p>Una vez que las tareas de cada persona est\u00e1n completas y no contiene errores, se debe integrar el trabajo en la rama principal (<code>main</code>). Esto lo puede hacer una persona responsable del proyecto o el equipo siguiendo estos pasos:</p> <pre><code>git checkout main\ngit pull origin main\ngit merge division-datos\ngit merge entrenamiento-modelo\ngit push origin main\n</code></pre>"},{"location":"entrenamiento/p7/#persona-a-o-b-prediccion-del-modelo","title":"Persona A \u00f3 B: Predicci\u00f3n del Modelo","text":"<ol> <li> <p>Crear y cambiar a una nueva rama:</p> <pre><code>git checkout -b prediccion-modelo\n</code></pre> </li> <li> <p>Agregar el archivo <code>stage3_prediction.py</code> en la carpeta  <code>src/model_eng/</code>.</p> <p>\ud83d\udee0\ufe0f Tarea</p> <p>Responda algunas preguntas en la plataforma virtual.</p> </li> <li> <p>Guardar los cambios y subirlos al repositorio remoto:</p> <pre><code>git add src/model_eng/stage3_prediction.py\ngit commit -m \"Agrega script de predicci\u00f3n del modelo\"\ngit push origin prediccion-modelo\n</code></pre> </li> <li> <p>Integrar el trabajo en la rama principal (<code>main</code>).</p> </li> </ol>"},{"location":"entrenamiento/p7/#actualizar-el-codigo-local-de-cada-persona","title":"Actualizar el c\u00f3digo local de cada persona","text":"<p>Una vez que las ramas han sido integradas en la rama principal del repositorio (main), cada integrante del equipo debe actualizar su copia local para trabajar con la versi\u00f3n m\u00e1s reciente del proyecto. A continuaci\u00f3n, se indican los pasos recomendados para hacerlo correctamente.</p> <ol> <li> <p>Cambiar a la rama <code>main</code> (si a\u00fan no est\u00e1n en ella)</p> <pre><code>git checkout main\n</code></pre> </li> <li> <p>Actualizar su rama <code>main</code> desde el repositorio remoto:</p> <pre><code>git pull origin main\n</code></pre> </li> </ol> <p>Esto descargar\u00e1 e integrar\u00e1 todos los cambios m\u00e1s recientes realizados por otros (como las integraciones de ramas que hiciste).</p>"},{"location":"entrenamiento/p8/","title":"Empaquetado y gesti\u00f3n de dependencias","text":"<p>En el desarrollo de proyectos de Machine Learning, no basta con que el c\u00f3digo funcione en el entorno local de un desarrollador. Para que un proyecto sea escalable, mantenible y reproducible \u2014objetivos clave dentro del ciclo de vida de MLOps \u2014 es fundamental contar con una buena estrategia de empaquetado y gesti\u00f3n de dependencias.</p> <p>El empaquetado convierte un proyecto en una unidad instalable, facilitando su distribuci\u00f3n, reutilizaci\u00f3n y ejecuci\u00f3n en distintos entornos (desarrollo, pruebas, producci\u00f3n). Por su parte, la gesti\u00f3n de dependencias garantiza que todas las librer\u00edas y herramientas necesarias est\u00e9n disponibles en versiones compatibles, evitando conflictos y fallos durante la ejecuci\u00f3n del pipeline.</p> <p>En este contexto, herramientas modernas como <code>pyproject.toml</code> o <code>conda</code> cumplen un rol clave al facilitar la instalaci\u00f3n ordenada del proyecto y sus componentes.</p>"},{"location":"entrenamiento/p8/#objetivos-de-la-practica","title":"\ud83c\udfaf Objetivos de la pr\u00e1ctica","text":"<p>Esta pr\u00e1ctica tiene como finalidad que los estudiantes comprendan y apliquen los principios fundamentales del empaquetado de proyectos y la gesti\u00f3n de dependencias, elementos clave para garantizar la reproducibilidad, portabilidad y mantenibilidad de soluciones de Machine Learning en entornos reales.</p> <p>Al finalizar esta pr\u00e1ctica, el estudiante ser\u00e1 capaz de:</p> <ul> <li>Diferenciar el rol del empaquetado frente al entorno de ejecuci\u00f3n (conda/env) en el flujo de trabajo MLOps.</li> <li>Detectar errores comunes como el t\u00edpico ModuleNotFoundError, y entender c\u00f3mo evitarlos mediante un empaquetado adecuado.</li> <li>Crear y configurar el archivo pyproject.toml para definir metadatos del proyecto y declarar sus dependencias.</li> <li>Instalar el proyecto como un paquete local y verificar que los m\u00f3dulos internos puedan ser importados correctamente.</li> </ul>"},{"location":"entrenamiento/p8/#trabajo-previo","title":"Trabajo Previo","text":"<p>Antes de abordar el empaquetado y la gesti\u00f3n de dependencias, se realizaron dos pasos fundamentales que sientan las bases para un flujo de trabajo robusto en MLOps:</p> <ol> <li> <p>Estructura del Proyecto con Cookiecutter. Se utiliz\u00f3 la herramienta cookiecutter para generar una estructura de proyecto estandarizada, que incluye:</p> <ul> <li>Separaci\u00f3n clara entre c\u00f3digo fuente (src/), datos (data/) y modelos (models/) .</li> <li>Archivos auxiliares como .gitignore, README.md, y carpetas para reportes (reports/).</li> </ul> <p>Esta estructura facilita la colaboraci\u00f3n, el versionado y la escalabilidad del proyecto.</p> </li> <li> <p>Creaci\u00f3n del Entorno de Ejecuci\u00f3n. Se defini\u00f3 y cre\u00f3 un entorno virtual con Conda, especificando:</p> <ul> <li>La versi\u00f3n de Python adecuada (por ejemplo, python=3.12).</li> <li>Dependencias base como pandas, scikit-learn, cookiecutter, entre otras.</li> <li>La inclusi\u00f3n del entorno en un archivo environment.yml para facilitar su recreaci\u00f3n.</li> </ul> <p>Este entorno garantiza que todos los participantes trabajen bajo las mismas condiciones de ejecuci\u00f3n, evitando errores relacionados con diferencias de versiones o paquetes.</p> </li> </ol>"},{"location":"entrenamiento/p8/#empaquetado-vs-entorno-entendiendo-sus-roles-en-mlops","title":"Empaquetado vs entorno: entendiendo sus roles en MLOps","text":"<p>El empaquetado y la configuraci\u00f3n del entorno son dos componentes fundamentales pero complementarios en un proyecto de Machine Learning dentro del ciclo de vida MLOps:</p> Aspecto Empaquetado (<code>pyproject.toml</code>) Entorno de Ejecuci\u00f3n (<code>conda</code>, <code>env</code>) \u00bfQu\u00e9 es? Descripci\u00f3n del proyecto y su instalaci\u00f3n como paquete de Python Definici\u00f3n de las librer\u00edas, versiones y dependencias para ejecutar el c\u00f3digo \u00bfQu\u00e9 resuelve? Facilita la distribuci\u00f3n e instalaci\u00f3n del c\u00f3digo como un m\u00f3dulo reutilizable. Garantiza reproducibilidad y compatibilidad del entorno. \u00bfQui\u00e9n lo usa? Otros desarrolladores o sistemas que consumen el proyecto. T\u00fa y tu equipo, durante desarrollo, entrenamiento o pruebas. Archivo principal <code>pyproject.toml</code> o antes <code>setup.py</code> + <code>setup.cfg</code> <code>environment.yml</code>, <code>requirements.txt</code>, o comandos <code>conda</code>"},{"location":"entrenamiento/p8/#error-comun-modulenotfound-al-ejecutar-un-script-del-proyecto","title":"\ud83d\udca5 Error com\u00fan: <code>ModuleNotFound</code> al ejecutar un script del proyecto","text":"<p>Para ilustrar un problema com\u00fan en proyectos mal configurados, ejecutaremos el script <code>src/data_eng/stage0_loading.py</code>, cuya funci\u00f3n es obtener datos desde una fuente externa.</p> <p>Este script importa otros m\u00f3dulos del mismo proyecto, como por ejemplo:</p> <pre><code>from app_logging import logging\n</code></pre> <p>Sin embargo, si intentamos ejecutarlo desde la terminal (o desde VS Code):</p> <pre><code>python src/data_eng/stage0_loading.py\n</code></pre> <p>Es muy probable que obtengamos un error como:</p> <pre><code>ModuleNotFoundError: No module named 'app_logging'\n</code></pre> <p>\u2753 \u00bfPor qu\u00e9 ocurre este error?</p> <p>Este error ocurre porque Python no reconoce autom\u00e1ticamente la ra\u00edz del proyecto ni las rutas relativas a otros m\u00f3dulos internos cuando se ejecuta un archivo de forma directa. Si el empaquetado del proyecto no est\u00e1 configurado correctamente (por ejemplo, sin un <code>pyproject.toml</code>, <code>setup.py</code>, o sin a\u00f1adir el proyecto al <code>PYTHONPATH</code>), Python no sabe c\u00f3mo resolver los imports relativos a la estructura del proyecto.</p> <p>Adem\u00e1s, en muchos casos, el desarrollador principal puede haber configurado su entorno de desarrollo (IDE) \u2014como PyCharm o VSCode\u2014 para que funcione solo en su m\u00e1quina, estableciendo manualmente el int\u00e9rprete o la carpeta ra\u00edz. Esto no garantiza que el proyecto funcione correctamente en otros entornos o m\u00e1quinas.</p>"},{"location":"entrenamiento/p8/#empaquetar-el-proyecto-como-un-modulo-python","title":"Empaquetar el proyecto como un m\u00f3dulo Python","text":"<p>Para garantizar que todos los m\u00f3dulos internos del proyecto se puedan importar correctamente \u2014sin depender de configuraciones espec\u00edficas del entorno o del IDE\u2014 es necesario empaquetar el proyecto como un m\u00f3dulo Python instalable. Esto se logra definiendo un archivo <code>pyproject.toml</code> (recomendado por los est\u00e1ndares modernos de Python) o, alternativamente, un <code>setup.py</code>, donde se describe la informaci\u00f3n del proyecto y sus dependencias.</p> <ol> <li> <p>Crear el archivo <code>pyproject.toml</code> en la ra\u00edz del proyecto:</p> <pre><code>[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"MLOPS-Project\"\nversion = \"0.1.0\"\nauthors = [{name = \"Tu Nombre\", email = \"tu@correo.com\"}]\ndescription = \"Proyecto de predicci\u00f3n de costos en cadena de suministro\"\nrequires-python = \"&gt;=3.12\"\ndynamic = [\"dependencies\"]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n[tool.setuptools.dynamic]\ndependencies = {file = [\"requirements.txt\"]}\n</code></pre> <p>Nota</p> <p>Para que <code>setuptools</code> encuentre los paquetes Python dentro de <code>src/</code>, debes indic\u00e1rselo expl\u00edcitamente con esta l\u00ednea en el archivo <code>pyproject.toml</code>.</p> <pre><code>[tool.setuptools.packages.find]\nwhere = [\"src\"]\n</code></pre> </li> <li> <p>Instala tu proyecto en modo editable. Usa el siguiente comando desde la ra\u00edz del proyecto: </p> <pre><code>pip install -e .\n</code></pre> <p>Esto permitir\u00e1 que cualquier script dentro del proyecto pueda importar otros m\u00f3dulos internos sin necesidad de hacks o configuraci\u00f3n especial en el IDE.</p> </li> <li> <p>Ahora seguramente podr\u00e1s ejecutar sin problemas el proyecto.</p> </li> </ol>"},{"location":"entrenamiento/p9/","title":"Buenas pr\u00e1cticas de codificaci\u00f3n","text":"<p>El desarrollo de sistemas de aprendizaje autom\u00e1tico (ML) ha evolucionado desde simples prototipos en notebooks hacia aplicaciones robustas que requieren principios de ingenier\u00eda de software. En este contexto, MLOps surge como un enfoque que combina las mejores pr\u00e1cticas del desarrollo de software y la operaci\u00f3n de modelos de ML, facilitando la reproducibilidad, escalabilidad y mantenibilidad de los sistemas de inteligencia artificial.</p> <p>Una parte fundamental de MLOps es la implementaci\u00f3n de buenas pr\u00e1cticas de codificaci\u00f3n, que no solo permiten mejorar la calidad del c\u00f3digo, sino tambi\u00e9n facilitar el trabajo colaborativo, la automatizaci\u00f3n de flujos de trabajo y la integraci\u00f3n con herramientas de versionado, testing y despliegue continuo. En esta pr\u00e1ctica se explorar\u00e1n tres pilares clave para lograr un desarrollo m\u00e1s profesional y organizado:</p> <ul> <li> <p>Uso de archivos de configuraci\u00f3n: Separar la l\u00f3gica del c\u00f3digo de los par\u00e1metros y configuraciones del sistema facilita la experimentaci\u00f3n, la reutilizaci\u00f3n del c\u00f3digo y la trazabilidad de los cambios.</p> </li> <li> <p>Interfaces de l\u00ednea de comandos: Incorporar interfaces CLI robustas y flexibles permite ejecutar scripts de forma controlada y program\u00e1tica, facilitando la integraci\u00f3n en pipelines automatizados y la ejecuci\u00f3n reproducible de experimentos.</p> </li> <li> <p>Buenas pr\u00e1cticas de documentaci\u00f3n y estilo: Aplicar principios como documentaci\u00f3n, seguimiento de estilo (PEP8, linters) y tipado permite construir c\u00f3digo m\u00e1s limpio, escalable y confiable, caracter\u00edsticas esenciales para proyectos de ciencia de datos que evolucionan hacia producci\u00f3n.</p> </li> </ul>"},{"location":"entrenamiento/p9/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<ul> <li> <p>Dise\u00f1ar e implementar estructuras de configuraci\u00f3n externas para separar los par\u00e1metros de ejecuci\u00f3n de la l\u00f3gica del c\u00f3digo.</p> </li> <li> <p>Explorar distintas formas de construir interfaces de l\u00ednea de comandos (CLI) para aplicaciones de ML.</p> </li> <li> <p>Aplicar buenas pr\u00e1cticas de codificaci\u00f3n en el desarrollo de componentes de un proyecto MLOps.</p> </li> </ul>"},{"location":"entrenamiento/p9/#uso-de-archivos-de-configuracion-con-hydra","title":"Uso de archivos de configuraci\u00f3n con Hydra","text":"<p>El c\u00f3digo actual del proyecto utiliza <code>argparse</code> para recibir ciertos par\u00e1metros necesarios para la ejecuci\u00f3n de los scripts; sin embargo, tambi\u00e9n incorpora otros par\u00e1metros directamente codificados en el cuerpo del programa. Por ejemplo, el script <code>stage0_loading.py</code>, utiliza <code>argparse</code> para recibir la URL donde est\u00e1 alojado  los datos externos, pero adem\u00e1s incluye valores codificados directamente en el c\u00f3digo fuente, como la ruta donde almacenar el archivo descargado. Este enfoque limita la flexibilidad y dificulta su uso en distintos entornos o etapas del pipeline.</p> <p>El uso de valores codificados (\"hardcoded\") como:</p> <pre><code>self.external_data = \"data/external\"\n</code></pre> <p>impide modificar rutas de entrada o salida sin alterar directamente el c\u00f3digo, lo cual va en contra de las buenas pr\u00e1cticas en proyectos MLOps.</p> <p>Por ello, se propone migrar a un enfoque basado en archivos de configuraci\u00f3n con Hydra, una herramienta moderna que permite manejar par\u00e1metros de manera flexible y desacoplada del c\u00f3digo, ideal para flujos complejos de entrenamiento, pruebas y despliegue de modelos.</p>"},{"location":"entrenamiento/p9/#propuesta-de-mejora","title":"Propuesta de mejora","text":"<p>Hydra permite manejar configuraciones con archivos <code>.yaml</code>, dejando el c\u00f3digo fuente limpio y desacoplado de decisiones contextuales como rutas de archivos, nombres, etc. A continuaci\u00f3n los pasos para migrar a Hydra:</p> <ol> <li> <p>Instalar Hydra</p> <p>Hasta la fecha actual, Hydra no est\u00e1 disponible directamente como paquete en los canales est\u00e1ndar de Conda (como <code>conda-forge</code> o <code>defaults</code>), por lo que la forma recomendada de instalar <code>hydra-core</code> es usando <code>pip</code>, incluso dentro de un entorno Conda:</p> <pre><code>pip install hydra-core\n</code></pre> </li> <li> <p>Crear un archivo de configuraci\u00f3n YAML</p> <p>Crea el archivo <code>configs/data_eng.yaml</code>, el cual contendr\u00e1 todos los par\u00e1metros necesarios para la fase Ingenier\u00eda de Datos (Data Enginnering) del flujo de trabajo de MLOPS. Por el momento, el archivo tendra el siguiente contenido:</p> <pre><code>data_source:\n    url: \"https://raw.githubusercontent.com/jmem-ec/KRRCourse/ccbd6ccf8389ba0988d53fc9300a64da00e6368b/Consignment_pricing.csv\"\n    external_data_dir: \"data/external\"\n    filename: \"Consignment_pricing.csv\"\n</code></pre> </li> <li> <p>Modificar el script para usar Hydra</p> <p>Se han realizado las siguientes modificaciones para integrar la configuraci\u00f3n con Hydra:</p> <ul> <li> <p>Incorporar en la seccion de importaci\u00f3n</p> <pre><code>import hydra\nfrom omegaconf import DictConfig\n</code></pre> </li> <li> <p>Reemplaza el uso de <code>argparse</code> por hydra, y usa los valores del archivo <code>.yaml</code></p> <p><pre><code>@hydra.main(config_path=f\"{os.getcwd()}/configs\", config_name=\"data_eng\", version_base=None)\ndef main(cfg: DictConfig):\n    logging.basicConfig(level=logging.INFO)\n    data = GetData().get_data(cfg)\n\nif _name_ == \"_main_\":\n    main()\n</code></pre> Toma en cuenta que no puedes llamar directamente a la funci\u00f3n decorada con <code>@hydra.main(...)</code> como <code>main()</code> desde <code>if __name__ == \"__main__\":</code>, porque Hydra necesita controlar el punto de entrada del script para realizar su redirecci\u00f3n de rutas y configuraci\u00f3n del entorno de ejecuci\u00f3n.</p> </li> <li> <p>Modifica la cabecera de la funcion <code>get_data</code> </p> <pre><code>def get_data(self, config)\n</code></pre> </li> <li> <p>Cambia los valores codificados (\"hardcoded\") usando los valores del archivo <code>.yaml</code> <pre><code>...\ngithub_csv_url = config.data_source.url\n\nself.external_data = config.data_source.external_data_dir\n\n...\nlocal_path = os.path.join(self.external_data, config.data_source.filename)\n</code></pre></p> </li> </ul> </li> </ol>"},{"location":"entrenamiento/p9/#tarea","title":"\ud83d\udee0\ufe0f Tarea","text":"<ul> <li> <p>Por defecto, Hydra escribir\u00e1 los resultados en una carpeta de resultados, con una subcarpeta para el d\u00eda en que se ejecut\u00f3 el experimento y, adem\u00e1s, la hora en que se inici\u00f3. Inspeccione su ejecuci\u00f3n revisando cada archivo que Hydra ha generado y compruebe que la informaci\u00f3n ha sido registrada.</p> </li> <li> <p>Hydra tambi\u00e9n permite cambiar y a\u00f1adir par\u00e1metros din\u00e1micamente sobre la marcha desde la l\u00ednea de comandos:</p> <ul> <li> <p>Pruebe a cambiar un par\u00e1metro desde la l\u00ednea de comandos. <pre><code>python src/data_eng/stage0_loading.py data_source.filename=Dataset1.csv\n</code></pre></p> </li> <li> <p>Pruebe a a\u00f1adir un par\u00e1metro desde la l\u00ednea de comandos. <pre><code>python src/data_eng/stage0_loading.py +data_source.additional=50\n</code></pre></p> </li> </ul> </li> <li> <p>Modifique el script <code>stage1_ingestion.py</code> para que obtenga los par\u00e1metros necesarios desde el archivo de configuraci\u00f3n <code>data_eng.yaml</code>, en lugar de tenerlos codificados directamente en el script. En particular, aseg\u00farate de que el archivo <code>data_eng.yaml</code> incluya la siguiente secci\u00f3n:</p> <pre><code>raw_data:\n    raw_data_dir: \"data/raw\"\n    raw_filename: \"Dataset.csv\"\n</code></pre> <p>Responde la pregunta en la plataforma virtual.</p> </li> <li> <p>Para mantener una configuraci\u00f3n coherente y flexible en todo el proyecto, incorpora el uso de Hydra en todos los scripts que a\u00fan no lo utilicen. Actualiza el contenido de los siguientes archivos para que usen Hydra como gestor de configuraci\u00f3n:</p> <p>\ud83d\udcc4 Archivos a modificar:</p> <ul> <li>src/data_eng/stage2_ingestion.py</li> <li>src/data_eng/stage3_labeling.py</li> <li>src/data_eng/stage4_splitting.py</li> <li>src/model_eng/stage1_2_train_evaluate.py</li> <li>src/model_eng/stage3_prediction.py</li> <li>configs/data_eng.yaml</li> <li>configs/model_eng.yaml</li> </ul> <p>Verifica que todo este funcionando normalmente.</p> </li> </ul>"},{"location":"entrenamiento/p9/#interfaces-de-linea-de-comandos","title":"Interfaces de l\u00ednea de comandos","text":"<p>Incorporar interfaces de l\u00ednea de comandos robustas y flexibles permite ejecutar scripts de forma controlada y program\u00e1tica, facilitando su integraci\u00f3n en pipelines automatizados (por ejemplo, para entrenamiento o evaluaci\u00f3n de modelos) y garantizando la reproducibilidad de los experimentos.</p> <p>En esta secci\u00f3n aprender\u00e1s a crear scripts ejecutables como comandos dentro de tu proyecto. Esto permite:</p> <ul> <li> <p>Ejecutar scripts sin invocar expl\u00edcitamente <code>python script.py</code>.</p> </li> <li> <p>Documentar y estandarizar la forma en que se ejecutan tareas clave del proyecto.</p> </li> <li> <p>Integrar f\u00e1cilmente con herramientas como DVC, MLFlow, o cualquier otro sistema de automatizaci\u00f3n o CI/CD.</p> </li> </ul> <p>Aunque existen muchas formas de construir una CLI en Python (como <code>argparse</code>, <code>click</code> o <code>typer</code>), aqu\u00ed comenzaremos por una soluci\u00f3n nativa usando <code>pyproject.toml</code>.</p>"},{"location":"entrenamiento/p9/#definir-una-tarea-cli-para-limpiar-los-datos","title":"Definir una tarea CLI para limpiar los datos","text":"<p>El proyecto incluye el archivo <code>stage2_cleaning.py</code>, encargado de realizar tareas de limpieza de datos, como la imputaci\u00f3n de valores faltantes, el tratamiento de valores nulos, la transformaci\u00f3n de columnas y otras operaciones necesarias para preparar los datos antes del entrenamiento del modelo.</p> <p><pre><code>src/\n\u251c\u2500\u2500 data_eng/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 stage2_cleaning.py\n\u2502   \u251c\u2500\u2500 ...\npyproject.toml\n</code></pre> La tarea consiste en ejecutar la limpieza de los datos simplemente escribiendo:</p> <pre><code>clean\n</code></pre> <p>en lugar de: <pre><code>python stage2_cleaning.py\n</code></pre></p> <p>Esto se consigue mediante:</p> <ol> <li>Asegurar que <code>stage2_cleaning.py</code> contenga una funci\u00f3n <code>main()</code> como punto de entrada.</li> <li> <p>Configurar el archivo <code>pyproject.toml</code> para registrar un comando CLI personalizado. Espec\u00edficamente, al a\u00f1adir:</p> <pre><code>[project.scripts]\nclean = \"data_eng.stage2_cleaning:main\"\n</code></pre> </li> <li> <p>Instalar el proyecto en modo editable:</p> <pre><code>pip install -e .\n</code></pre> </li> </ol> <p>Ahora se puede ejecutar en la terminal, el comando:</p> <pre><code>clean\n</code></pre>"},{"location":"entrenamiento/p9/#tarea_1","title":"\ud83d\udee0\ufe0f Tarea","text":"<p>Agrega otros comandos CLI para diferentes etapas del flujo de trabajo MLOps. Por ejemplo:</p> <pre><code>ingestion = \"data_eng.stage1_ingestion:main\"\nfeature_engineering  = \"data_eng.stage3_labeling:main\"\nsplitting  = \"data_eng.stage4_splitting:main\"\ntrain&amp;evaluate = \n</code></pre> <p>\u00bfQu\u00e9 comando CLI usaste para la tarea train&amp;evaluate?. Responde la pregunta en la plataforma virtual.</p>"},{"location":"entrenamiento/p9/#ejecutar-codigo-no-python-desde-cli","title":"Ejecutar c\u00f3digo no-Python desde CLI","text":"<p>Hasta ahora, hemos aprendido a construir interfaces de l\u00ednea de comandos (CLI) para scripts en Python, facilitando su ejecuci\u00f3n directa desde el terminal. Sin embargo, en proyectos de Machine Learning reales, no todo el c\u00f3digo es Python. A menudo necesitamos ejecutar herramientas de l\u00ednea de comandos como <code>git</code>, <code>conda</code>, <code>dvc</code> o incluso <code>docker</code>, y estas invocaciones pueden volverse complejas, largas y dif\u00edciles de recordar.</p> <p>Por ejemplo, ejecutar una aplicaci\u00f3n con Docker que use GPU puede implicar un comando tan largo como este:</p> <pre><code>docker run -v $(pwd):/app -w /app --gpus all --rm -it my_image:latest python my_script.py --arg1 val1 --arg2 val2\n</code></pre> <p>Recordar y mantener estos comandos puede ser engorroso. Una alternativa mucho m\u00e1s limpia y mantenible es definir comandos m\u00e1s amigables como:</p> <pre><code>invoke run_my_experiment --arg1=val1 --arg2=val2\n</code></pre> <p>Para lograr esto, vamos a utilizar el paquete <code>invoke</code>, una herramienta que permite definir tareas reutilizables desde Python para ser ejecutadas como comandos CLI. <code>invoke</code> act\u00faa como una versi\u00f3n moderna de <code>Makefile</code>, y su integraci\u00f3n como paquete de Python facilita su uso multiplataforma.</p>"},{"location":"entrenamiento/p9/#automatizacion-de-la-fase-de-ingenieria-de-datos","title":"Automatizaci\u00f3n de la fase de Ingenier\u00eda de Datos","text":"<p>Para automatizar esta etapa del flujo de trabajo en MLOps, es necesario ejecutar las siguientes tareas de forma secuencial:</p> <ol> <li> <p>Instala el paquete <code>invoke</code></p> </li> <li> <p>Crea un archivo llamado <code>tasks.py</code> en la ra\u00edz del proyecto</p> <pre><code>src/\n\u251c\u2500\u2500 data_eng/\n\u2502   \u251c\u2500\u2500 stage1_ingestion.py\n\u2502   \u251c\u2500\u2500 stage2_cleaning.py\n\u2502   \u251c\u2500\u2500 stage3_labeling.py\n\u2502   \u2514\u2500\u2500 stage4_splitting.py\ntasks.py\n</code></pre> </li> <li> <p>Agrega el siguiente contenido al archivo <code>tasks.py</code>. Se asume que cada script contiene una funci\u00f3n <code>main()</code> como punto de entrada.</p> <pre><code>from invoke import task\n\n@task\ndef ingest(ctx):\n    ctx.run(\"python data_eng/stage1_ingestion.py\")\n\n@task\ndef clean(ctx):\n    ctx.run(\"python data_eng/stage2_cleaning.py\")\n\n@task\ndef features(ctx):\n    ctx.run(\"python data_eng/stage3_labeling.py\")\n\n@task\ndef split(ctx):\n    ctx.run(\"python data_eng/stage4_splitting.py\")\n\n@task\ndef data_eng_pipeline(ctx):\n    ingest(ctx)\n    clean(ctx)\n    features(ctx)\n    split(ctx)\n</code></pre> <p>El primer argumento de cualquier funci\u00f3n decorada con <code>@task</code> es <code>ctx</code>, un contexto que permite ejecutar comandos como si estuvi\u00e9ramos en la terminal, usando su m\u00e9todo <code>run</code>.</p> <p>\ud83d\udcdd Nota: Si no est\u00e1s seguro de las tareas disponibles en <code>invoke</code>, puedes usar el comando:</p> <pre><code>invoke --list\n</code></pre> </li> <li> <p>Desde la terminal, puedes ahora ejecutar el pipeline data engineering con un solo comando:</p> <pre><code>invoke data_eng_pipeline\n</code></pre> </li> </ol>"},{"location":"entrenamiento/p9/#tarea_2","title":"\ud83d\udee0\ufe0f Tarea","text":"<ul> <li> <p>En lugar de invocar directamente los scripts usando Python desde las tareas definidas en <code>tasks-py</code>, te recomendamos utilizar los comandos CLI personalizados que hayas configurado en el archivo <code>pyproject.toml</code>. Esto permite una ejecuci\u00f3n m\u00e1s clara, consistente y alineada con las buenas pr\u00e1cticas de automatizaci\u00f3n.</p> </li> <li> <p>Crea una tarea que simplifique el proceso de subir cambios del proyecto local al repositorio remoto. Esta tarea debe ejecutar de forma secuencial los siguientes comandos de Git:</p> <ul> <li><code>git add .</code></li> <li><code>git commit -m \"Mi mensaje de commit\"</code></li> <li><code>git push origin main</code></li> </ul> <p>El objetivo es que puedas ejecutar todo este flujo con un solo comando desde la terminal usando <code>invoke</code>, por ejemplo:</p> <pre><code>invoke git --mensaje \u00abMi mensaje de commit\u00bb\n</code></pre> <p>Responde la pregunta en la plataforma virtual.</p> </li> </ul>"},{"location":"entrenamiento/p9/#buenas-practicas-de-documentacion-y-estilo","title":"Buenas pr\u00e1cticas de documentaci\u00f3n y estilo","text":"<p>Mantener scripts bien documentados es una parte clave en las buenas pr\u00e1cticas de codificaci\u00f3n dentro del flujo MLOps. Los docstrings permiten describir el prop\u00f3sito, los par\u00e1metros y los valores de retorno de funciones y clases. Esto mejora la comprensi\u00f3n del c\u00f3digo y facilita su integraci\u00f3n en herramientas automatizadas.</p> <p>GitHub Copilot puede ayudarte a generar estos docstrings de manera r\u00e1pida y consistente. Te animamos a que pruebes estas funcionalidades posteriormente.</p>"},{"location":"negocio/p4/","title":"Alineaci\u00f3n Estrat\u00e9gica del Proyecto","text":"<p>En el contexto de las operaciones de aprendizaje autom\u00e1tico (MLOps), es fundamental planificar y estructurar los proyectos de forma clara antes de implementar soluciones t\u00e9cnicas. ML Canvas es una herramienta visual que permite modelar, comunicar y alinear los distintos componentes de un proyecto de machine learning, promoviendo una visi\u00f3n compartida entre equipos multidisciplinarios.</p> <p>Esta herramienta facilita la identificaci\u00f3n de objetivos de negocio, fuentes de datos, m\u00e9tricas de \u00e9xito, riesgos y requerimientos t\u00e9cnicos, integrando todos estos elementos dentro del flujo de trabajo de MLOps. Su uso temprano en el desarrollo de un proyecto mejora la comunicaci\u00f3n, reduce ambig\u00fcedades y gu\u00eda la toma de decisiones t\u00e9cnicas con un enfoque sistem\u00e1tico.</p>"},{"location":"negocio/p4/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Esta pr\u00e1ctica tiene como objetivo introducir el uso de ML Canvas como herramienta de planificaci\u00f3n en proyectos de MLOps. Al finalizar, los participantes ser\u00e1n capaces de:</p> <ul> <li> <p>Identificar y documentar los componentes clave de un proyecto de machine learning de forma estructurada.</p> </li> <li> <p>Relacionar los objetivos del negocio con las decisiones t\u00e9cnicas dentro del flujo de trabajo de MLOps.</p> </li> </ul>"},{"location":"negocio/p4/#contexto-del-proyecto","title":"Contexto del Proyecto","text":"<p>El proyecto Predicci\u00f3n de Precios en Art\u00edculos de Consignaci\u00f3n se enfoca en desarrollar un modelo de aprendizaje autom\u00e1tico (ML) que permita predecir con precisi\u00f3n el precio de venta de productos consignados, a partir de diversas caracter\u00edsticas.</p> <p>La consignaci\u00f3n es un modelo de negocio en el cual una tienda vende productos en nombre de terceros, reteniendo una comisi\u00f3n sobre el precio final de venta. Este proyecto busca asistir tanto a los due\u00f1os de tiendas de consignaci\u00f3n como a los vendedores en la determinaci\u00f3n de precios \u00f3ptimos, lo cual puede traducirse en una mayor tasa de ventas y rentabilidad.</p> <p>Para lograr este objetivo, el proyecto se desarrolla dentro de un flujo de trabajo MLOps, lo que implica considerar desde el an\u00e1lisis de datos y el entrenamiento del modelo, hasta su despliegue, monitoreo y mantenimiento continuo. En este contexto, el uso del AI &amp; ML Canvas resulta esencial para planificar y comunicar de forma clara todos los aspectos clave del proyecto.</p>"},{"location":"negocio/p4/#actividad-practica","title":"\ud83e\uddea Actividad Pr\u00e1ctica","text":"<p>Usa esta informaci\u00f3n para completar cada bloque del AI &amp; ML Canvas, asegur\u00e1ndote de:</p> <ul> <li>Redactar cada secci\u00f3n con claridad.</li> <li>Indicar supuestos si falta informaci\u00f3n (se pueden validar despu\u00e9s).</li> <li>Discutir el resultado con tu equipo para afinar los elementos del proyecto.</li> <li>Crea una subcarpeta llamada <code>Practica-4</code> dentro del directorio principal de pr\u00e1cticas y utiliza ese espacio para almacenar las evidencias del trabajo desarrollado en equipo- Suba esta evidencia a la plataforma virtual tambi\u00e9n.</li> </ul>"},{"location":"versionado_datos/p10/","title":"Almacenamiento remoto con DVC","text":"<p>En proyectos de ciencia de datos o machine learning, los conjuntos de datos y modelos pueden ser muy grandes y dif\u00edciles de gestionar dentro de Git. Para resolver esto, DVC (Data Version Control) ofrece una forma eficiente de versionar y compartir archivos grandes sin almacenarlos directamente en el repositorio.</p> <p>DVC permite configurar un almacenamiento remoto (Remote Storage) para sincronizar autom\u00e1ticamente los archivos grandes con servicios como Google Cloud Storage, Google Drive, S3, entre otros. As\u00ed, se facilita la colaboraci\u00f3n en equipo y la reproducci\u00f3n de experimentos.</p> <p>\ud83d\udcddDVC act\u00faa como un complemento de Git: mientras Git versiona el c\u00f3digo, DVC se encarga de los datos y modelos.</p>"},{"location":"versionado_datos/p10/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<p>Aprender a gestionar datos de forma eficiente y colaborativa en proyectos de ciencia de datos mediante la herramienta DVC (Data Version Control). Al finalizar esta pr\u00e1ctica, podr\u00e1s:</p> <ul> <li>Instalar y configurar DVC en tu entorno local.</li> <li>Entender c\u00f3mo DVC complementa a Git para versionar y rastrear archivos grandes.</li> <li>Configurar un almacenamiento remoto para compartir datos con tu equipo.</li> </ul>"},{"location":"versionado_datos/p10/#requisitos-previos","title":"Requisitos Previos","text":"<ul> <li>Tener instalado git.</li> <li>Contar con una cuenta de Google.</li> </ul>"},{"location":"versionado_datos/p10/#instalacion-de-dvc","title":"Instalaci\u00f3n de DVC","text":"<p>\ud83d\udca1 Se recomienda realizar esta instalaci\u00f3n dentro de un entorno Conda previamente creado para el proyecto.</p> <ol> <li> <p>Verifica que tienes tu entorno Conda activo</p> </li> <li> <p>Para instalar DVC, la forma m\u00e1s directa es usando <code>pip</code>, ya que DVC se encuentra activamente mantenido en PyPI. Se recomienda para esta practica instalar DVC con soporte para almacenamiento en la nube. </p> <p>Para usar Google Drive:     <pre><code>pip install 'dvc[gdrive]'\n</code></pre> Y para usar Google Cloud Storage     <pre><code>pip install 'dvc[gcs]'\n</code></pre></p> <p>\ud83d\udca1 Se puede usar el comando <code>pip install 'dvc[all]'</code> para instalar soporte para todos los  principales backends de almacenamiento remoto, incluyendo:</p> <ul> <li>Google Drive (gdrive)</li> <li>Google Cloud Storage (gcs)</li> <li>Amazon S3 (s3)</li> <li>Azure Blob Storage (azure)</li> <li>SSH, HDFS, WebDAV, entre otros.</li> </ul> <p>Porsupuesto, la instalaci\u00f3n con <code>[all]</code> puede tardar m\u00e1s y ocupar m\u00e1s espacio, ya que instala muchas dependencias adicionales. Tambi\u00e9n puede usar <code>pip install \"dvc[gdrive, gcs]\"</code></p> </li> <li> <p>Verificar instalaci\u00f3n     <pre><code>dvc --version\n</code></pre></p> </li> </ol>"},{"location":"versionado_datos/p10/#parte-1-configurar-google-drive-como-almacenamiento-remoto-en-dvc","title":"Parte 1: Configurar Google Drive como almacenamiento remoto en DVC","text":"<p>Google Drive puede usarse como un almacenamiento remoto para proyectos gestionados con DVC. Esta opci\u00f3n es \u00fatil para proyectos colaborativos que usan almacenamiento en la nube gratuito y accesible. A continuaci\u00f3n se detallan los pasos para conectar el proyecto a Google Drive.</p>"},{"location":"versionado_datos/p10/#obtener-credenciales-para-conectar-dvc-con-google-drive","title":"\ud83d\udd10 Obtener credenciales para conectar DVC con Google Drive","text":"<p>En la gu\u00eda oficial de DVC para la configuraci\u00f3n de almacenamiento remoto con Google Drive se describen varias formas de conexi\u00f3n. Sin embargo, para este proyecto se recomienda configurar una cuenta de servicio para conectar el almacenamiento remoto con tu proyecto DVC.</p> <p>Esta opci\u00f3n ofrece varias ventajas, especialmente en escenarios donde el acceso a los datos debe realizarse de forma autom\u00e1tica, como en entornos de c\u00f3mputo en la nube, procesos de integraci\u00f3n y entrega continua (CI/CD), entre otros. A diferencia de la autenticaci\u00f3n interactiva con OAuth, el uso de una cuenta de servicio permite el acceso sin intervenci\u00f3n manual del usuario.</p> <ol> <li> <p>Accede a Google Cloud Console</p> <ul> <li>Ve a: https://console.cloud.google.com</li> </ul> </li> <li> <p>Crea un proyecto (si a\u00fan no lo tienes)</p> <ul> <li>Men\u00fa \u2192 IAM &amp; Admin \u2192 Manage resources \u2192 Create a project.    </li> </ul> </li> <li> <p>Activa la API de Google Drive</p> <ul> <li>Men\u00fa \u2192 APIs &amp; Services \u2192 Library \u2192 Busca Google Drive API \u2192 Haz clic en Enable.</li> </ul> </li> <li> <p>Crea una cuenta de servicio</p> <ul> <li> <p>Men\u00fa \u2192 IAM &amp; Admin \u2192 Service Accounts \u2192 Haz clic en Create Service Account.</p> </li> <li> <p>Nombre de la cuenta (ejemplo): dvc-service</p> </li> <li> <p>ID de la cuenta: se autogenera</p> </li> <li> <p>En los siguientes pasos, no asignes roles (puedes omitirlos).</p> </li> <li> <p>Haz clic en Done.</p> </li> </ul> </li> <li> <p>Crea una clave para la cuenta de servicio</p> <ul> <li> <p>En la lista de cuentas de servicio, haz clic en los tres puntos sobre la que creaste.</p> </li> <li> <p>Ve a la pesta\u00f1a Keys \u2192 Add Key \u2192 Create new key \u2192 Elige JSON \u2192 Descargar el archivo .json.</p> </li> </ul> </li> <li> <p>Almacena de forma segura este archivo <code>.json</code>, pues contiene las credenciales privadas que DVC usar\u00e1 para autenticarse.</p> <ul> <li>En el proyecto puedes guardarlo en la carpeta <code>configs/credentials/</code>.</li> </ul> </li> </ol> <p>\ud83d\udcdd Este archivo de credenciales ser\u00e1 utilizado por DVC como mecanismo de autenticaci\u00f3n al conectarse con Google Cloud.</p> <p>Recuerda que no es una buena idea compartir este archivo por razones de seguridad:</p> <ul> <li>Contiene una clave privada que permite autenticarse como la cuenta de servicio.</li> <li>Si alguien obtiene el archivo, puede acceder a los recursos de Google Cloud o Drive como si fuera esa cuenta.</li> </ul> <p>Una opci\u00f3n recomendable es :</p> <ol> <li>Agregar el archivo <code>.json</code> a tu <code>.gitignore</code></li> <li>Cada miembro del equipo descarga el archivo por separado desde una ubicaci\u00f3n segura.</li> <li>Usar una variable de entorno para apuntar a la ruta del archivo</li> </ol> <p>!! M\u00e1s adelante podr\u00e1s pensar en otras opciones de seguridad!!.</p>"},{"location":"versionado_datos/p10/#crear-y-conectar-una-carpeta-en-google-drive","title":"Crear y conectar una carpeta en Google Drive","text":"<ol> <li>Accede a Google Drive.</li> <li>Crea una nueva carpeta y as\u00edgnale un nombre \u00fanico (por ejemplo, dvc-mlops-storage).</li> <li>Haz clic derecho sobre la carpeta y selecciona \"Compartir\".</li> <li>Comparte esa carpeta con el correo electr\u00f3nico de la cuenta de servicio (XXXX@XXXX.iam.gserviceaccount.com) como Editor</li> <li> <p>Copia el identificador de la carpeta desde la URL. </p> <p>Abre la carpeta en tu navegador y miara la URL. El identificador es lo que va luego de <code>folders/</code>:</p> <p>https://drive.google.com/drive/u/2/folders/1mOn9UmUYMxqsvEYmk9svT8zf5n9YEhr5</p> </li> </ol>"},{"location":"versionado_datos/p10/#parte-2-configurar-google-cloud-storage-como-almacenamiento-remoto-en-dvc","title":"Parte 2: Configurar Google Cloud Storage como almacenamiento remoto en DVC","text":"<p>IMPORTANTE</p> <p>Nota: La cuenta organizacional de la Universidad de Cuenca requiere permisos  para crear nuevos proyectos y adem\u00e1s no cuenta con cr\u00e9ditos habilitados para crear un bucket en Google Cloud Storage. Por otro lado, si se utiliza una cuenta individual (por ejemplo, de Gmail), es necesario asociar una tarjeta de cr\u00e9dito v\u00e1lida para activar los servicios de Google Cloud.</p> <p>Por este motivo, la creaci\u00f3n del almacenamiento remoto usando Google Cloud Storage se presenta \u00fanicamente con fines informativos y no es un requisito obligatorio para completar esta pr\u00e1ctica.</p> <p>Google Cloud Storage (GCS) puede usarse como un almacenamiento remoto para proyectos gestionados con DVC. Esta opci\u00f3n es ideal para proyectos colaborativos que requieren mayor capacidad, control de acceso y velocidad de transferencia, especialmente en entornos de producci\u00f3n o investigaci\u00f3n. A continuaci\u00f3n se detallan los pasos para conectar tu proyecto a Google Cloud Storage.</p>"},{"location":"versionado_datos/p10/#crear-un-bucket-en-google-cloud-storage","title":"Crear un Bucket en Google Cloud Storage","text":"<ol> <li>Accede a Google Cloud Console<ul> <li>Ve a Google Cloud Console.</li> <li>Aseg\u00farate de haber iniciado sesi\u00f3n con tu cuenta de Google.</li> </ul> </li> <li>Crea un nuevo proyecto (si a\u00fan no lo tienes)<ul> <li>En la barra superior, haz clic en el selector de proyecto (a la izquierda del buscador). </li> <li>Haz clic en \"Nuevo proyecto\".</li> <li>Asigna un nombre representativo (por ejemplo, Proyecto-MLOPS).</li> <li>Opcionalmente, selecciona una organizaci\u00f3n o carpeta.</li> <li>Haz clic en \"Crear\" y espera unos segundos.</li> </ul> </li> <li>Activa el nuevo proyecto    <ul> <li>Aseg\u00farate de que el proyecto reci\u00e9n creado est\u00e9 seleccionado (aparecer\u00e1 en la parte superior de la consola).</li> </ul> </li> <li>Ve a Cloud Storage<ul> <li>En el men\u00fa lateral izquierdo (\u00edcono de tres l\u00edneas), navega a: Cloud Storage &gt; Buckets</li> <li>Tambi\u00e9n puedes acceder directamente desde:   https://console.cloud.google.com/storage/browser.</li> </ul> </li> <li>Crea un nuevo bucket<ul> <li>Haz clic en el bot\u00f3n azul \"Create\".</li> </ul> </li> <li>Configura el bucket<ul> <li>Nombre del bucket: Debe ser \u00fanico globalmente (por ejemplo, mlops-practica-2025).</li> <li>Ubicaci\u00f3n:<ul> <li>Selecciona la regi\u00f3n <code>us-east1</code>.</li> </ul> </li> </ul> </li> <li>Opciones adicionales (puedes dejar por defecto)<ul> <li>Clase de almacenamiento: Standard</li> <li>Control de acceso: Uniform</li> <li>Encriptaci\u00f3n: Google-managed encryption key</li> <li>Haz clic en \"Continue\" hasta completar la creaci\u00f3n del bucket.</li> </ul> </li> <li>Crear<ul> <li>Haz clic en \"Crear\" y espera a que se genere el bucket.</li> </ul> </li> </ol>"},{"location":"versionado_datos/p10/#obtener-credenciales-para-conectar-dvc-con-gcp","title":"\ud83d\udd10 Obtener credenciales para conectar DVC con GCP","text":"<p>Una vez creado el bucket, es necesario obtener las credenciales para conectar el almacenamiento remoto de GCP con tu proyecto DVC:</p> <ol> <li>Ve a IAM &amp; Admin y selecciona Service Accounts en la barra lateral izquierda.</li> <li>Haz clic en el bot\u00f3n \"Create Service Account\" para crear una nueva cuenta de servicio que usar\u00e1s para conectar con el proyecto DVC.</li> <li>Asigna un nombre e ID a esta cuenta (por ejemplo, <code>lab2</code>) y deja la configuraci\u00f3n predeterminada.</li> <li>Haz clic en \"Create and Continue\".</li> <li>En la secci\u00f3n de permisos, selecciona Owner en el men\u00fa desplegable y haz clic en \"Continue\".</li> <li>A\u00f1ade tu usuario para que tenga acceso a esta cuenta de servicio y haz clic en \"Done\".</li> </ol>"},{"location":"versionado_datos/p10/#descargar-credenciales","title":"\ud83d\udd11 Descargar credenciales","text":"<ol> <li>Ser\u00e1s redirigido a la p\u00e1gina de Service Accounts.</li> <li>Busca tu cuenta de servicio reci\u00e9n creada, haz clic en \"Actions\" (\u00edcono de tres puntos), y selecciona \"Manage keys\".</li> <li>Haz clic en el bot\u00f3n \"Add Key\".</li> <li>Selecciona la opci\u00f3n de generar una nueva clave en formato JSON.</li> <li>Descarga y almacena de forma segura este archivo <code>.json</code>.</li> </ol> <p>\ud83d\udcdd Este archivo de credenciales ser\u00e1 utilizado por DVC como mecanismo de autenticaci\u00f3n al conectarse con Google Cloud.</p>"},{"location":"versionado_datos/p10/#conectar-dvc-a-google-drive-o-google-cloud-storage","title":"Conectar DVC a Google Drive o Google Cloud Storage","text":"<p>En esta pr\u00e1ctica damos las instrucciones para conectar DVC a dos almacenamientos remotos. DVC permite definir m\u00faltiples remotos y seleccionar cu\u00e1l usar como predeterminado o indicar uno espec\u00edfico al momento de hacer <code>dvc push</code>, <code>dvc pull</code>, o <code>dvc fetch</code>.</p> <p>Antes de configurar los accesos remotos debes iniciar DVC usando: <pre><code>dvc init\n</code></pre></p> <p>Este comando crea la estructura b\u00e1sica del proyecto DVC, incluyendo:</p> <ul> <li>El directorio .dvc/</li> <li>El archivo .dvc/config donde se guardan las configuraciones del almacenamiento remoto.</li> </ul> <p>Es similar a <code>git init</code>, pero para el control de versiones de datos. Sin esa estructura, los comandos <code>dvc remote add</code> o <code>dvc remote modify</code> no tienen d\u00f3nde escribir la configuraci\u00f3n, y fallar\u00e1n o no har\u00e1n nada \u00fatil.</p> <p>Para configurar dos accesos remotos, podemos usar: <pre><code># Agregar Google Drive como almacenamiento remoto\ndvc remote add gdrive-remote gdrive://&lt;ID-de-carpeta&gt;\ndvc remote modify gdrive-remote gdrive_use_service_account true\ndvc remote modify gdrive-remote gdrive_service_account_json_file_path /ruta/a/credenciales.json\n\n# Agregar Google Cloud Storage como almacenamiento remoto\ndvc remote add gcs-remote gs://mi-bucket-dvc\ndvc remote modify gcs-remote credentialpath /ruta/a/credenciales.json\n</code></pre></p> <p>Nota: Aqu\u00ed no se establece ninguno como predeterminado a\u00fan.</p>"},{"location":"versionado_datos/p10/#elegir-almacenamiento-remoto-al-hacer-pushpull","title":"Elegir almacenamiento remoto al hacer <code>push/pull</code>","text":"<p>Puedes subir tus datos a un almacenamiento remoto espec\u00edfico sin necesidad de cambiar el predeterminado. Por ahora estos comandos son solo informativos.</p> <pre><code>dvc push -r gdrive-remote     # Subir a Google Drive\ndvc push -r gcs-remote        # Subir a GCS\n</code></pre>"},{"location":"versionado_datos/p10/#o-definir-un-almacenamiento-remoto-por-defecto","title":"\u2b50 O definir un almacenamiento remoto por defecto","text":"<p>Se recomienda establecer alguno de los repositorios por defecto. Incluso si solo has configurado uno.</p> <p><pre><code>dvc remote default gdrive-remote\n</code></pre> Luego, cada vez que hagas <code>dvc push</code> o <code>dvc pull</code>, se usar\u00e1 ese sin tener que especificarlo.</p> <p>Recomendaci\u00f3n</p> <p>Si trabajas con equipos o deseas redundancia en almacenamiento, tener m\u00faltiples remotos puede ayudarte a:</p> <ul> <li>Compartir con quienes no tienen acceso a ciertas plataformas.</li> <li>Hacer backups autom\u00e1ticos a diferentes nubes.</li> <li>Evaluar velocidad y costos de acceso.</li> </ul>"},{"location":"versionado_datos/p11/","title":"Control de versiones de datos","text":"<p>En los proyectos de ciencia de datos, los archivos de datos suelen cambiar durante el desarrollo: nuevas versiones, transformaciones, limpieza, etc. Git no es adecuado para archivos grandes, por lo que DVC permite versionar datasets de forma eficiente, integr\u00e1ndose con Git y almacenando los datos reales en un repositorio remoto (como Google Drive o Google Cloud Storage). En esta pr\u00e1ctica, aprender\u00e1s a rastrear y recuperar versiones previas de tus datos.</p>"},{"location":"versionado_datos/p11/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a versionar datos en un proyecto de Machine Learning utilizando DVC (Data Version Control). Esta pr\u00e1ctica incluye registrar archivos de datos, hacer seguimiento a los cambios y restaurar versiones anteriores, incluso si los archivos han sido eliminados localmente. </p>"},{"location":"versionado_datos/p11/#supuestos-iniciales","title":"\ud83d\udcc1 Supuestos iniciales","text":"<ul> <li>Tienes configurado un entorno Conda con DVC instalado.</li> <li>El repositorio ya tiene configurado el acceso remoto a Google Drive y/o Google Cloud Storage.</li> <li>El archivo que contiene los datos para entrenamiento del modelo (<code>data/raw/Dataset.csv</code>), ya existe en el repositorio local y ha sido agregado al proyecto.</li> </ul>"},{"location":"versionado_datos/p11/#flujo-de-trabajo-tipico-para-el-versionado-de-datos-con-dvc","title":"Flujo de trabajo t\u00edpico para el versionado de datos con DVC","text":"<p>Una vez configurado el almacenamiento remoto (Google Drive, por ejemplo), puedes comenzar a versionar sus datos de manera sencilla y eficiente.</p>"},{"location":"versionado_datos/p11/#1-inicializar-dvc-en-el-proyecto-si-aun-no-esta-hecho","title":"1. Inicializar DVC en el proyecto (si a\u00fan no est\u00e1 hecho)","text":"<pre><code>dvc init\n</code></pre> <p>Recuerda, este comando crea la carpeta <code>.dvc/</code> y archivos de configuraci\u00f3n necesarios para que DVC funcione. </p>"},{"location":"versionado_datos/p11/#2-configurar-git-para-registrar-los-archivos-dvc","title":"2. Configurar Git para registrar los archivos DVC","text":"<p>Antes de aplicar el comando para agregar archivos con Git, es muy recomendable revisar el contenido del archivo <code>.gitignore</code>. Este archivo le indica a Git qu\u00e9 archivos o carpetas deben ser ignorados y, por tanto, no ser\u00e1n versionados ni subidos al repositorio.</p> <p>\u00bfQu\u00e9 buscar en <code>.gitignore</code>?</p> <p>Aqu\u00ed algunos ejemplos comunes que s\u00ed deben estar:</p> <pre><code># Ignorar datos reales\n/data/\n!data/\n!data/**/*.dvc   # Excepto los archivos de control de DVC\n\n# Ignorar credenciales\nservice-account.json\n\n# Ignorar salidas de modelos\n/models/*\n!models/\n!models/**/*.dvc\n\n# Ignorar salidas de reportes\nreports/*\n!reports/\n!reports/**/*.dvc\n\n# Otros\n__pycache__/\n*.log\n</code></pre> <p>Una vez verificado el contenido del archivo <code>.gitignore</code> y confirmado que los archivos necesarios no est\u00e1n siendo excluidos por error, puedes proceder a ejecutar el siguiente comando:</p> <pre><code>git add .dvc .gitignore\ngit commit -m \"Inicializa DVC en el proyecto\"\n</code></pre> <p>Esto agrega al control de versiones la configuraci\u00f3n de DVC y las reglas definidas para ignorar archivos, asegurando una gesti\u00f3n adecuada del proyecto desde el inicio.</p> <ul> <li>git add .dvc .gitignore: Este comando le indica a Git que agregue dos cosas al \u00e1rea de preparaci\u00f3n (staging):<ul> <li><code>.dvc/</code> contiene la configuraci\u00f3n de DVC </li> <li><code>.gitignore</code> es el archivo que le dice a Git qu\u00e9 archivos/directorios debe ignorar (por ejemplo, archivos de datos reales).</li> </ul> </li> <li>git commit -m \"Inicializa DVC en el proyecto\": Guarda estos cambios en el historial de Git.</li> </ul>"},{"location":"versionado_datos/p11/#2-agregar-archivo-de-datos-para-seguimiento","title":"2. Agregar archivo de datos para seguimiento","text":"<p>En este proyecto, el archivo de datos para entrenar el modelo se genera autom\u00e1ticamente al ejecutar el script <code>data_eng/stage1_ingestion.py</code>, el cual copia la fuente de datos a la carpeta <code>data/raw/</code> con el nombre <code>Dataset.csv</code>, de acuerdo con la configuraci\u00f3n establecida en el archivo <code>configs/data_eng.yaml</code>.</p> <p>Para incluir este archivo bajo el control de versiones de datos del proyecto, el primer paso es registrarlo en el seguimiento local de DVC y almacenarlo en la cach\u00e9 con el siguiente comando:</p> <pre><code>dvc add data/raw/Dataset.csv\ngit add data/raw/Dataset.csv.dvc .gitignore\ngit commit -m \"Agrega archivo Dataset.csv al seguimiento de DVC\"\n</code></pre> <ul> <li> <p>dvc add data/raw/dataset.csv: Este comando funciona de manera similar a <code>git add</code>. Tu conjunto de datos ahora est\u00e1 bajo control local de DVC y almacenado en su cach\u00e9 (por defecto local, aunque se puede configurar para ser compartido).</p> <p>Al ejecutar este comando, DVC genera dos archivos en la carpeta donde est\u00e1 tu dato:</p> <ul> <li>Dataset.csv.dvc: Este archivo contiene la referencia a la ubicaci\u00f3n real de tu archivo y cambia cada vez que el archivo de datos se modifica.</li> <li>.gitignore: Este archivo evita que Git suba el archivo de datos real al repositorio, evitando que el repositorio se llene de archivos pesados. DVC lo crea autom\u00e1ticamente.</li> </ul> <p>Estos archivos <code>.dvc</code> son archivos YAML que guardan informaci\u00f3n clave sobre tu archivo de datos, incluyendo un hash MD5 que se recalcula cada vez que el archivo cambia. Esto permite a tu equipo seguir los cambios en los datos de forma segura y confiable.</p> </li> <li> <p>git add data/raw/Dataset.csv.dvc .gitignore: Este comando agrega al control de versiones el archivo <code>.dvc</code> que rastrea el dataset con DVC y el archivo <code>.gitignore</code> actualizado para evitar que Git rastree directamente el archivo de datos pesado. Esto permite mantener el repositorio ligero mientras se versionan los datos de forma eficiente con DVC.</p> </li> </ul>"},{"location":"versionado_datos/p11/#3-sincronizar-datos-con-el-almacenamiento-remoto","title":"3. Sincronizar datos con el almacenamiento remoto","text":"<p>Para subir los datos versionados a tu almacenamiento remoto (Google Drive), usa:</p> <pre><code>dvc push data/raw/Dataset.csv\ngit commit -m \"data push\"\n</code></pre> <p>Esto enviar\u00e1 el archivo al almacenamiento remoto configurado. Luego, realiza un commit en Git para registrar este cambio:</p> <p>\u26a0\ufe0f Nota: Las siguientes opciones se presentan \u00fanicamente con fines informativos. No las ejecutes directamente en tu proyecto, pues no tienen relevancia en este momento. Una referencia completa de los comandos DVC puede ser encontrado aqu\u00ed.</p>"},{"location":"versionado_datos/p11/#recuperar-datos","title":"Recuperar datos","text":"<p>Si en alg\u00fan momento eliminas el archivo original de datos, puedes recuperarlo f\u00e1cilmente ejecutando:</p> <pre><code>dvc checkout data/raw/Dataset.csv.dvc\n</code></pre> <p>Esto restaurar\u00e1 el archivo de datos tal como estaba bajo la versi\u00f3n controlada.</p>"},{"location":"versionado_datos/p11/#descargar-datos","title":"Descargar datos","text":"<p>Cuando cambies a otra rama o quieras obtener los datos relacionados con otro experimento, simplemente usa:</p> <pre><code>git checkout nombre_de_rama\ndvc pull\n</code></pre> <p>Con esto, descargar\u00e1s desde el almacenamiento remoto la versi\u00f3n correcta de los datos para la rama en la que est\u00e9s trabajando.</p>"},{"location":"versionado_datos/p11/#tarea-eliminar-accidentalmente-el-archivo-local","title":"\ud83d\udee0\ufe0f Tarea: Eliminar accidentalmente el archivo local","text":"<p>En proyectos que usan DVC, los archivos de datos grandes no se almacenan directamente en Git, sino que se gestionan por separado a trav\u00e9s del almacenamiento de DVC (local o remoto). Esto significa que si un archivo de datos como <code>data/raw/Dataset.csv</code> se elimina accidentalmente del sistema de archivos, no todo est\u00e1 perdido.</p> <p>A continuaci\u00f3n, simulamos esta situaci\u00f3n eliminando manualmente el archivo local:</p> <ul> <li> <p>Elimina el archivo de datos local:</p> <pre><code>rm data/raw/Dataset.csv\n</code></pre> </li> <li> <p>Verifica que el archivo ya no est\u00e1:</p> <pre><code>ls data/raw/\n</code></pre> </li> <li> <p>Recuperar los datos desde el almacenamiento remoto</p> <pre><code>dvc pull\n</code></pre> </li> </ul> <p>\u2705 Esto restaura <code>data/raw/Dataset.csv</code> desde el remoto. Ideal para trabajar en diferentes m\u00e1quinas o recuperar archivos perdidos. Responda la pregunta en la plataforma virtual.</p>"},{"location":"versionado_datos/p11/#extra-ver-historial-de-versiones","title":"\ud83d\udccc Extra: Ver historial de versiones","text":"<p>Puedes volver a una versi\u00f3n anterior del archivo de datos siguiendo estos pasos:</p> <p><pre><code>git checkout &lt;hash_version_anterior&gt; data/raw/Dataset.csv.dvc\ndvc checkout\n</code></pre> Esto restaurar\u00e1 el estado del archivo a la versi\u00f3n vinculada con ese commit espec\u00edfico.</p> <p>Importante</p> <p>Esta metodolog\u00eda asegura que el proyecto completo, incluyendo los datos, se mantenga sincronizado entre todos los colaboradores sin necesidad de subir archivos grandes directamente a GitHub, evitando problemas de espacio y rendimiento.</p>"},{"location":"versionado_datos/p12/","title":"Trabajando con Pipelines","text":"<p>Despu\u00e9s de configurar el almacenamiento remoto y configurar el inicio de un proyecto DVC para versionar datos y modelos, el siguiente paso es automatizar y estructurar flujos de trabajo mediante la construcci\u00f3n de un pipeline de experimentos. Un pipeline permite encadenar etapas del proceso (por ejemplo, preprocesamiento, entrenamiento y evaluaci\u00f3n) de forma reproducible y trazable, facilitando el trabajo colaborativo y la experimentaci\u00f3n sistem\u00e1tica.</p> <p>Esta pr\u00e1ctica est\u00e1 basada en el conjunto de datos previamente versionado con DVC (<code>data/raw/Dataset.csv</code>). Este archivo es la fuente de datos para entrenar el modelo de predicci\u00f3n. Recordemos que el proyecto implementado tiene tres etapas principales:</p> <ul> <li>Preprocesamiento de datos (limpieza, extracci\u00f3n de caracter\u00edsticas, y divisi\u00f3n de datos)</li> <li>Entrenamiento del modelo</li> <li>Evaluaci\u00f3n del modelo</li> </ul>"},{"location":"versionado_datos/p12/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<ul> <li>Construir un pipeline de ML utilizando DVC.</li> <li>Declarar las dependencias y salidas de cada etapa del flujo de trabajo.</li> <li>Ejecutar y rastrear autom\u00e1ticamente los cambios en los datos o en el c\u00f3digo que afecten el resultado del experimento.</li> <li>Comprender c\u00f3mo DVC administra los archivos intermedios y el modelo final en cada etapa.</li> </ul>"},{"location":"versionado_datos/p12/#requisitos","title":"\ud83d\udee0\ufe0f Requisitos","text":"<p>Debes contar con los siguientes scripts organizados en tu proyecto:</p> <ul> <li> <p>stage2_cleaning.py: Realiza la limpieza y preprocesamiento de los datos brutos, eliminando valores inconsistentes o faltantes y estandarizando formatos para su posterior an\u00e1lisis.</p> </li> <li> <p>stage3_labeling.py: Extrae caracter\u00edsticas relevantes desde la fuente de datos y asigna etiquetas necesarias para el entrenamiento supervisado del modelo.</p> </li> <li> <p>stage4_splitting.py: Divide el conjunto de datos procesado en subconjuntos de entrenamiento y prueba, asegurando una distribuci\u00f3n adecuada para la validaci\u00f3n del modelo.</p> </li> <li> <p>stage1_2_train_evaluate.py: Entrena el modelo utilizando el conjunto de entrenamiento y eval\u00faa su desempe\u00f1o sobre el conjunto de prueba, generando m\u00e9tricas de rendimiento clave.</p> </li> </ul> <p>Advertencia:</p> <p>Los pipelines propuestos a continuaci\u00f3n est\u00e1n basados en la estructura del repositorio (creado en la pr\u00e1ctica Estructura del Proyecto y especializado en las pr\u00e1cticas Versionado del C\u00f3digo, Empaquetado y gesti\u00f3n de dependencias y Buenas pr\u00e1cticas de codificaci\u00f3n). Sin embargo, es posible crear otros scripts y crear pipelines espec\u00edficos para las necesidades de su proyecto.</p>"},{"location":"versionado_datos/p12/#creacion-de-pipelines","title":"Creaci\u00f3n de pipelines","text":"<p>DVC construye un pipeline basado en tres componentes: Entradas (Inputs), Salidas (Outputs) y Comando (Command). Por ejemplo, para la etapa de limpieza de datos, estos ser\u00edan los componentes:</p> <ul> <li>Entradas: archivo <code>data/raw/Dataset.csv</code> y el script <code>stage2_cleaning.py</code></li> <li>Salidas: archivo <code>data/interim/Cleaned_Dataset.csv</code></li> <li>Comando: <code>python3 stage2_cleaning.py</code></li> </ul> <p>Para crear esta etapa, utilizamos el comando <code>dvc stage add</code> de la siguiente forma:</p> <pre><code>dvc stage add -n limpieza \\\n  -d src/data_eng/stage2_cleaning.py -d data/raw/Dataset.csv \\\n  -o data/interim/Cleaned_Dataset.csv \\\n  python3 src/data_eng/stage2_cleaning.py\n</code></pre> <p>Aqu\u00ed nombramos esta etapa como \"limpieza\" usando la opci\u00f3n <code>-n</code>. Tambi\u00e9n definimos las entradas con la opci\u00f3n <code>-d</code> y las salidas con la opci\u00f3n <code>-o</code>. El comando que se ejecuta siempre va al final del comando <code>dvc stage add</code>, sin ninguna opci\u00f3n.</p> <p>Consejo</p> <p>Los archivos de salida se agregan al control de DVC cuando reproduces una etapa de DVC. Cuando finalices tu experimento, recuerda usar dvc push para versionar no solo los datos usados sino tambi\u00e9n los resultados generados durante el experimento.</p> <p>En este punto, es posible que hayas notado que se creo un nuevo archivo: <code>dvc.yaml</code>. Este archivo es responsable de guardar lo que se describi\u00f3 en cada comando <code>dvc stage add</code>. Por lo tanto, si deseas crear o modificar una etapa espec\u00edfica, es posible editar directamente el archivo <code>dvc.yaml</code>. El archivo actual se ver\u00eda as\u00ed:</p> <pre><code>stages:\n  limpieza:\n    cmd: python3 src/data_eng/stage2_cleaning.py\n    deps:\n    - src/data_eng/stage2_cleaning.py\n    - data/raw/Dataset.csv\n    outs:\n    - data/interim/Cleaned_Dataset.csv\n</code></pre> <p>El pipeline actual se ve as\u00ed:</p> <p></p>"},{"location":"versionado_datos/p12/#probar-la-primera-etapa","title":"Probar la primera etapa","text":"<p>Una vez que se ha creado una etapa del pipeline usando <code>dvc stage add</code>, ya es posible probar la ejecuci\u00f3n del pipeline utilizando el comando:</p> <p><pre><code>dvd repro\n</code></pre> Por ahora, el pipeline cuenta \u00fanicamente con una etapa llamada <code>limpieza</code>, la cual se encarga del preprocesamiento inicial de los datos. Sin embargo, m\u00e1s adelante se integrar\u00e1n otras fases como extracci\u00f3n de caracter\u00edsticas, division de datos, entrenamiento y evaluaci\u00f3n del modelo.</p>"},{"location":"versionado_datos/p12/#que-hace-dvc-repro","title":"\u00bfQu\u00e9 hace dvc repro?","text":"<ul> <li>Ejecuta las etapas necesarias del pipeline en el orden definido por sus dependencias.</li> <li>Reproduce etapas autom\u00e1ticamente solo si detecta que alguna dependencia ha cambiado (por ejemplo, si se modific\u00f3 un archivo de entrada o un par\u00e1metro).</li> <li>Encadena etapas si hay varias definidas y conectadas entre s\u00ed.</li> </ul> <p>Cada vez que se ejecuta <code>dvc repro</code>, DVC genera o actualiza el archivo <code>dvc.lock</code>. Este tambi\u00e9n es un archivo YAML y su funci\u00f3n es similar a los archivos <code>.dvc</code>. En su interior, podemos encontrar la ruta y un c\u00f3digo hash para cada archivo de cada etapa, lo que permite a DVC hacer seguimiento de los cambios. Este seguimiento es importante porque ahora DVC puede saber cu\u00e1ndo una etapa debe ejecutarse de nuevo o no, bas\u00e1ndose en si sus dependencias cambiaron.</p> <p>\ud83d\udee0\ufe0f Tarea</p> <p>La primera vez que ejecutaste <code>dvc repro</code>, se ejecut\u00f3 por completo la etapa de limpieza, generando los archivos de salida correspondientes.</p> <ul> <li>\u00bfQu\u00e9 ocurre si vuelves a ejecutar <code>dvc repro</code> sin realizar ning\u00fan cambio en los datos, par\u00e1metros o c\u00f3digo?. Responde la pregunta en la plataforma virtual.</li> </ul>"},{"location":"versionado_datos/p12/#actualizacion-de-las-demas-etapas-del-pipeline","title":"Actualizaci\u00f3n de las dem\u00e1s etapas del pipeline","text":"<p>A partir de este punto, puedes definir y actualizar las siguientes etapas del pipeline directamente en el archivo <code>dvc.yaml</code>. Cada tarea o paso del flujo de trabajo se especifica como una etapa (stage) dentro de dicho archivo, siguiendo la estructura del pipeline. Por ejemplo:</p> <pre><code>stages:\n  limpieza:\n    cmd: python3 src/data_eng/stage2_cleaning.py\n    deps:\n    - data/raw/Dataset.csv\n    - src/data_eng/stage2_cleaning.py\n    outs:\n    - data/interim/Cleaned_Dataset.csv\n\n  extraccion_caracteristicas:\n    cmd: python3 src/data_eng/stage3_labeling.py\n    deps:\n    - data/interim/Cleaned_Dataset.csv\n    - src/data_eng/stage3_labeling.py\n    outs:\n    - data/processed/Processed_Dataset.csv\n\n  division:\n    cmd: python3 src/data_eng/stage4_splitting.py\n    deps:\n    - data/processed/Processed_Dataset.csv\n    - src/data_eng/stage4_splitting.py\n    outs:\n    - data/processed/Train_Dataset.csv\n    - data/processed/Test_Dataset.csv\n\n  entrenamiento_evaluacion:\n    cmd: python3 src/model_eng/stage1_2_train_evaluate.py\n    deps:\n    - data/processed/Test_Dataset.csv\n    - data/processed/Train_Dataset.csv\n    - src/model_eng/stage1_2_train_evaluate.py\n    params:\n      - configs/model_eng.yaml:\n        - RandomizedSearchCV.scoring\n        - RandomizedSearchCV.n_iter\n    outs:\n    - models/model_rf.pkl\n    metrics:\n    - reports/scores.json\n</code></pre> <p>El pipeline que representa el proyecto quedar\u00eda as\u00ed:</p> <p></p> <p>Entre las etapas definidas en el archivo <code>dvc.yaml</code>, a continuaci\u00f3n se describe en detalle la etapa entrenamiento_evaluacion:</p> <p>Esta etapa forma parte del pipeline de DVC y tiene como objetivo entrenar y evaluar el modelo de aprendizaje autom\u00e1tico utilizando conjuntos de datos previamente procesados:</p> <ul> <li><code>entrenamiento_evaluacion</code>: es el indentificador de la etapa.</li> <li> <p><code>cmd: python3 src/model_eng/stage1_2_train_evaluate.py</code>: Este es el script que se ejecuta cuando se corre esta etapa. En este caso, se trata de un script en Python que:</p> <ul> <li>Entrena el modelo y eval\u00faa su desempe\u00f1o.</li> <li>Guarda el modelo entrenado y las m\u00e9tricas.</li> </ul> </li> <li> <p><code>deps</code>:</p> <ul> <li><code>data/processed/Test_Dataset.csv</code></li> <li><code>data/processed/Train_Dataset.csv</code></li> <li><code>src/model_eng/stage1_2_train_evaluate.py</code></li> </ul> <p>Estas son las entradas que DVC monitorea para saber si el stage debe volver a ejecutarse. Si alguno de estos archivos cambia, DVC vuelve a ejecutar el stage. Aqu\u00ed se incluyen:</p> <ul> <li>Los datos de entrenamiento y prueba procesados.</li> <li>El script de entrenamiento/evaluaci\u00f3n.</li> </ul> </li> <li> <p><code>params:</code></p> <ul> <li><code>configs/model_eng.yaml:</code></li> <li><code>RandomizedSearchCV.scoring</code></li> <li><code>RandomizedSearchCV.n_iter</code></li> </ul> <p>Estos son par\u00e1metros que vienen de un archivo YAML externo (<code>configs/model_eng.yaml</code>) y que el script usa. Si cambias estos valores, DVC detectar\u00e1 el cambio y volver\u00e1 a ejecutar el stage.</p> </li> <li> <p><code>outs:</code></p> <ul> <li><code>models/model_rf.pkl</code></li> </ul> <p>La salida es el modelo entrenado que se guarda como resultado. DVC lo versiona autom\u00e1ticamente.</p> </li> <li> <p><code>metrics:</code></p> <ul> <li><code>reports/scores.json</code></li> </ul> <p>En DVC las m\u00e9tricas se pueden mostrar, comparar entre versiones y usarlas en visualizaciones. Para el caso del proyecto las m\u00e9tricas a incluir est\u00e1n en el archivo JSON <code>reports/scores.json</code>, el cual contiene m\u00e9tricas del modelo como: rmse, r2_score, mse, train_score, etc. </p> </li> </ul> <p>\ud83d\udee0\ufe0f Tarea</p> <ul> <li> <p>Una vez definido todo el pipeline del proyecto vuelve a ejecuar el comando <code>dvc repro</code>, para comprobar que todo funciona correctamente. </p> <p>\ud83d\udccc Observa cuidadosamente qu\u00e9 etapas no se ejecutan: si DVC detecta que sus dependencias y salidas no han cambiado, las considerar\u00e1 actualizadas y omitir\u00e1 su ejecuci\u00f3n. </p> <p>Este comportamiento confirma que la l\u00f3gica de seguimiento de cambios del pipeline est\u00e1 funcionando como se espera. Responda las preguntas en la plataforma virtual.</p> <p>Reproducibilidad garantizada con <code>dvc repro</code></p> <p>Cualquier persona que acceda a tu proyecto (por ejemplo, clon\u00e1ndolo desde Git y teniendo acceso al almacenamiento remoto de DVC) puede ejecutar:</p> <pre><code>dvc repro\n</code></pre> <p>y as\u00ed reconstruir autom\u00e1ticamente todos los resultados del pipeline, de forma reproducible y en el mismo orden en que fueron generados originalmente.</p> <p>Esto es posible porque los archivos <code>dvc.yaml</code> y <code>dvc.lock</code> contienen toda la informaci\u00f3n necesaria sobre:</p> <ul> <li>Las etapas del pipeline.</li> <li>Los comandos utilizados.</li> <li>Los archivos de entrada y salida.</li> <li>Los par\u00e1metros utilizados.</li> </ul> <p>\ud83d\udca1 Esto asegura que los experimentos sean totalmente trazables, repetibles y colaborativos. para reconstruir resultados reproducibles.</p> </li> <li> <p>Modifica la secci\u00f3n <code>params</code>: dentro del archivo <code>dvc.yaml</code>, reemplazando el par\u00e1metro: <code>RandomizedSearchCV.scoring</code> por <code>RandomizedSearchCV.cv</code>, suponiendo que por un error se escogi\u00f3 el incorrecto. Mientras que <code>scoring</code> se refiere a la m\u00e9trica utilizada para evaluar los modelos, el par\u00e1metro que realmente se quer\u00eda controlar era <code>cv</code>, el cual define el n\u00famero de particiones utilizadas en la validaci\u00f3n cruzada (cross-validation).</p> <p>\u00bfQu\u00e9 etapas se volvieron a ejecutar?. Responde las preguntas en la plataforma virtual.</p> </li> <li> <p>Muestra las m\u00e9tricas del estado m\u00e1s reciente de tu proyecto reproducido con DVC. Para ello puedes usar el comando:</p> <pre><code>dvc metrics show\n</code></pre> <p>Este comando muestra las m\u00e9tricas actuales guardadas en los archivos definidos en <code>metrics:</code> dentro de <code>dvc.yaml</code>. Responde la pregunta en la plataforma virtual.</p> </li> </ul>"},{"location":"versionado_datos/p12/#ejecutar-experimentos","title":"Ejecutar experimentos","text":"<p>Una caracter\u00edstica fundamental de DVC es que permite ejecutar experimentos reproducibles sobre tus pipelines de Machine Learning. Los experimentos en DVC son variantes del pipeline que pueden modificar par\u00e1metros, datos o c\u00f3digo, sin alterar tu rama principal de Git. Esto es ideal para comparar modelos y evaluar distintas configuraciones de entrenamiento.</p> <p>Los experimentos:</p> <ul> <li>Se ejecutan con <code>dvc exp run</code>.</li> <li>Pueden modificar par\u00e1metros f\u00e1cilmente usando <code>--set-param (-S)</code>.</li> <li>Se almacenan como commits temporales y se pueden comparar con m\u00e9tricas y gr\u00e1ficos.</li> <li>Se pueden promover a Git si se desea conservar.</li> </ul> <p>A continuaci\u00f3n se propone un par de experimentos con diferentes valores de par\u00e1metros:</p> <ul> <li> <p>Paso 1: Ejecutar el primer experimento   <pre><code>dvc exp run \\\n      --name exp1_cv3_n10 \\\n      -S configs/model_eng.yaml:RandomizedSearchCV.cv=3 \\\n      -S configs/model_eng.yaml:RandomizedSearchCV.n_iter=10\n</code></pre>   Este experimento con nombre <code>exp1_cv3_n10</code> se prueba con los par\u00e1metros <code>cv=3</code> y <code>n_iter=10</code>.</p> </li> <li> <p>Paso 2: Ejecutar el segundo experimento   <pre><code>dvc exp run \\\n      --name exp2_cv5_n30 \\\n      -S configs/model_eng.yaml:RandomizedSearchCV.cv=5 \\\n      -S configs/model_eng.yaml:RandomizedSearchCV.n_iter=30\n</code></pre>   Este experimento con nombre <code>exp2_cv5_n30</code> se prueba con los par\u00e1metros <code>cv=5</code> y <code>n_iter=30</code>.</p> </li> </ul> <p>Cada experimento genera nuevas salidas y m\u00e9tricas, almacenadas temporalmente por DVC.</p>"},{"location":"versionado_datos/p12/#generar-metricas","title":"Generar m\u00e9tricas","text":"<p>Es importante notar que en la etapa entrenamiento_evaluacion usamos la opci\u00f3n <code>metrics</code>. Esto es relevante porque ahora podemos revisar y comparar las m\u00e9tricas generadas por cada experimento.</p> <p>Para comparar los resultados entre los experimentos::</p> <pre><code>dvc exp show\n</code></pre> <p>Este comando mostrar\u00e1 una tabla con los experimentos ejecutados, indicando los valores de par\u00e1metros y m\u00e9tricas (extra\u00eddas de <code>reports/scores.json</code>).</p> <p>Tambi\u00e9n puedes usar:</p> <pre><code>dvc exp diff exp1_cv3_n10 exp2_cv5_n30\n</code></pre> <p>Para ver una comparaci\u00f3n directa de los cambios en par\u00e1metros y m\u00e9tricas entre el experimento actual y el anterior.</p> <p>\ud83d\udcbe (Opcional) Guardar un experimento que te interesa</p> <p>Si uno de los experimentos produjo buenos resultados:</p> <pre><code>dvc exp apply &lt;experiment_name&gt;\ngit commit -m \"Aplicar configuraci\u00f3n del mejor experimento\"\n</code></pre> <p>\ud83d\udee0\ufe0f Tarea</p> <ul> <li>Una vez ejecutados los experimentos y visualizados los resultados responda las preguntas en la plataforma virtual.</li> </ul>"},{"location":"versionado_modelo/p13/","title":"Control de Versiones del Modelo","text":"<p>Hasta este momento, se ha utilizado DVC (Data Version Control) como parte fundamental del flujo MLOps para versionar datasets, mantener un control detallado sobre los modelos entrenados y estructurar un pipeline reproducible mediante <code>dvc.yaml</code>. Esta herramienta ha sido clave para asegurar la trazabilidad de los datos y el control sobre los artefactos generados durante el desarrollo.</p> <p>Sin embargo, cuando se trata de realizar m\u00faltiples experimentos, comparar resultados entre ejecuciones o registrar m\u00e9tricas y par\u00e1metros de manera organizada, DVC presenta limitaciones. No est\u00e1 dise\u00f1ado espec\u00edficamente para gestionar experimentos en detalle ni para proporcionar una visualizaci\u00f3n clara de su evoluci\u00f3n. En este contexto, se vuelve necesario incorporar una herramienta que permita complementar este control con capacidades de seguimiento m\u00e1s ricas.</p> <p>MLflow cumple precisamente este prop\u00f3sito. Es una herramienta que se integra f\u00e1cilmente en el flujo de trabajo existente y permite registrar, comparar y visualizar los experimentos de machine learning. Su inclusi\u00f3n dentro del proceso MLOps permite documentar autom\u00e1ticamente los par\u00e1metros usados, las m\u00e9tricas obtenidas y los modelos generados, facilitando la comparaci\u00f3n entre experimentos y mejorando la capacidad de an\u00e1lisis.</p> <p>Por tanto, mientras DVC se encarga de mantener el control sobre los datos y asegurar la reproducibilidad, MLflow se enfoca en el seguimiento de los experimentos y la gesti\u00f3n de resultados. El uso conjunto de ambas herramientas aporta una soluci\u00f3n m\u00e1s completa para desarrollar, evaluar, mantener y monitorear modelos de machine learning de forma robusta y escalable. </p> <p>IMPORTANTE</p> <p>MLflow permite monitorear y registrar el comportamiento de los modelos durante la etapa de experimentaci\u00f3n, facilitando la comparaci\u00f3n de resultados, el an\u00e1lisis de m\u00e9tricas y la trazabilidad del proceso de entrenamiento.</p> <p>La herramienta tiene cuatro componentes principales, y uno de ellos \u2014MLflow Tracking\u2014 permite monitorear el proceso de entrenamiento de modelos, registrando:</p> <ul> <li>Par\u00e1metros usados en cada experimento</li> <li>M\u00e9tricas obtenidas</li> <li>Artefactos generados (modelos, gr\u00e1ficos, logs)</li> <li>Versiones de c\u00f3digo y entorno</li> </ul> <p>Si se desea monitorear el modelo en producci\u00f3n se suelen usar herramientas como Prometheus, Grafana, EvidentlyAI o Seldon Core, integradas en un stack de MLOps m\u00e1s completo.</p>"},{"location":"versionado_modelo/p13/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Complementar el flujo de trabajo MLOps basado en DVC con el uso de MLflow para llevar un seguimiento sistem\u00e1tico de experimentos, incluyendo par\u00e1metros, m\u00e9tricas y versiones de modelos.</p>"},{"location":"versionado_modelo/p13/#instalacion-de-mlflow","title":"Instalaci\u00f3n de MLflow","text":"<p>Para comenzar a usar MLflow en tu entorno de desarrollo, puedes instalarlo f\u00e1cilmente utilizando <code>pip</code> o <code>conda</code>, seg\u00fan c\u00f3mo est\u00e9s gestionando tus entornos. Puedes instalar MLflow ejecutando uno de los dos comandos:</p> <pre><code>conda install -c conda-forge mlflow\npip install mlflow\n</code></pre> <p>Una vez instalado, puedes verificar su disponibilidad ejecutando el siguiente comando en la terminal:</p> <pre><code>mlflow --version\n</code></pre>"},{"location":"versionado_modelo/p13/#modelo-base","title":"Modelo Base","text":"<p>En el contexto de nuestro proyecto, el c\u00f3digo para entrenar el modelo de predicci\u00f3n se encuentra en el archivo <code>src/model_eng/stage_1_2_train_evaluate.py</code>, el cual corresponde a la etapa <code>entrenamiento_evaluacion</code> definida en el flujo de trabajo con DVC. Esta etapa se encarga de ejecutar el proceso de entrenamiento y evaluaci\u00f3n del modelo utilizando <code>scikit-learn</code>. Para complementar el control de versiones y reproducibilidad que ofrece DVC, vamos a extender este script incorporando el registro de experimentos con MLflow, lo que permitir\u00e1 llevar un seguimiento detallado de los modelos, los par\u00e1metros utilizados y los resultados obtenidos.</p> <p>A este c\u00f3digo base se han incorporado las siguientes instrucciones clave:</p> <ul> <li> <p>Importaciones necesarias: Se incluyen las bibliotecas de <code>mlflow</code>, <code>mlflow.sklearn</code> y la funci\u00f3n <code>infer_signature</code>, que se utiliza para capturar la estructura de entrada y salida del modelo (por ejemplo, columnas y tipos de datos) al momento de registrarlo en MLflow. Esto permite que, al guardar el modelo con <code>mlflow.log_model()</code>, MLflow registre tambi\u00e9n la firma (signature) del modelo, lo cual facilita futuras validaciones autom\u00e1ticas al momento de reutilizar o desplegar el modelo.</p> <pre><code>import mlflow\nimport mlflow.sklearn\nfrom mlflow.models import infer_signature\n</code></pre> </li> <li> <p>Definici\u00f3n del experimento: Dentro de la funci\u00f3n <code>def model_eval()</code> se establece el nombre del experimento, lo que permite organizar y visualizar f\u00e1cilmente los distintos modelos entrenados dentro de la interfaz de MLflow. Cada ejecuci\u00f3n del script se registrar\u00e1 como una \"run\" dentro del experimento definido, permitiendo comparar configuraciones e identificar la mejor versi\u00f3n del modelo.</p> <p><pre><code>mlflow.set_experiment(exp_name)\n</code></pre> Cabe destacar que, en nuestro proyecto, el nombre del experimento se genera de forma din\u00e1mica, incorporando la fecha y hora actuales.</p> </li> <li> <p>Ejecuci\u00f3n de una \"run\" con <code>mlflow.start_run()</code>: Para que MLflow registre los detalles de un experimento, es necesario encapsular el bloque de c\u00f3digo correspondiente dentro de <code>mlflow.start_run()</code>. Esta funci\u00f3n crea una nueva ejecuci\u00f3n (\"run\") dentro del experimento activo. Dentro de este bloque se colocan todas las instrucciones que se desean registrar: los par\u00e1metros del modelo (<code>mlflow.log_param()</code>), las m\u00e9tricas obtenidas (<code>mlflow.log_metric()</code>), y el propio modelo (<code>mlflow.sklearn.log_model()</code>). Al finalizar el bloque, MLflow cierra autom\u00e1ticamente la ejecuci\u00f3n y registra los resultados.</p> </li> </ul> <p>Identifica este c\u00f3digo (m\u00e1s adelante) en el archivo <code>stage_1_2_train_evaluate.py</code> actualizado. </p>"},{"location":"versionado_modelo/p13/#uso-de-mlflow-ejecucion-local-vs-servidor-de-tracking","title":"Uso de MLflow: ejecuci\u00f3n local vs. servidor de tracking","text":"<p>MLflow fue dise\u00f1ado para adaptarse a distintos niveles de madurez en los proyectos de Machine Learning. Por eso, ofrece dos formas principales de ejecutar y registrar experimentos: una ejecuci\u00f3n b\u00e1sica y autom\u00e1tica a nivel local, y otra m\u00e1s avanzada y centralizada a trav\u00e9s de un servidor de tracking.</p>"},{"location":"versionado_modelo/p13/#por-que-existen-estas-dos-opciones","title":"\u00bfPor qu\u00e9 existen estas dos opciones?","text":"<p>En un flujo t\u00edpico de desarrollo MLOps, los requerimientos cambian con el tiempo:</p> <ul> <li> <p>Al inicio, un desarrollador puede trabajar de forma individual, experimentando r\u00e1pidamente con distintos modelos y par\u00e1metros. En este contexto, tener que levantar un servidor para registrar cada experimento ser\u00eda innecesario y poco pr\u00e1ctico. Por eso, MLflow permite registrar y visualizar experimentos directamente en el sistema de archivos local, sin necesidad de configuraci\u00f3n adicional.</p> </li> <li> <p>A medida que el proyecto avanza, se vuelve necesario compartir experimentos, mantener un historial organizado, o integrar con herramientas externas para despliegue y monitoreo. En este escenario, MLflow permite ejecutar un servidor local (o remoto) de tracking, donde los experimentos pueden ser almacenados de forma centralizada y accesibles por varios usuarios o procesos.</p> </li> </ul> <p>Estas dos opciones permiten que el uso de MLflow escale progresivamente desde un entorno personal hasta un entorno colaborativo y reproducible, alineado con los principios de MLOps.</p>"},{"location":"versionado_modelo/p13/#opcion-a-uso-local-archivo-tracking-por-defecto","title":"\ud83d\udd39 Opci\u00f3n A: Uso local (archivo tracking por defecto)","text":"<p>En esta opci\u00f3n, MLflow guarda autom\u00e1ticamente los experimentos en una carpeta local llamada <code>mlruns</code>, sin necesidad de configurar un servidor. Es el comportamiento por defecto cuando no se especifica ninguna URI de tracking.</p> <p>Caracter\u00edsticas:</p> <ul> <li>No requiere configuraci\u00f3n adicional.</li> <li>Los datos se almacenan en el sistema de archivos local.</li> <li>Ideal para uso individual y pruebas r\u00e1pidas.</li> <li>Experimentos visibles ejecutando <code>mlflow ui</code> en la misma m\u00e1quina.</li> </ul> <p>Etapa recomendada:</p> <ul> <li> <p>Etapas tempranas del proyecto o desarrollo personal, cuando est\u00e1s iterando en el modelo y probando configuraciones de manera local.</p> </li> <li> <p>\u00datil para experimentar de forma \u00e1gil sin preocuparte a\u00fan por colaboraci\u00f3n o despliegue.</p> </li> </ul> <p>Par\u00e1metros relevantes:</p> <ul> <li>No es necesario usar <code>mlflow.set_tracking_uri()</code>.</li> <li>El directorio <code>mlruns/</code> aparece autom\u00e1ticamente al ejecutar cualquier experimento.</li> </ul>"},{"location":"versionado_modelo/p13/#tarea","title":"\ud83d\udee0\ufe0f Tarea","text":"<ul> <li>Descarga la versi\u00f3n del script <code>stage_1_2_train_evaluate.py</code> y actualiza este codigo en tu proyecto. </li> <li> <p>Revisa el c\u00f3digo e identifica las secciones incorporadas para el uso de MLflow. En particular, ubica el uso de los siguientes m\u00e9todos:</p> <ul> <li> <p><code>mlflow.set_tracking_uri()</code>: no es necesario utilizarlo en el caso de uso local, por lo que esta l\u00ednea esta documentada en el c\u00f3digo, como referencia para configuraciones m\u00e1s avanzadas con un servidor de tracking. </p> </li> <li> <p><code>mlflow.log_params()</code>: para registrar los par\u00e1metros utilizados durante el entrenamiento.</p> </li> <li> <p><code>mlflow.log_metric()</code>: para almacenar m\u00e9tricas de evaluaci\u00f3n del modelo.</p> </li> <li> <p><code>mlflow.sklearn.log_model()</code>: para guardar el modelo entrenado dentro del sistema de tracking de MLflow. Ten en cuenta que durante la etapa de desarrollo no siempre es necesario registrar el modelo completo en cada ejecuci\u00f3n. Si lo consideras conveniente, puedes comentar temporalmente esta l\u00ednea para evitar almacenar m\u00faltiples versiones del modelo. </p> </li> </ul> </li> <li> <p>Ejecuta nuevamente los dos experimentos realizados en la pr\u00e1ctica Trabajando con Pipelines, entonces compara los resultados llamando a la interfaz Web de MlFlow. </p> <p>Abre con:</p> <p><pre><code>mlflow ui\n</code></pre> Dentro de la herramienta MLFlow compara los resultados de los dos experimentos mostrando los detalles de la ejecuci\u00f3n:</p> <p></p> </li> </ul>"},{"location":"versionado_modelo/p13/#opcion-b-uso-con-servidor-de-tracking","title":"\ud83d\udd39 Opci\u00f3n B: Uso con servidor de tracking","text":"<p>Aqu\u00ed se lanza un servidor de tracking de MLflow que act\u00faa como backend centralizado, usando una base de datos (como SQLite, PostgreSQL, etc.) y un almacenamiento definido para los artefactos.</p> <p>Caracter\u00edsticas:</p> <ul> <li>Requiere ejecutar mlflow server con par\u00e1metros espec\u00edficos.</li> <li>Permite consultas m\u00e1s robustas y almacenamiento estructurado.</li> <li>Facilita la colaboraci\u00f3n en equipo y organizaci\u00f3n de m\u00faltiples experimentos.</li> <li>Puedes conectarte desde distintos scripts o m\u00e1quinas (si configuras acceso por red).</li> </ul> <p>Etapa recomendada:</p> <ul> <li> <p>Etapas intermedias o avanzadas del proyecto, cuando ya est\u00e1s trabajando con varios experimentos, varios miembros del equipo, o cuando necesitas mantener un historial m\u00e1s persistente y ordenado de los modelos.</p> </li> <li> <p>Muy \u00fatil antes del despliegue o al comenzar evaluaciones comparativas</p> </li> </ul> <p>Par\u00e1metros relevantes al lanzar el servidor: Para iniciar el servidor de MLflow con una configuraci\u00f3n m\u00ednima, puedes ejecutar el siguiente comando desde una nueva terminal. Debes usar estos comandos m\u00e1s adelante cuando se te pida ejecutar la tarea: </p> <pre><code>mlflow server \\\n  --backend-store-uri sqlite:///mlflow.db \\\n  --default-artifact-root ./mlruns \\\n  --host 127.0.0.1 \\\n  --port 5000\n</code></pre> <p>Y en tu script debes especificar la URI del tracking (descomenta la l\u00ednea que ya existe en el c\u00f3digo): <pre><code>mlflow.set_tracking_uri(config.mlflow.tracking_uri)\n</code></pre></p>"},{"location":"versionado_modelo/p13/#tarea_1","title":"\ud83d\udee0\ufe0f Tarea","text":"<ul> <li> <p>Elimina la carpeta <code>mlruns</code> que se gener\u00f3 al ejecutar MLflow de forma local. A partir de ahora, al ejecutar los experimentos, los resultados deber\u00edan almacenarse directamente en la base de datos gestionada por el servidor de MLflow.</p> </li> <li> <p>Agrega los siguientes par\u00e1metros en el archivo de configuraci\u00f3n <code>model_eng.yaml</code></p> <pre><code>mlflow:\n  mlruns_path = \"models\"\n  tracking_uri = \"http://127.0.0.1:5000\"\n</code></pre> </li> <li> <p>Configura el uso de MLFlow con servidor de tracking. Para ello recuerda ejecutar estas acciones en el siguiente orden:</p> <ol> <li>Inicia el servidor de MLFlow</li> <li>Descomenta la linea que define la URI del tracking server en el archivo <code>stage_1_2_train_evaluate.py</code>.</li> </ol> </li> <li> <p>Vuelve a ejecutar los dos experimentos realizados en la pr\u00e1ctica Trabajando con Pipelines. Modifica al menos un par\u00e1metro en cada experimento para forzar su reejecuci\u00f3n. Si es necesario, cambia tambi\u00e9n el nombre del experimento. Luego, compara los resultados obtenidos directamente en el servidor de MLflow.</p> </li> </ul>"},{"location":"versionado_modelo/p13/#resumen-del-uso-de-mlflow","title":"Resumen del uso de MLFlow","text":"<p>MLflow permite registrar y visualizar experimentos, ya sea de forma local (Opci\u00f3n A) o mediante un servidor de tracking dedicado (Opci\u00f3n B), seg\u00fan las necesidades del proyecto.</p> Aspecto Opci\u00f3n A: Uso local Opci\u00f3n B: Servidor tracking Configuraci\u00f3n Ninguna Requiere levantar servidor Persistencia Carpeta local (<code>mlruns</code>) Backend + almacenamiento definido Colaboraci\u00f3n Limitada Posible si se configura acceso remoto Escalabilidad Baja Alta (puede migrar a servidor remoto) Etapa recomendada Inicial / desarrollo personal Intermedia / colaborativa / producci\u00f3n <code>set_tracking_uri</code> necesario No S\u00ed"}]}