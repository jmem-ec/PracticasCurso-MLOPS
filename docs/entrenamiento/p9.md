El desarrollo de sistemas de aprendizaje autom√°tico (ML) ha evolucionado desde simples prototipos en notebooks hacia aplicaciones robustas que requieren principios de ingenier√≠a de software. En este contexto, MLOps surge como un enfoque que combina las mejores pr√°cticas del desarrollo de software y la operaci√≥n de modelos de ML, facilitando la reproducibilidad, escalabilidad y mantenibilidad de los sistemas de inteligencia artificial.

Una parte fundamental de MLOps es la **implementaci√≥n de buenas pr√°cticas de codificaci√≥n**, que no solo permiten mejorar la calidad del c√≥digo, sino tambi√©n facilitar el trabajo colaborativo, la automatizaci√≥n de flujos de trabajo y la integraci√≥n con herramientas de versionado, testing y despliegue continuo. En esta pr√°ctica se explorar√°n tres pilares clave para lograr un desarrollo m√°s profesional y organizado:

- **Uso de archivos de configuraci√≥n**: Separar la l√≥gica del c√≥digo de los par√°metros y configuraciones del sistema facilita la experimentaci√≥n, la reutilizaci√≥n del c√≥digo y la trazabilidad de los cambios.

- **Interfaces de l√≠nea de comandos**: Incorporar interfaces CLI robustas y flexibles permite ejecutar scripts de forma controlada y program√°tica, facilitando la integraci√≥n en pipelines automatizados y la ejecuci√≥n reproducible de experimentos.

- **Buenas pr√°cticas de documentaci√≥n y estilo**: Aplicar principios como documentaci√≥n, seguimiento de estilo (PEP8, linters) y tipado permite construir c√≥digo m√°s limpio, escalable y confiable, caracter√≠sticas esenciales para proyectos de ciencia de datos que evolucionan hacia producci√≥n.

## üéØ Objetivos

- Dise√±ar e implementar estructuras de configuraci√≥n externas para separar los par√°metros de ejecuci√≥n de la l√≥gica del c√≥digo.

- Explorar distintas formas de construir interfaces de l√≠nea de comandos (CLI) para aplicaciones de ML.

- Aplicar buenas pr√°cticas de codificaci√≥n en el desarrollo de componentes de un proyecto MLOps.

## Uso de archivos de configuraci√≥n con Hydra
El c√≥digo actual del proyecto utiliza `argparse` para recibir ciertos par√°metros necesarios para la ejecuci√≥n de los scripts; sin embargo, tambi√©n **incorpora otros par√°metros directamente codificados en el cuerpo del programa**. Por ejemplo, el script `stage0_loading.py`, utiliza `argparse` para recibir la URL donde est√° alojado  los datos externos, pero adem√°s incluye valores codificados directamente en el c√≥digo fuente, como la ruta donde almacenar el archivo descargado. Este enfoque limita la flexibilidad y dificulta su uso en distintos entornos o etapas del pipeline.

El uso de valores codificados ("hardcoded") como:

```bash
self.external_data = "data/external"
```

impide modificar rutas de entrada o salida sin alterar directamente el c√≥digo, lo cual va en contra de las buenas pr√°cticas en proyectos MLOps.

Por ello, se propone migrar a un enfoque basado en archivos de configuraci√≥n con [Hydra](https://hydra.cc/), una herramienta moderna que permite manejar par√°metros de manera flexible y desacoplada del c√≥digo, ideal para flujos complejos de entrenamiento, pruebas y despliegue de modelos.

### Propuesta de mejora

Hydra permite manejar configuraciones con archivos `.yaml`, dejando el c√≥digo fuente limpio y desacoplado de decisiones contextuales como rutas de archivos, nombres, etc. A continuaci√≥n los pasos para migrar a Hydra:

1. Instalar Hydra

    Hasta la fecha actual, Hydra no est√° disponible directamente como paquete en los canales est√°ndar de Conda (como `conda-forge` o `defaults`), por lo que la forma recomendada de instalar `hydra-core` es usando `pip`, incluso dentro de un entorno Conda:

    ```bash
    pip install hydra-core
    ```

2. Crear un archivo de configuraci√≥n YAML

    Crea el archivo `configs/data_eng.yaml`, el cual contendr√° todos los par√°metros necesarios para la fase Ingenier√≠a de Datos (Data Enginnering) del flujo de trabajo de MLOPS. Por el momento, el archivo tendra el siguiente contenido:

    ```yaml
    data_source:
        url: "https://raw.githubusercontent.com/jmem-ec/KRRCourse/ccbd6ccf8389ba0988d53fc9300a64da00e6368b/Consignment_pricing.csv"
        external_data_dir: "data/external"
        filename: "Consignment_pricing.csv"
    ```

3. Modificar el script para usar Hydra

    Se han realizado las siguientes modificaciones para integrar la configuraci√≥n con Hydra:
    
    - Incorporar en la seccion de importaci√≥n 
        ```python
            import hydra
            from omegaconf import DictConfig
        ```
    
    - Reemplaza el uso de `argparse` por hydra, y usa los valores del archivo `.yaml`

        ```python
        @hydra.main(config_path=f"{os.getcwd()}/configs", config_name="data_eng", version_base=None)
        def main(cfg: DictConfig):
            logging.basicConfig(level=logging.INFO)
            data = GetData().get_data(cfg)

        if _name_ == "_main_":
            main()
        ```
        Toma en cuenta que **no puedes llamar directamente** a la funci√≥n decorada con `@hydra.main(...)` como `main()` desde `if __name__ == "__main__":`, porque Hydra necesita controlar el punto de entrada del script para realizar su redirecci√≥n de rutas y configuraci√≥n del entorno de ejecuci√≥n.

    - Modifica la cabecera de la funcion `get_data` 

        ```python
        def get_data(self, config)
        ```

    - Cambia los valores codificados ("hardcoded") usando los valores del archivo `.yaml`
        ```python
        ...
        github_csv_url = config.data_source.url
        
        self.external_data = config.data_source.external_data_dir

        ...
        local_path = os.path.join(self.external_data, config.data_source.filename)
        ```

#### üõ†Ô∏è Tarea

- Por defecto, Hydra escribir√° los resultados en una carpeta de resultados, con una subcarpeta para el d√≠a en que se ejecut√≥ el experimento y, adem√°s, la hora en que se inici√≥. Inspeccione su ejecuci√≥n revisando cada archivo que Hydra ha generado y compruebe que la informaci√≥n ha sido registrada.

- Hydra tambi√©n permite cambiar y a√±adir par√°metros din√°micamente sobre la marcha desde la l√≠nea de comandos:

    - Pruebe a cambiar un par√°metro desde la l√≠nea de comandos.
    ```python
    python src/data_eng/stage0_loading.py data_source.filename=Dataset1.csv
    ```

    - Pruebe a a√±adir un par√°metro desde la l√≠nea de comandos.
    ```python
    python src/data_eng/stage0_loading.py +data_source.additional=50
    ```

- Realice un nuevo experimento utilizando un nuevo archivo de configuraci√≥n en el que cambie un par√°metro de su elecci√≥n. No se le permite cambiar el archivo de configuraci√≥n en el script, sino que deber√≠a poder proporcionarlo como argumento al iniciar el script, por ejemplo, algo como

    ```bash
    python src/data_eng/stage0_loading.py experiment=exp2
    ```

    Le recomendamos que utilice una estructura de archivos como la siguiente:

    ```bash
    |--confs
    |  |--model1.yaml
    |  |--data_eng.yaml
    |  |--experiments
    |     |--exp1.yaml
    |     |--exp2.yaml
    |--data
    |--...
    ```

    Este ser√≠a el archivo base de configuraci√≥n `data_eng.yaml`, que incluye un grupo `experiment`:

    ```bash
    defaults:
        - experiment: exp1   # valor por defecto

    data_source:
        url: "https://raw.githubusercontent.com/jmem-ec/KRRCourse/ccbd6ccf8389ba0988d53fc9300a64da00e6368b/Consignment_pricing.csv"
        external_data_dir: "data/external"
        filename: "Consignment_pricing.csv"
    ```

    Experimento 1: configs/experiments/exp1.yaml

    ```bash
    data_source:
        filename: "Dataset1.csv"
    ```

    Experimento 2: configs/experiments/exp2.yaml

    ```bash
    data_source:
        filename: "Dataset2.csv"
    ```

    Ahora puede probar los experimentos

    ```bash
    # Ejecuta con la configuraci√≥n por defecto (exp1)
    python src/data_eng/stage0_loading.py

    # Ejecuta con la configuraci√≥n de experimento 2
    python src/data_eng/stage0_loading.py experiment=exp2
    ```

- Modifique el script `stage1_ingestion.py` para que obtenga los par√°metros necesarios desde el archivo de configuraci√≥n `data_eng.yaml`, en lugar de tenerlos codificados directamente en el script.
En particular, aseg√∫rate de que el archivo `data_eng.yaml` incluya la siguiente secci√≥n:

    ```bash
    raw_data:
        raw_data_dir: "data/raw"
        raw_filename: "Dataset.csv"
    ```

## Interfaces de l√≠nea de comandos
Incorporar interfaces de l√≠nea de comandos robustas y flexibles permite ejecutar scripts de forma controlada y program√°tica, facilitando su integraci√≥n en pipelines automatizados (por ejemplo, para entrenamiento o evaluaci√≥n de modelos) y garantizando la reproducibilidad de los experimentos.

En esta secci√≥n aprender√°s a crear scripts ejecutables como comandos dentro de tu proyecto. Esto permite:

- Ejecutar scripts sin invocar expl√≠citamente `python script.py`.

- Documentar y estandarizar la forma en que se ejecutan tareas clave del proyecto.

- Integrar f√°cilmente con herramientas como DVC, MLFlow, o cualquier otro sistema de automatizaci√≥n o CI/CD.

Aunque existen muchas formas de construir una CLI en Python (como [`argparse`](https://docs.python.org/es/3/library/argparse.html), [`click`](https://github.com/pallets/click) o [`typer`](https://typer.tiangolo.com/)), aqu√≠ comenzaremos por una soluci√≥n nativa usando `pyproject.toml`.

### Definir una tarea CLI para limpiar los datos
El proyecto incluye el archivo `stage2_cleaning.py`, encargado de realizar tareas de limpieza de datos, como la imputaci√≥n de valores faltantes, el tratamiento de valores nulos, la transformaci√≥n de columnas y otras operaciones necesarias para preparar los datos antes del entrenamiento del modelo.

```bash
src/
‚îú‚îÄ‚îÄ data_eng/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ stage2_cleaning.py
‚îÇ   ‚îú‚îÄ‚îÄ ...
pyproject.toml
```
La tarea consiste en ejecutar la limpieza de los datos simplemente escribiendo:

```bash
clean
```

en lugar de:
```bash
python stage2_cleaning.py
```

Esto se consigue mediante:

1. Asegurar que `stage2_cleaning.py` contenga una funci√≥n `main()` como punto de entrada.
2. Configurar el archivo `pyproject.toml` para registrar un comando CLI personalizado. Espec√≠ficamente, al a√±adir:

    ```bash
    [project.scripts]
    clean = "data_eng.stage2_cleaning:main"
    ```

3. Instalar el proyecto en modo editable:

    ```bash
    pip install -e .
    ```

Ahora se puede ejecutar en la terminal, el comando:

```bash
clean
```

#### üõ†Ô∏è Tarea
Agrega otros comandos CLI para diferentes etapas del flujo de trabajo MLOps. Por ejemplo:

```bash
ingestion = "data_eng.stage1_ingestion:main"
feature_engineering  = "data_eng.stage3_labeling:main"
splitting  = "data_eng.stage4_splitting:main"
train&evaluate = "model_eng.model1.stage1-2_train-evaluate:main"
```

### Ejecutar c√≥digo no-Python desde CLI
Hasta ahora, hemos aprendido a construir interfaces de l√≠nea de comandos (CLI) para scripts en Python, facilitando su ejecuci√≥n directa desde el terminal. Sin embargo, en proyectos de Machine Learning reales, no todo el c√≥digo es Python. A menudo necesitamos ejecutar herramientas de l√≠nea de comandos como `git`, `conda`, `dvc` o incluso `docker`, y estas invocaciones pueden volverse complejas, largas y dif√≠ciles de recordar.

Por ejemplo, ejecutar una aplicaci√≥n con Docker que use GPU puede implicar un comando tan largo como este:

```bash
docker run -v $(pwd):/app -w /app --gpus all --rm -it my_image:latest python my_script.py --arg1 val1 --arg2 val2
```

Recordar y mantener estos comandos puede ser engorroso. Una alternativa mucho m√°s limpia y mantenible es definir comandos m√°s amigables como:

```bash
invoke run_my_experiment --arg1=val1 --arg2=val2
```

Para lograr esto, vamos a utilizar el paquete `invoke`, una herramienta que permite definir tareas reutilizables desde Python para ser ejecutadas como comandos CLI. `invoke` act√∫a como una versi√≥n moderna de `Makefile`, y su integraci√≥n como paquete de Python facilita su uso multiplataforma.

#### Automatizaci√≥n de la fase de Ingenier√≠a de Datos
Para automatizar esta etapa del flujo de trabajo en MLOps, es necesario ejecutar las siguientes tareas de forma secuencial:

1. Instala el paquete `invoke`

2. Crea un archivo llamado `tasks.py` en la ra√≠z del proyecto

    ```bash
    src/
    ‚îú‚îÄ‚îÄ data_eng/
    ‚îÇ   ‚îú‚îÄ‚îÄ stage1_ingestion.py
    ‚îÇ   ‚îú‚îÄ‚îÄ stage2_cleaning.py
    ‚îÇ   ‚îú‚îÄ‚îÄ stage3_labeling.py
    ‚îÇ   ‚îî‚îÄ‚îÄ stage4_splitting.py
    tasks.py
    ```

3. Agrega el siguiente contenido al archivo `tasks.py`. Se asume que cada script contiene una funci√≥n `main()` como punto de entrada.

    ```python
    from invoke import task

    @task
    def ingest(ctx):
        ctx.run("python data_eng/stage1_ingestion.py")

    @task
    def clean(ctx):
        ctx.run("python data_eng/stage2_cleaning.py")

    @task
    def features(ctx):
        ctx.run("python data_eng/stage3_labeling.py")

    @task
    def split(ctx):
        ctx.run("python data_eng/stage4_splitting.py")

    @task
    def data_eng_pipeline(ctx):
        ingest(ctx)
        clean(ctx)
        features(ctx)
        split(ctx)
    ```

    El primer argumento de cualquier funci√≥n decorada con `@task` es `ctx`, un contexto que permite ejecutar comandos como si estuvi√©ramos en la terminal, usando su m√©todo `run`.

    > üìù **Nota**:  
    > Si no est√°s seguro de las tareas disponibles en `invoke`, puedes usar el comando:
    > 
    >```bash
    >invoke --list
    >```

4. Desde la terminal, puedes ahora ejecutar todo el **pipeline de data engineering** con un solo comando:

    ```bash
    invoke data_eng_pipeline
    ```

##### üõ†Ô∏è Tarea
- En lugar de invocar directamente los scripts usando Python desde las tareas definidas en `tasks-py`, te recomendamos utilizar los comandos CLI personalizados que hayas configurado en el archivo `pyproject.toml`. Esto permite una ejecuci√≥n m√°s clara, consistente y alineada con las buenas pr√°cticas de automatizaci√≥n.

- Crea una tarea que simplifique el proceso de subir cambios del proyecto local al repositorio remoto. Esta tarea debe ejecutar de forma secuencial los siguientes comandos de Git:

    - `git add .`
    - `git commit -m "Mi mensaje de commit"`
    - `git push origin main`
    
    El objetivo es que puedas ejecutar todo este flujo con un solo comando desde la terminal usando `invoke`, por ejemplo:

    ```bash
    invoke git --mensaje ¬´Mi mensaje de commit¬ª
    ```

## Buenas pr√°cticas de documentaci√≥n y estilo
Mantener scripts bien documentados es una parte clave en las buenas pr√°cticas de codificaci√≥n dentro del flujo MLOps. Los docstrings permiten describir el prop√≥sito, los par√°metros y los valores de retorno de funciones y clases. Esto mejora la comprensi√≥n del c√≥digo y facilita su integraci√≥n en herramientas automatizadas.

GitHub Copilot puede ayudarte a generar estos docstrings de manera r√°pida y consistente.