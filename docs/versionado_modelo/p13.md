Hasta este momento, se ha utilizado DVC (Data Version Control) como parte fundamental del flujo MLOps para versionar datasets, mantener un control detallado sobre los modelos entrenados y estructurar un pipeline reproducible mediante `dvc.yaml`. Esta herramienta ha sido clave para asegurar la trazabilidad de los datos y el control sobre los artefactos generados durante el desarrollo.

Sin embargo, cuando se trata de realizar m√∫ltiples experimentos, comparar resultados entre ejecuciones o registrar m√©tricas y par√°metros de manera organizada, DVC presenta limitaciones. No est√° dise√±ado espec√≠ficamente para gestionar experimentos en detalle ni para proporcionar una visualizaci√≥n clara de su evoluci√≥n. En este contexto, se vuelve necesario incorporar una herramienta que permita complementar este control con capacidades de seguimiento m√°s ricas.

MLflow cumple precisamente este prop√≥sito. Es una herramienta que se integra f√°cilmente en el flujo de trabajo existente y permite registrar, comparar y visualizar los experimentos de machine learning. Su inclusi√≥n dentro del proceso MLOps permite documentar autom√°ticamente los par√°metros usados, las m√©tricas obtenidas y los modelos generados, facilitando la comparaci√≥n entre experimentos y mejorando la capacidad de an√°lisis.

Por tanto, mientras DVC se encarga de mantener el control sobre los datos y asegurar la reproducibilidad, MLflow se enfoca en el seguimiento de los experimentos y la gesti√≥n de resultados. El uso conjunto de ambas herramientas aporta una soluci√≥n m√°s completa para desarrollar, evaluar, mantener y monitorear modelos de machine learning de forma robusta y escalable. 

!!! note "IMPORTANTE"
    MLflow permite monitorear y registrar el comportamiento de los modelos durante la etapa de experimentaci√≥n, facilitando la comparaci√≥n de resultados, el an√°lisis de m√©tricas y la trazabilidad del proceso de entrenamiento.

    La herramienta tiene cuatro componentes principales, y uno de ellos ‚Äî**MLflow Tracking**‚Äî permite monitorear el proceso de entrenamiento de modelos, registrando:

    - Par√°metros usados en cada experimento
    - M√©tricas obtenidas
    - Artefactos generados (modelos, gr√°ficos, logs)
    - Versiones de c√≥digo y entorno

    Si se desea monitorear el modelo en producci√≥n se suelen usar herramientas como Prometheus, Grafana, EvidentlyAI o Seldon Core, integradas en un stack de MLOps m√°s completo.

## üéØ Objetivo

Complementar el flujo de trabajo MLOps basado en DVC con el uso de MLflow para llevar un seguimiento sistem√°tico de experimentos, incluyendo par√°metros, m√©tricas y versiones de modelos.

## Instalaci√≥n de MLflow

Para comenzar a usar MLflow en tu entorno de desarrollo, puedes instalarlo f√°cilmente utilizando `pip` o `conda`, seg√∫n c√≥mo est√©s gestionando tus entornos. Puedes instalar MLflow ejecutando **uno de los dos comandos**:

```bash
conda install -c conda-forge mlflow
pip install mlflow
```

Una vez instalado, puedes verificar su disponibilidad ejecutando el siguiente comando en la terminal:

```bash
mlflow --version
```

## Modelo Base

En el contexto de nuestro proyecto, el c√≥digo para entrenar el modelo de predicci√≥n se encuentra en el archivo `src/model_eng/stage_1_2_train_evaluate.py`, el cual corresponde a la etapa `entrenamiento_evaluacion` definida en el flujo de trabajo con DVC. Esta etapa se encarga de ejecutar el proceso de entrenamiento y evaluaci√≥n del modelo utilizando `scikit-learn`. Para complementar el control de versiones y reproducibilidad que ofrece DVC, vamos a extender este script incorporando el registro de experimentos con MLflow, lo que permitir√° llevar un seguimiento detallado de los modelos, los par√°metros utilizados y los resultados obtenidos.

A este c√≥digo base se han incorporado las siguientes instrucciones clave:

- **Importaciones necesarias**: Se incluyen las bibliotecas de `mlflow`, `mlflow.sklearn` y la funci√≥n `infer_signature`, que se utiliza para capturar la estructura de entrada y salida del modelo (por ejemplo, columnas y tipos de datos) al momento de registrarlo en MLflow. Esto permite que, al guardar el modelo con `mlflow.log_model()`, MLflow registre tambi√©n la firma (signature) del modelo, lo cual facilita futuras validaciones autom√°ticas al momento de reutilizar o desplegar el modelo.

    ```python
    import mlflow
    import mlflow.sklearn
    from mlflow.models import infer_signature
    ```

- **Definici√≥n del experimento**: Dentro de la funci√≥n `def model_eval()` se establece el nombre del experimento, lo que permite organizar y visualizar f√°cilmente los distintos modelos entrenados dentro de la interfaz de MLflow. Cada ejecuci√≥n del script se registrar√° como una "run" dentro del experimento definido, permitiendo comparar configuraciones e identificar la mejor versi√≥n del modelo.

    ```python
    mlflow.set_experiment(exp_name)
    ```
    Cabe destacar que, en nuestro proyecto, el nombre del experimento se genera de forma din√°mica, incorporando la fecha y hora actuales.

- **Ejecuci√≥n de una "run" con `mlflow.start_run()`**: Para que MLflow registre los detalles de un experimento, es necesario encapsular el bloque de c√≥digo correspondiente dentro de `mlflow.start_run()`. Esta funci√≥n crea una nueva ejecuci√≥n ("run") dentro del experimento activo. Dentro de este bloque se colocan todas las instrucciones que se desean registrar: los par√°metros del modelo (`mlflow.log_param()`), las m√©tricas obtenidas (`mlflow.log_metric()`), y el propio modelo (`mlflow.sklearn.log_model()`). Al finalizar el bloque, MLflow cierra autom√°ticamente la ejecuci√≥n y registra los resultados.

Identifica este c√≥digo (m√°s adelante) en el archivo `stage_1_2_train_evaluate.py` actualizado. 

## Uso de MLflow: ejecuci√≥n local vs. servidor de tracking

MLflow fue dise√±ado para adaptarse a distintos niveles de madurez en los proyectos de Machine Learning. Por eso, ofrece **dos formas principales de ejecutar y registrar experimentos**: una ejecuci√≥n b√°sica y autom√°tica a nivel local, y otra m√°s avanzada y centralizada a trav√©s de un servidor de tracking.

### ¬øPor qu√© existen estas dos opciones?

En un flujo t√≠pico de desarrollo MLOps, los requerimientos cambian con el tiempo:

- Al inicio, un desarrollador puede trabajar de forma individual, experimentando r√°pidamente con distintos modelos y par√°metros. En este contexto, tener que levantar un servidor para registrar cada experimento ser√≠a innecesario y poco pr√°ctico. Por eso, **MLflow permite registrar y visualizar experimentos directamente en el sistema de archivos local**, sin necesidad de configuraci√≥n adicional.

- A medida que el proyecto avanza, se vuelve necesario compartir experimentos, mantener un historial organizado, o integrar con herramientas externas para despliegue y monitoreo. En este escenario, MLflow permite ejecutar un **servidor local (o remoto) de tracking**, donde los experimentos pueden ser almacenados de forma centralizada y accesibles por varios usuarios o procesos.

Estas dos opciones permiten que el uso de MLflow escale progresivamente desde un entorno personal hasta un entorno colaborativo y reproducible, alineado con los principios de MLOps.


## üîπ Opci√≥n A: Uso local (archivo tracking por defecto)

En esta opci√≥n, MLflow guarda autom√°ticamente los experimentos en una carpeta local llamada `mlruns`, sin necesidad de configurar un servidor. Es el comportamiento por defecto cuando no se especifica ninguna URI de tracking.

**Caracter√≠sticas**:

- No requiere configuraci√≥n adicional.
- Los datos se almacenan en el sistema de archivos local.
- Ideal para uso individual y pruebas r√°pidas.
- Experimentos visibles ejecutando `mlflow ui` en la misma m√°quina.

**Etapa recomendada**:

- Etapas tempranas del proyecto o desarrollo personal, cuando est√°s iterando en el modelo y probando configuraciones de manera local.

- √ötil para experimentar de forma √°gil sin preocuparte a√∫n por colaboraci√≥n o despliegue.

**Par√°metros relevantes**:

- No es necesario usar `mlflow.set_tracking_uri()`.
- El directorio `mlruns/` aparece autom√°ticamente al ejecutar cualquier experimento.

### üõ†Ô∏è Tarea
- Descarga la versi√≥n del script `stage_1_2_train_evaluate.py` y actualiza este codigo en tu proyecto. 
- Revisa el c√≥digo e identifica las secciones incorporadas para el uso de MLflow. En particular, ubica el uso de los siguientes m√©todos:

    - `mlflow.set_tracking_url()`: no es necesario utilizarlo en el caso de uso local, por lo que esta l√≠nea **esta documentada en el c√≥digo**, como referencia para configuraciones m√°s avanzadas con un servidor de tracking. 

    - `mlflow.log_params()`: para registrar los par√°metros utilizados durante el entrenamiento.

    - `mlflow.log_metric()`: para almacenar m√©tricas de evaluaci√≥n del modelo.

    - `mlflow.sklearn.log_model()`: para guardar el modelo entrenado dentro del sistema de tracking de MLflow. Ten en cuenta que durante la etapa de desarrollo **no siempre es necesario registrar el modelo completo en cada ejecuci√≥n**. Si lo consideras conveniente, puedes comentar temporalmente esta l√≠nea para evitar almacenar m√∫ltiples versiones del modelo. 

- Revisa los resultados de la ejecuci√≥n de los experimentos, llamando a la interfaz Web de MlFlow. 

    Abre con:

    ```bash
    mlflow ui
    ```


## üîπ Opci√≥n B: Uso con servidor de tracking

Aqu√≠ se lanza un servidor de tracking de MLflow que act√∫a como backend centralizado, usando una base de datos (como SQLite, PostgreSQL, etc.) y un almacenamiento definido para los artefactos.

**Caracter√≠sticas**:

- Requiere ejecutar mlflow server con par√°metros espec√≠ficos.
- Permite consultas m√°s robustas y almacenamiento estructurado.
- Facilita la colaboraci√≥n en equipo y organizaci√≥n de m√∫ltiples experimentos.
- Puedes conectarte desde distintos scripts o m√°quinas (si configuras acceso por red).

**Etapa recomendada**:

- Etapas intermedias o avanzadas del proyecto, cuando ya est√°s trabajando con varios experimentos, varios miembros del equipo, o cuando necesitas mantener un historial m√°s persistente y ordenado de los modelos.

- Muy √∫til antes del despliegue o al comenzar evaluaciones comparativas

**Par√°metros relevantes al lanzar el servidor**:
Para iniciar el servidor de MLflow con una configuraci√≥n m√≠nima, puedes ejecutar el siguiente comando desde una nueva terminal: 

```bash
mlflow server \
  --backend-store-uri sqlite:///mlflow.db \
  --default-artifact-root ./mlruns \
  --host 0.0.0.0 \
  --port 5000
```

Y en tu script debes especificar la URI del tracking:
```bash
mlflow.set_tracking_uri("http://localhost:5000")
```

### üõ†Ô∏è Tarea
- Agrega los siguientes par√°metros en el archivo de configuraci√≥n `model_eng.yaml`

    ```bash
    mlflow:
      mlruns_path = "models"
      tracking_uri = "http://localhost:5000"
    ```

- Prueba el uso de MLFlow con servidor de tracking. Para ello recuerda ejecutar estas acciones en el siguiente orden:

    1. Inicia el servidor de MLFlow
    2. Descomenta la linea que define la URI del tracking server en el archivo `stage_1_2_train_evaluate.py`.


## Resumen del uso de MLFlow

MLflow permite registrar y visualizar experimentos, ya sea de forma local (Opci√≥n A) o mediante un servidor de tracking dedicado (Opci√≥n B), seg√∫n las necesidades del proyecto.

| Aspecto                      | Opci√≥n A: Uso local           | Opci√≥n B: Servidor tracking             |
| ---------------------------- | ----------------------------- | -------------------------------------- |
| Configuraci√≥n                | Ninguna                       | Requiere levantar servidor             |
| Persistencia                 | Carpeta local (`mlruns`)      | Backend + almacenamiento definido      |
| Colaboraci√≥n                 | Limitada                      | Posible si se configura acceso remoto  |
| Escalabilidad                | Baja                          | Alta (puede migrar a servidor remoto)  |
| Etapa recomendada            | Inicial / desarrollo personal | Intermedia / colaborativa / producci√≥n |
| `set_tracking_uri` necesario | No                            | S√≠                                     |
