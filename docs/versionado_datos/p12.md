Despu√©s de configurar el almacenamiento remoto y configurar el inicio de un proyecto DVC para versionar datos y modelos, el siguiente paso es automatizar y estructurar flujos de trabajo mediante la construcci√≥n de un pipeline de experimentos. Un pipeline permite encadenar etapas del proceso (por ejemplo, preprocesamiento, entrenamiento y evaluaci√≥n) de forma reproducible y trazable, facilitando el trabajo colaborativo y la experimentaci√≥n sistem√°tica.

Esta pr√°ctica est√° basada en el conjunto de datos previamente versionado con DVC (`data/raw/Dataset.csv`). Este archivo es la fuente de datos para entrenar el modelo de predicci√≥n. Recordemos que el proyecto implementado tiene tres etapas principales:

- Preprocesamiento de datos (limpieza, extracci√≥n de caracter√≠sticas, y divisi√≥n de datos)
- Entrenamiento del modelo
- Evaluaci√≥n del modelo

## üéØ Objetivos

- Construir un pipeline de ML utilizando DVC.
- Declarar las dependencias y salidas de cada etapa del flujo de trabajo.
- Ejecutar y rastrear autom√°ticamente los cambios en los datos o en el c√≥digo que afecten el resultado del experimento.
- Comprender c√≥mo DVC administra los archivos intermedios y el modelo final en cada etapa.

## üõ†Ô∏è Requisitos

Debes contar con los siguientes scripts organizados en tu proyecto:

- **stage2_cleaning.py**: Realiza la limpieza y preprocesamiento de los datos brutos, eliminando valores inconsistentes o faltantes y estandarizando formatos para su posterior an√°lisis.

- **stage3_labeling.py**: Extrae caracter√≠sticas relevantes desde la fuente de datos y asigna etiquetas necesarias para el entrenamiento supervisado del modelo.

- **stage4_splitting.py**: Divide el conjunto de datos procesado en subconjuntos de entrenamiento y prueba, asegurando una distribuci√≥n adecuada para la validaci√≥n del modelo.

- **stage1_2_train_evaluate.py**: Entrena el modelo utilizando el conjunto de entrenamiento y eval√∫a su desempe√±o sobre el conjunto de prueba, generando m√©tricas de rendimiento clave.

!!! warning "Advertencia:"
    Los pipelines propuestos a continuaci√≥n est√°n basados en la estructura del repositorio (creado en la pr√°ctica [Estructura del Proyecto](../entorno/p5.md) y especializado en las pr√°cticas [Versionado del C√≥digo](../entrenamiento/p7.md), [Empaquetado y gesti√≥n de dependencias](../entrenamiento/p8.md) y [Buenas pr√°cticas de codificaci√≥n](../entrenamiento/p9.md)). Sin embargo, es posible crear otros scripts y crear pipelines espec√≠ficos para las necesidades de su proyecto.


## Creaci√≥n de pipelines
DVC construye un pipeline basado en tres componentes: Entradas (Inputs), Salidas (Outputs) y Comando (Command). Por ejemplo, para la etapa de limpieza de datos, estos ser√≠an los componentes:

- **Entradas**: archivo `data/raw/Dataset.csv` y el script `stage2_cleaning.py`
- **Salidas**: archivo `data/interim/Cleaned_Dataset.csv`
- **Comando**: `python3 stage2_cleaning.py`

Para crear esta etapa, utilizamos el comando `dvc stage add` de la siguiente forma:

```bash
dvc stage add -n limpieza \
  -d src/data_eng/stage2_cleaning.py -d data/raw/Dataset.csv \
  -o data/interim/Cleaned_Dataset.csv \
  python3 src/data_eng/stage2_cleaning.py
```

Aqu√≠ nombramos esta etapa como "limpieza" usando la opci√≥n `-n`. Tambi√©n definimos las entradas con la opci√≥n `-d` y las salidas con la opci√≥n `-o`. El comando que se ejecuta siempre va al final del comando `dvc stage add`, sin ninguna opci√≥n.

!!! warning "Consejo"
    Los archivos de salida se agregan al control de DVC cuando reproduces una etapa de DVC. Cuando finalices tu experimento, recuerda usar dvc push para versionar no solo los datos usados sino tambi√©n los resultados generados durante el experimento.

En este punto, es posible que hayas notado que se creo un nuevo archivo: `dvc.yaml`. Este archivo es responsable de guardar lo que se describi√≥ en cada comando `dvc stage add`. Por lo tanto, si deseas crear o modificar una etapa espec√≠fica, es posible editar directamente el archivo `dvc.yaml`. El archivo actual se ver√≠a as√≠:

```bash
stages:
  limpieza:
    cmd: python3 src/data_eng/stage2_cleaning.py
    deps:
    - src/data_eng/stage2_cleaning.py
    - data/raw/Dataset.csv
    outs:
    - data/interim/Cleaned_Dataset.csv
```

El pipeline actual se ve as√≠:

![Pipeline_1](recursos/FaseLimpieza.png)

### Probar la primera etapa
Una vez que se ha creado una etapa del pipeline usando `dvc stage add`, ya es posible probar la ejecuci√≥n del pipeline utilizando el comando:

```bash
dvd repro
```
Por ahora, el pipeline cuenta √∫nicamente con una etapa llamada `limpieza`, la cual se encarga del preprocesamiento inicial de los datos. Sin embargo, m√°s adelante se integrar√°n otras fases como extracci√≥n de caracter√≠sticas, division de datos, entrenamiento y evaluaci√≥n del modelo.

#### ¬øQu√© hace dvc repro?
- Ejecuta las etapas necesarias del pipeline en el orden definido por sus dependencias.
- Reproduce etapas autom√°ticamente solo si detecta que alguna dependencia ha cambiado (por ejemplo, si se modific√≥ un archivo de entrada o un par√°metro).
- Encadena etapas si hay varias definidas y conectadas entre s√≠.

Cada vez que se ejecuta `dvc repro`, DVC genera o actualiza el archivo `dvc.lock`. Este tambi√©n es un archivo YAML y su funci√≥n es similar a los archivos `.dvc`. En su interior, podemos encontrar la ruta y un c√≥digo hash para cada archivo de cada etapa, lo que permite a DVC hacer seguimiento de los cambios. Este seguimiento es importante porque ahora DVC puede saber cu√°ndo una etapa debe ejecutarse de nuevo o no, bas√°ndose en si sus dependencias cambiaron.

üõ†Ô∏è Tarea

La primera vez que ejecutaste `dvc repro`, se ejecut√≥ por completo la etapa de limpieza, generando los archivos de salida correspondientes.

- ¬øQu√© ocurre si vuelves a ejecutar `dvc repro` sin realizar ning√∫n cambio en los datos, par√°metros o c√≥digo?. Responde la pregunta en la plataforma virtual.

### Actualizaci√≥n de las dem√°s etapas del pipeline

A partir de este punto, puedes definir y actualizar las siguientes etapas del pipeline directamente en el archivo `dvc.yaml`. Cada tarea o paso del flujo de trabajo se especifica como una etapa (stage) dentro de dicho archivo, siguiendo la estructura del pipeline. Por ejemplo:

```yaml
stages:
  limpieza:
    cmd: python3 src/data_eng/stage2_cleaning.py
    deps:
    - data/raw/Dataset.csv
    - src/data_eng/stage2_cleaning.py
    outs:
    - data/interim/Cleaned_Dataset.csv

  extraccion_caracteristicas:
    cmd: python3 src/data_eng/stage3_labeling.py
    deps:
    - data/interim/Cleaned_Dataset.csv
    - src/data_eng/stage3_labeling.py
    outs:
    - data/processed/Processed_Dataset.csv

  division:
    cmd: python3 src/data_eng/stage4_splitting.py
    deps:
    - data/processed/Processed_Dataset.csv
    - src/data_eng/stage4_splitting.py
    outs:
    - data/processed/Train_Dataset.csv
    - data/processed/Test_Dataset.csv

  entrenamiento_evaluacion:
    cmd: python3 src/model_eng/stage1_2_train_evaluate.py
    deps:
    - data/processed/Test_Dataset.csv
    - data/processed/Train_Dataset.csv
    - src/model_eng/stage1_2_train_evaluate.py
    params:
      - configs/model_eng.yaml:
        - RandomizedSearchCV.scoring
        - RandomizedSearchCV.n_iter
    outs:
    - models/model_rf.pkl
    metrics:
    - reports/scores.json
```

El pipeline que representa el proyecto quedar√≠a as√≠:

![Pipeline_2](recursos/Pipeline.png)

Entre las etapas definidas en el archivo `dvc.yaml`, a continuaci√≥n se describe en detalle la etapa **entrenamiento_evaluacion**:

Esta etapa forma parte del pipeline de DVC y tiene como objetivo entrenar y evaluar el modelo de aprendizaje autom√°tico utilizando conjuntos de datos previamente procesados:

- `entrenamiento_evaluacion`: es el indentificador de la etapa.
- `cmd: python3 src/model_eng/stage1_2_train_evaluate.py`: Este es el script que se ejecuta cuando se corre esta etapa. En este caso, se trata de un script en Python que:

    - Entrena el modelo y eval√∫a su desempe√±o.
    - Guarda el modelo entrenado y las m√©tricas.

- `deps`:
    - `data/processed/Test_Dataset.csv`
    - `data/processed/Train_Dataset.csv`
    - `src/model_eng/stage1_2_train_evaluate.py`

      Estas son las entradas que DVC monitorea para saber si el stage debe volver a ejecutarse. Si alguno de estos archivos cambia, DVC vuelve a ejecutar el stage. Aqu√≠ se incluyen:

      - Los datos de entrenamiento y prueba procesados.
      - El script de entrenamiento/evaluaci√≥n.

- `params:`
    - `configs/model_eng.yaml:`
      - `RandomizedSearchCV.scoring`
      - `RandomizedSearchCV.n_iter`

    Estos son par√°metros que vienen de un archivo YAML externo (`configs/model_eng.yaml`) y que el script usa. Si cambias estos valores, DVC detectar√° el cambio y volver√° a ejecutar el stage.

- `outs:`
    - `models/model_rf.pkl`

    La salida es el modelo entrenado que se guarda como resultado. DVC lo versiona autom√°ticamente.

- `metrics:`
    - `reports/scores.json`

    En DVC las m√©tricas se pueden mostrar, comparar entre versiones y usarlas en visualizaciones. Para el caso del proyecto las m√©tricas a incluir est√°n en el archivo JSON `reports/scores.json`, el cual contiene m√©tricas del modelo como: rmse, r2_score, mse, train_score, etc. 

üõ†Ô∏è Tarea

- Una vez definido todo el pipeline del proyecto vuelve a ejecuar el comando `dvc repro`, para comprobar que todo funciona correctamente. 

    üìå Observa cuidadosamente qu√© etapas no se ejecutan: si DVC detecta que sus dependencias y salidas no han cambiado, las considerar√° actualizadas y omitir√° su ejecuci√≥n. 

    Este comportamiento confirma que la l√≥gica de seguimiento de cambios del pipeline est√° funcionando como se espera. Responda las preguntas en la plataforma virtual.

    !!! note "Reproducibilidad garantizada con `dvc repro`"
        Cualquier persona que acceda a tu proyecto (por ejemplo, clon√°ndolo desde Git y teniendo acceso al almacenamiento remoto de DVC) puede ejecutar:
        
        ```bash
        dvc repro
        ```
        
        y as√≠ reconstruir autom√°ticamente todos los resultados del pipeline, de forma reproducible y en el mismo orden en que fueron generados originalmente.

        Esto es posible porque los archivos `dvc.yaml` y `dvc.lock` contienen toda la informaci√≥n necesaria sobre:

          - Las etapas del pipeline.
          - Los comandos utilizados.
          - Los archivos de entrada y salida.
          - Los par√°metros utilizados.

        üí° Esto asegura que los experimentos sean totalmente trazables, repetibles y colaborativos. para **reconstruir resultados reproducibles**.

- Modifica la secci√≥n `params`: dentro del archivo `dvc.yaml`, reemplazando el par√°metro:
`RandomizedSearchCV.scoring` por `RandomizedSearchCV.cv`, suponiendo que por un error se escogi√≥ el incorrecto. Mientras que `scoring` se refiere a la m√©trica utilizada para evaluar los modelos, el par√°metro que realmente se quer√≠a controlar era `cv`, el cual define el n√∫mero de particiones utilizadas en la validaci√≥n cruzada (cross-validation).

    ¬øQu√© etapas se volvieron a ejecutar?. Responde las preguntas en la plataforma virtual.

- Muestra las m√©tricas del estado m√°s reciente de tu proyecto reproducido con DVC. Para ello puedes usar el comando:

    ```bash
    dvc metrics show
    ```

    Este comando muestra las m√©tricas actuales guardadas en los archivos definidos en `metrics:` dentro de `dvc.yaml`. Responde la pregunta en la plataforma virtual.

## Ejecutar experimentos
Una caracter√≠stica fundamental de DVC es que permite ejecutar experimentos reproducibles sobre tus pipelines de Machine Learning. Los experimentos en DVC son variantes del pipeline que pueden modificar par√°metros, datos o c√≥digo, sin alterar tu rama principal de Git. Esto es ideal para comparar modelos y evaluar distintas configuraciones de entrenamiento.

Los experimentos:

- Se ejecutan con `dvc exp run`.
- Pueden modificar par√°metros f√°cilmente usando `--set-param (-S)`.
- Se almacenan como commits temporales y se pueden comparar con m√©tricas y gr√°ficos.
- Se pueden promover a Git si se desea conservar.

A continuaci√≥n se propone un par de experimentos con diferentes valores de par√°metros:

- Paso 1: Ejecutar el primer experimento
  ```bash
  dvc exp run \
        --name exp1_cv3_n10 \
        -S configs/model_eng.yaml:RandomizedSearchCV.cv=3 \
        -S configs/model_eng.yaml:RandomizedSearchCV.n_iter=10
  ```
  Este experimento con nombre `exp1_cv3_n10` se prueba con los par√°metros `cv=3` y `n_iter=10`.

- Paso 2: Ejecutar el segundo experimento
  ```bash
  dvc exp run \
        --name exp2_cv5_n30 \
        -S configs/model_eng.yaml:RandomizedSearchCV.cv=5 \
        -S configs/model_eng.yaml:RandomizedSearchCV.n_iter=30
  ```
  Este experimento con nombre `exp2_cv5_n30` se prueba con los par√°metros `cv=5` y `n_iter=30`.

> Cada experimento genera nuevas salidas y m√©tricas, almacenadas temporalmente por DVC.

## Generar m√©tricas
  
Es importante notar que en la etapa **entrenamiento_evaluacion** usamos la opci√≥n `metrics`. Esto es relevante porque ahora podemos revisar y comparar las m√©tricas generadas por cada experimento.

Para comparar los resultados entre los experimentos::

```bash
dvc exp show
```

Este comando mostrar√° una tabla con los experimentos ejecutados, indicando los valores de par√°metros y m√©tricas (extra√≠das de `reports/scores.json`).

Tambi√©n puedes usar:

```bash
dvc exp diff exp1_cv3_n10 exp2_cv5_n30
```

Para ver una comparaci√≥n directa de los cambios en par√°metros y m√©tricas entre el experimento actual y el anterior.

üíæ (Opcional) Guardar un experimento que te interesa

Si uno de los experimentos produjo buenos resultados:

```bash
dvc exp apply <experiment_name>
git commit -m "Aplicar configuraci√≥n del mejor experimento"
```

üõ†Ô∏è Tarea

- Una vez ejecutados los experimentos y visualizados los resultados responda las preguntas en la plataforma virtual.